# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_biasutti2019riu.ipynb.

# %% auto 0
__all__ = ['Block', 'Encoder', 'Decoder', 'RIUNet', 'LitDataModule', 'LitModel']

# %% ../nbs/01_biasutti2019riu.ipynb 3
import torch
from torch.nn import Module, Sequential, Conv2d, BatchNorm2d, ReLU, ModuleList, MaxPool2d, ConvTranspose2d, CrossEntropyLoss
from torch.optim import Adam
from torch.utils.data import DataLoader, random_split
from lightning import LightningDataModule, LightningModule
from .behley2019iccv import SemanticKITTIDataset, UnfoldingProjection, ProjectionTransform, ProjectionToTensorTransform
from torchvision.transforms import v2
from torchmetrics.classification import Accuracy

# %% ../nbs/01_biasutti2019riu.ipynb 4
class Block(Module):
    "Convolutional block repeatedly used in the RIU-Net encoder and decoder."
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.net = Sequential(
            Conv2d(in_channels, out_channels, 3, 1, 1, bias=False, padding_mode='circular'), 
            BatchNorm2d(out_channels, momentum=0.01), 
            ReLU(),
            Conv2d(out_channels, out_channels, 3, 1, 1, bias=False, padding_mode='circular'), 
            BatchNorm2d(out_channels, momentum=0.01), 
            ReLU(),
        )
    
    def forward(self, x):
        return self.net(x)

# %% ../nbs/01_biasutti2019riu.ipynb 7
class Encoder(Module):
    "RIU-Net encoder architecture."
    def __init__(self, channels=(5, 64, 128, 256, 512, 1024)):
        super().__init__()
        self.blocks = ModuleList(
            [Block(channels[i], channels[i+1]) for i in range(len(channels)-1)]
        )
        self.pool = MaxPool2d(2)
    
    def forward(self, x):
        enc_features = []
        for block in self.blocks:
            x = block(x)
            enc_features.append(x)
            x = self.pool(x)
        return enc_features

# %% ../nbs/01_biasutti2019riu.ipynb 10
class Decoder(Module):
    "RIU-Net decoder architecture."
    def __init__(self, channels=(1024, 512, 256, 128, 64)):
        super().__init__()
        self.upconvs = ModuleList(
            [ConvTranspose2d(channels[i], channels[i+1], 6, 2, 2) for i in range(len(channels)-1)]
        )
        self.blocks = ModuleList(
            [Block(channels[i], channels[i+1]) for i in range(len(channels)-1)]
        )
    
    def forward(self, enc_features):
        x = enc_features[-1]
        for i, (upconv, block) in enumerate(zip(self.upconvs, self.blocks)):
            x = upconv(x)
            x = torch.cat([x, enc_features[-(i+2)]], dim=1)
            x = block(x)
        return x

# %% ../nbs/01_biasutti2019riu.ipynb 13
class RIUNet(Module):
    "RIU-Net complete architecture."
    def __init__(self, in_channels=5, hidden_channels=(64, 128, 256, 512, 1024), n_classes=20):
        super().__init__()
        self.backbone = Sequential(
            Encoder((in_channels, *hidden_channels)),
            Decoder(hidden_channels[::-1])
        )
        self.head = Conv2d(hidden_channels[0], n_classes, 1)
    
    def forward(self, x):
        features = self.backbone(x)
        prediction = self.head(features)
        
        return prediction

# %% ../nbs/01_biasutti2019riu.ipynb 16
class LitDataModule(LightningDataModule):
    "Lightning DataModule to facilitate reproducibility of experiments in the original paper."
    def __init__(self, train_batch_size=8, eval_batch_size=16, num_workers=8):
        super().__init__()
        self.train_batch_size = train_batch_size
        self.eval_batch_size = eval_batch_size
        self.num_workers = num_workers
    
    def setup(self, stage: str):
        data_path = '/workspace/data'
        proj = UnfoldingProjection(W=512, H=64)
        tfms = v2.Compose([
            ProjectionTransform(proj),
            ProjectionToTensorTransform(),
        ])
        if stage == "fit":
            ds = SemanticKITTIDataset(data_path, transform=tfms)
            self.ds_train, self.ds_val = random_split(
                ds, [0.7, 0.3], generator=torch.Generator().manual_seed(42)
            )
        if stage == "test":
            self.ds_test = SemanticKITTIDataset(data_path, is_train=False, transform=tfms)
        if stage == "predict":
            self.ds_predict = SemanticKITTIDataset(data_path, is_train=False, transform=tfms)
            

    def train_dataloader(self):
        return DataLoader(self.ds_train, batch_size=self.train_batch_size, num_workers=self.num_workers)

    def val_dataloader(self):
        return DataLoader(self.ds_val, batch_size=self.eval_batch_size, num_workers=self.num_workers)

    def test_dataloader(self):
        return DataLoader(self.ds_test, batch_size=2*self.eval_batch_size, num_workers=self.num_workers)

    def predict_dataloader(self):
        return DataLoader(self.ds_predict, batch_size=self.eval_batch_size, num_workers=self.num_workers)

# %% ../nbs/01_biasutti2019riu.ipynb 18
class LitModel(LightningModule):
    "Lightning Module to facilitate reproducibility of experiments in the original paper."
    def __init__(self, debugging=False, debugging_hook=None):
        super().__init__()
        self.bn = BatchNorm2d(5, affine=False, momentum=None)
        self.net = RIUNet()
        self.loss_fn = CrossEntropyLoss(reduction='none')
        self.train_accuracy = Accuracy(task="multiclass", num_classes=20)
        self.val_accuracy = Accuracy(task="multiclass", num_classes=20)
        self.debugging = debugging
        self.debugging_hook = debugging_hook
        
        for n, m in self.net.named_modules():
            m.name = n

    def repetitive_step_routine(self, batch, batch_idx, stage, metric):
        img, label, mask = batch
        label[~mask] = 0

        bn_img = self.bn(img)
        pred = self.net(bn_img)
        
        loss = self.loss_fn(pred, label)
        std = loss[mask].std()
        loss = loss[mask].mean()

        pred_f = torch.permute(pred, (0, 2, 3, 1))
        pred_f = torch.flatten(pred_f, 0, -2)
        mask_f = torch.flatten(mask)
        pred_m = pred_f[mask_f, :]
        label_m = label[mask]
        metric(pred_m, label_m)

        self.log(f"{stage}_loss_std", std)
        self.log(f"{stage}_acc_step", metric)
        self.log(f"{stage}_loss", loss)

        if stage=="train" and self.debugging:
            if self.global_step % 10 == 0:
                wandb_logger = self.logger.experiment
                with torch.nn.modules.module.register_module_forward_hook(
                    self.debugging_hook(wandb_logger, self.global_step)
                ):
                    self.net(bn_img)  # run forward pass on current batch to log activations
        
        return loss

    def training_step(self, batch, batch_idx):
        return self.repetitive_step_routine(batch, batch_idx, "train", self.train_accuracy)

    def on_train_epoch_end(self):
        self.log('train_acc_epoch', self.train_accuracy)

    def validation_step(self, batch, batch_idx):
        self.repetitive_step_routine(batch, batch_idx, "val", self.val_accuracy)

    def on_validation_epoch_end(self):
        self.log('val_acc_epoch', self.val_accuracy)

    def configure_optimizers(self):
        optimizer = Adam(self.parameters(), lr=1e-3)
        return optimizer
