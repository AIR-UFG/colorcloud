# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_biasutti2019riu.ipynb.

# %% auto 0
__all__ = ['Block', 'Encoder', 'Decoder', 'RIUNet', 'log_activations', 'SemanticSegmentationTask']

# %% ../nbs/01_biasutti2019riu.ipynb 3
import torch
import torch.nn.functional as F
from torch.nn import Module, Sequential, Conv2d, BatchNorm2d, ReLU, ModuleList, ConvTranspose2d
from torch.nn.init import kaiming_normal_, constant_, zeros_
from torch.nn.modules.module import register_module_forward_hook
from torch.optim import Adam
from torch.optim.lr_scheduler import OneCycleLR
from lightning import LightningModule
from torchmetrics.classification import Accuracy
from collections import OrderedDict
import re
import wandb
import numpy as np

# %% ../nbs/01_biasutti2019riu.ipynb 5
class Block(Sequential):
    "Convolutional block repeatedly used in the RIU-Net encoder and decoder."
    def __init__(self, in_channels, out_channels):
        super().__init__(OrderedDict([
            (f'conv1', Conv2d(in_channels, out_channels, 3, 1, 1, bias=False, padding_mode='circular')),
            (f'bn1', BatchNorm2d(out_channels, momentum=0.01)),
            (f'relu1', ReLU()),
            (f'conv2', Conv2d(out_channels, out_channels, 3, 1, 1, bias=False, padding_mode='circular')),
            (f'bn2', BatchNorm2d(out_channels, momentum=0.01)),
            (f'relu2', ReLU()),
        ]))
        self.init_params()
    
    def init_params(self):
        for n, p in self.named_parameters():
            if re.search('conv\d\.weight', n):
                kaiming_normal_(p, nonlinearity='relu')
            if re.search('bn\d\.bias', n):
                constant_(p, 0.1)

# %% ../nbs/01_biasutti2019riu.ipynb 11
class Encoder(Module):
    "RIU-Net encoder architecture."
    def __init__(self, channels=(5, 64, 128, 256, 512, 1024)):
        super().__init__()
        self.blocks = ModuleList(
            [Block(channels[i], channels[i+1]) for i in range(len(channels)-1)]
        )
    
    def forward(self, x):
        enc_features = []
        for block in self.blocks:
            x = block(x)
            enc_features.append(x)
            x = F.max_pool2d(x, 2)
        return enc_features

# %% ../nbs/01_biasutti2019riu.ipynb 14
class Decoder(Module):
    "RIU-Net decoder architecture."
    def __init__(self, channels=(1024, 512, 256, 128, 64)):
        super().__init__()
        self.upconvs = ModuleList(
            [ConvTranspose2d(channels[i], channels[i+1], 6, 2, 2, bias=False) for i in range(len(channels)-1)]
        )
        self.blocks = ModuleList(
            [Block(channels[i], channels[i+1]) for i in range(len(channels)-1)]
        )
        self.init_params()

    def init_params(self):
        for n, p in self.named_parameters():
            if re.search('upconvs\.\d\.weight', n):
                kaiming_normal_(p, nonlinearity='linear')
    
    def forward(self, enc_features):
        x = enc_features[-1]
        for i, (upconv, block) in enumerate(zip(self.upconvs, self.blocks)):
            x = upconv(x)
            x = torch.cat([x, enc_features[-(i+2)]], dim=1)
            x = block(x)
        return x

# %% ../nbs/01_biasutti2019riu.ipynb 17
class RIUNet(Module):
    "RIU-Net complete architecture."
    def __init__(self, in_channels=5, hidden_channels=(64, 128, 256, 512, 1024), n_classes=20):
        super().__init__()
        self.n_classes = n_classes
        self.input_norm = BatchNorm2d(in_channels, affine=False, momentum=None)
        self.backbone = Sequential(OrderedDict([
            (f'enc', Encoder((in_channels, *hidden_channels))),
            (f'dec', Decoder(hidden_channels[::-1]))
        ]))
        self.head = Conv2d(hidden_channels[0], n_classes, 1)

    def init_params(self):
        for n, p in self.named_parameters():
            if re.search('head\.weight', n):
                kaiming_normal_(p, nonlinearity='linear')
            if re.search('head\.bias', n):
                zeros_(p)
    
    def forward(self, x):
        x = self.input_norm(x)
        features = self.backbone(x)
        prediction = self.head(features)
        
        return prediction

# %% ../nbs/01_biasutti2019riu.ipynb 21
def log_activations(logger, step, model, img):
    "Function that uses a Pytorch forward hook to log properties of activations for debugging purposes."
    def debugging_hook(module, inp, out):
        if 'relu' in module.name:
            acts = out.detach()
            min_count = (acts < 1e-1).sum((0, 2, 3))
            shape = acts.shape
            total_count = shape[0]*shape[2]*shape[3]
            rate = min_count/total_count
            logger.log({"max_dead_rate/" + str(module.name): rate.max()}, step=step)
    with register_module_forward_hook(debugging_hook):
        model(img)

# %% ../nbs/01_biasutti2019riu.ipynb 23
class SemanticSegmentationTask(LightningModule):
    "Lightning Module to standardize experiments with semantic segmentation tasks."
    def __init__(self, model, loss_fn, viz_tfm, total_steps, lr=1e-3):
        super().__init__()
        self.model = model
        self.loss_fn = loss_fn
        self.viz_tfm = viz_tfm
        self.lr = lr
        self.total_steps = total_steps
        self.train_accuracy = Accuracy(task="multiclass", num_classes=model.n_classes)
        self.val_accuracy = Accuracy(task="multiclass", num_classes=model.n_classes)
        self.automatic_optimization = False
        
        for n, m in self.model.named_modules():
            assert not hasattr(m, 'name')
            m.name = n
        
    def configure_optimizers(self):
        optimizer = Adam(self.model.parameters(), lr=self.lr, eps=1e-5)
        lr_scheduler = OneCycleLR(optimizer, self.lr, self.total_steps)
        return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler}
    
    def training_step(self, batch, batch_idx):
        loss = self.step(batch, batch_idx, "train", self.train_accuracy)
        if self.global_step % int(0.01*self.total_steps) == 0:
            log_activations(self.logger.experiment, self.global_step, self.model, batch[0])
        self.manual_optimization(loss)
    
    def on_train_epoch_end(self):
        self.log('train_acc_epoch', self.train_accuracy)

    def validation_step(self, batch, batch_idx):
        self.step(batch, batch_idx, "val", self.val_accuracy)
    
    def step(self, batch, batch_idx, stage, metric):
        img, label, mask = batch
        label[~mask] = 0
        
        pred = self.model(img)
        
        loss = self.loss_fn(pred, label)
        loss = loss[mask]
        loss = loss.mean()

        pred_f = torch.permute(pred, (0, 2, 3, 1)) # N,C,H,W -> N,H,W,C
        pred_f = torch.flatten(pred_f, 0, -2)      # N,H,W,C -> N*H*W,C
        mask_f = torch.flatten(mask)               # N,H,W   -> N*H*W
        pred_m = pred_f[mask_f, :]
        label_m = label[mask]
        metric(pred_m, label_m)
        
        self.log(f"{stage}_acc_step", metric)
        self.log(f"{stage}_loss_step", loss)
        
        if self.global_step % int(0.25*self.total_steps) == 0:
            pred_np = pred[0].detach().cpu().numpy().argmax(0)
            label_np = label[0].detach().cpu().numpy()
            mask_np = mask[0].detach().cpu().numpy()
            _, pred_img, _ = self.viz_tfm(None, pred_np, mask_np)
            _, label_img, _ = self.viz_tfm(None, label_np, mask_np)
            img_cmp = np.concatenate((pred_img, label_img), axis=0)
            img_cmp = wandb.Image(img_cmp)
            self.logger.experiment.log({f"{stage}_examples": img_cmp}, step=self.global_step)
        
        return loss
    
    def manual_optimization(self, loss):
        optimizer = self.optimizers()
        optimizer.zero_grad()
        self.manual_backward(loss)
        
        p_old = {}
        for n, p in self.model.named_parameters():
            p_old[n] = p.detach().clone()
        
        optimizer.step()
        
        for n, p in self.model.named_parameters():
            optim_step = p.detach() - p_old[n]
            self.logger.experiment.log({"ud/" + str(n): (optim_step.std()/(p_old[n].std() + 1e-5)).log10()}, step=self.global_step)
        
        lr_scheduler = self.lr_schedulers()
        lr_scheduler.step()
