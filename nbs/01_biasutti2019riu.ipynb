{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11c69cc-d200-4710-addf-8062129cfe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec6d3ae-45a2-486d-9fab-a7b1b854babc",
   "metadata": {},
   "source": [
    "# biasutti2019riu\n",
    "\n",
    "> Module that implements the model from [RIU-Net: Embarrassingly simple semantic segmentation of 3D LiDAR point cloud](https://arxiv.org/abs/1905.08748)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0028db0e-08f5-4c13-a6fc-d50701159819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp biasutti2019riu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b407d042-2215-4712-9e2c-d951a8cf0fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Module, Sequential, Conv2d, BatchNorm2d, ReLU, ModuleList, ConvTranspose2d, CrossEntropyLoss\n",
    "from torch.nn.init import kaiming_normal_, constant_, zeros_\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from lightning import LightningDataModule, LightningModule\n",
    "from colorcloud.behley2019iccv import SemanticKITTIDataset, UnfoldingProjection, ProjectionTransform, ProjectionToTensorTransform\n",
    "from torchvision.transforms import v2\n",
    "from torchmetrics.classification import Accuracy\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import wandb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4818cc4f-4ebb-454d-9ab1-edeb6f5b22e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Block(Sequential):\n",
    "    \"Convolutional block repeatedly used in the RIU-Net encoder and decoder.\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__(OrderedDict([\n",
    "            (f'conv1', Conv2d(in_channels, out_channels, 3, 1, 1, bias=False, padding_mode='circular')),\n",
    "            (f'bn1', BatchNorm2d(out_channels, momentum=0.01)),\n",
    "            (f'relu1', ReLU()),\n",
    "            (f'conv2', Conv2d(out_channels, out_channels, 3, 1, 1, bias=False, padding_mode='circular')),\n",
    "            (f'bn2', BatchNorm2d(out_channels, momentum=0.01)),\n",
    "            (f'relu2', ReLU()),\n",
    "        ]))\n",
    "        self.init_params()\n",
    "    \n",
    "    def init_params(self):\n",
    "        for n, p in self.named_parameters():\n",
    "            if re.search('conv\\d\\.weight', n):\n",
    "                kaiming_normal_(p, nonlinearity='relu')\n",
    "            if re.search('bn\\d\\.bias', n):\n",
    "                constant_(p, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2893cb46-6646-4581-8683-5ffaa8425c1b",
   "metadata": {},
   "source": [
    "It implements the following architecture:\n",
    "\n",
    "```{mermaid}\n",
    "flowchart LR\n",
    "  A((\"\n",
    "  Input\n",
    "  (bs, in_c, h, w)\")) --> B[\"\n",
    "  Conv(3x3)\n",
    "  in_c -> out_c\"]\n",
    "  B --> C[\"BatchNorm2d\"]\n",
    "  C --> D[\"ReLU\"]\n",
    "  D --> E[\"\n",
    "  Conv(3x3)\n",
    "  out_c -> out_c\"]\n",
    "  E --> F[\"BatchNorm2d\"]\n",
    "  F --> G[\"ReLU\"]\n",
    "  G --> H((\"\n",
    "  Output\n",
    "  (bs, out_c, h, w)\"))\n",
    "```\n",
    "\n",
    "Here is an example on how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb46d424-aa62-4686-b746-b5f144063376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 64, 512]) == (1, 64, 64, 512)\n"
     ]
    }
   ],
   "source": [
    "bs, in_c, out_c, h, w = 1, 5, 64, 64, 512\n",
    "inp = torch.randn(bs, in_c, h, w)\n",
    "\n",
    "b = Block(in_c, out_c)\n",
    "outp = b(inp)\n",
    "assert outp.shape == (bs, out_c, h, w)\n",
    "print(outp.shape, f'== ({bs}, {out_c}, {h}, {w})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1e51fb-d9ee-48f3-a4c9-b6a26e8788a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Encoder(Module):\n",
    "    \"RIU-Net encoder architecture.\"\n",
    "    def __init__(self, channels=(5, 64, 128, 256, 512, 1024)):\n",
    "        super().__init__()\n",
    "        self.blocks = ModuleList(\n",
    "            [Block(channels[i], channels[i+1]) for i in range(len(channels)-1)]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        enc_features = []\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            enc_features.append(x)\n",
    "            x = F.max_pool2d(x, 2)\n",
    "        return enc_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cf6a19-277f-43a2-ad10-efb3a96164fb",
   "metadata": {},
   "source": [
    "It implements the following architecture:\n",
    "\n",
    "```{mermaid}\n",
    "flowchart LR\n",
    "  A((\"\n",
    "  Input\n",
    "  (bs, 5, h, w)\")) --> B[\"\n",
    "  Block\n",
    "  5 -> 64\"]\n",
    "  B --> C[\"MaxPool(2x2)\"]\n",
    "  C --> D[\"\n",
    "  Block\n",
    "  64 -> 128\"]\n",
    "  D --> E[\"MaxPool(2x2)\"]\n",
    "  E --> F[\"\n",
    "  Block\n",
    "  128 -> 256\"]\n",
    "  F --> G[\"MaxPool(2x2)\"]\n",
    "  G --> H[\"\n",
    "  Block\n",
    "  256 -> 512\"]\n",
    "  H --> I[\"MaxPool(2x2)\"]\n",
    "  I --> J[\"\n",
    "  Block\n",
    "  512 -> 1024\"]\n",
    "  B --> L((\"\n",
    "  Output\n",
    "  [(bs, 64, h, w),\n",
    "  (bs, 128, h/2, w/2),\n",
    "  (bs, 256, h/4, w/4),\n",
    "  (bs, 512, h/8, w/8),\n",
    "  (bs, 1024, h/16, w/16)]\"))\n",
    "  D --> L\n",
    "  F --> L\n",
    "  H --> L\n",
    "  J --> L\n",
    "```\n",
    "\n",
    "Here is an example on how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0961c1d1-19f7-402e-bdb5-e1b12dcff13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 64, 64, 512]),\n",
       " torch.Size([1, 128, 32, 256]),\n",
       " torch.Size([1, 256, 16, 128]),\n",
       " torch.Size([1, 512, 8, 64]),\n",
       " torch.Size([1, 1024, 4, 32])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = Encoder()\n",
    "outp = enc(inp)\n",
    "[o.shape for o in outp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e039c48-213f-4411-8d6d-884ecb8f7dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Decoder(Module):\n",
    "    \"RIU-Net decoder architecture.\"\n",
    "    def __init__(self, channels=(1024, 512, 256, 128, 64)):\n",
    "        super().__init__()\n",
    "        self.upconvs = ModuleList(\n",
    "            [ConvTranspose2d(channels[i], channels[i+1], 6, 2, 2, bias=False) for i in range(len(channels)-1)]\n",
    "        )\n",
    "        self.blocks = ModuleList(\n",
    "            [Block(channels[i], channels[i+1]) for i in range(len(channels)-1)]\n",
    "        )\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        for n, p in self.named_parameters():\n",
    "            if re.search('upconvs\\.\\d\\.weight', n):\n",
    "                kaiming_normal_(p, nonlinearity='linear')\n",
    "    \n",
    "    def forward(self, enc_features):\n",
    "        x = enc_features[-1]\n",
    "        for i, (upconv, block) in enumerate(zip(self.upconvs, self.blocks)):\n",
    "            x = upconv(x)\n",
    "            x = torch.cat([x, enc_features[-(i+2)]], dim=1)\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30318b56-525f-4c6d-8368-9ea7ce3fef25",
   "metadata": {},
   "source": [
    "It implements the following architecture:\n",
    "\n",
    "```{mermaid}\n",
    "flowchart LR\n",
    "  A((\"\n",
    "  Input\n",
    "  [(bs, 1024, h/16, w/16),\n",
    "  (bs, 512, h/8, w/8),\n",
    "  (bs, 256, h/4, w/4),\n",
    "  (bs, 128, h/2, w/2),\n",
    "  (bs, 64, h, w)]\")) --> B[\"\n",
    "  ConvTranspose(2x2)\n",
    "  1024 -> 512\"]\n",
    "  B --> C[\"\n",
    "  Block\n",
    "  concat(512,512) -> 512\"]\n",
    "  A --> C\n",
    "  C --> D[\"\n",
    "  ConvTranspose(2x2)\n",
    "  512 -> 256\"]\n",
    "  D --> E[\"\n",
    "  Block\n",
    "  concat(256,256) -> 256\"]\n",
    "  A --> E\n",
    "  E --> F[\"\n",
    "  ConvTranspose(2x2)\n",
    "  256 -> 128\"]\n",
    "  F --> G[\"\n",
    "  Block\n",
    "  concat(128,128) -> 128\"]\n",
    "  A --> G\n",
    "  G --> H[\"\n",
    "  ConvTranspose(2x2)\n",
    "  128 -> 64\"]\n",
    "  H --> I[\"\n",
    "  Block\n",
    "  concat(64,64) -> 64\"]\n",
    "  A --> I\n",
    "  I --> J((\"\n",
    "  Output\n",
    "  (bs, 64, h, w)\"))\n",
    "```\n",
    "\n",
    "Here is an example on how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95b30e7-5000-462b-8a1e-c46c9ad5f210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 64, 512]) == (1, 64, 64, 512)\n"
     ]
    }
   ],
   "source": [
    "dec = Decoder()\n",
    "fts = dec(outp)\n",
    "assert fts.shape == (bs, out_c, h, w)\n",
    "print(fts.shape, f'== ({bs}, {out_c}, {h}, {w})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2b5b8a-c437-4e65-8027-182568366723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RIUNet(Module):\n",
    "    \"RIU-Net complete architecture.\"\n",
    "    def __init__(self, in_channels=5, hidden_channels=(64, 128, 256, 512, 1024), n_classes=20):\n",
    "        super().__init__()\n",
    "        self.backbone = Sequential(OrderedDict([\n",
    "            (f'enc', Encoder((in_channels, *hidden_channels))),\n",
    "            (f'dec', Decoder(hidden_channels[::-1]))\n",
    "        ]))\n",
    "        self.head = Conv2d(hidden_channels[0], n_classes, 1)\n",
    "\n",
    "    def init_params(self):\n",
    "        for n, p in self.named_parameters():\n",
    "            if re.search('head\\.weight', n):\n",
    "                kaiming_normal_(p, nonlinearity='linear')\n",
    "            if re.search('head\\.bias', n):\n",
    "                zeros_(p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        prediction = self.head(features)\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f598dd-e2ca-4b49-90b9-8641cd2e9f99",
   "metadata": {},
   "source": [
    "It implements the following architecture:\n",
    "\n",
    "```{mermaid}\n",
    "flowchart LR\n",
    "  A((\"\n",
    "  Input\n",
    "  (bs, 5, h, w)\")) --> B[\"Encoder\"]\n",
    "  B --> C[\"Decoder\"]\n",
    "  C --> D[\"\n",
    "  Conv(1x1)\n",
    "  64 -> 20\"]\n",
    "  D --> E((\"\n",
    "  Output\n",
    "  (bs, 20, h, w)\"))\n",
    "```\n",
    "\n",
    "Here is an example on how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffcce5f-3640-4f92-9f8d-f2e47389dbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 64, 512]) == (1, 20, 64, 512)\n"
     ]
    }
   ],
   "source": [
    "n_classes=20\n",
    "model = RIUNet()\n",
    "logits = model(inp)\n",
    "assert logits.shape == (bs, n_classes, h, w)\n",
    "print(logits.shape, f'== ({bs}, {n_classes}, {h}, {w})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ad2781-7937-4383-82b3-a34e0fd18a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LitData(LightningDataModule):\n",
    "    \"Lightning DataModule to facilitate reproducibility of experiments in the original paper.\"\n",
    "    def __init__(self, train_batch_size=8, eval_batch_size=16, num_workers=8):\n",
    "        super().__init__()\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.eval_batch_size = eval_batch_size\n",
    "        self.num_workers = num_workers\n",
    "    \n",
    "    def setup(self, stage: str):\n",
    "        data_path = '/workspace/data'\n",
    "        proj = UnfoldingProjection(W=512, H=64)\n",
    "        tfms = v2.Compose([\n",
    "            ProjectionTransform(proj),\n",
    "            ProjectionToTensorTransform(),\n",
    "        ])\n",
    "        if stage == \"fit\":\n",
    "            ds = SemanticKITTIDataset(data_path, transform=tfms)\n",
    "            self.ds_train, self.ds_val = random_split(\n",
    "                ds, [0.7, 0.3], generator=torch.Generator().manual_seed(42)\n",
    "            )\n",
    "        if stage == \"test\":\n",
    "            self.ds_test = SemanticKITTIDataset(data_path, is_train=False, transform=tfms)\n",
    "        if stage == \"predict\":\n",
    "            self.ds_predict = SemanticKITTIDataset(data_path, is_train=False, transform=tfms)\n",
    "            \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.ds_train, batch_size=self.train_batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.ds_val, batch_size=self.eval_batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.ds_test, batch_size=2*self.eval_batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.ds_predict, batch_size=self.eval_batch_size, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cc6c14-ddd4-4381-8820-3f9c16d19465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LitLearner(LightningModule):\n",
    "    \"Lightning Module to facilitate reproducibility of experiments in the original paper.\"\n",
    "    def __init__(self, total_steps, debugging=False, debugging_hook=None, proj_viz_tfm=None):\n",
    "        super().__init__()\n",
    "        self.total_steps = total_steps\n",
    "        self.input_norm = BatchNorm2d(5, affine=False, momentum=None)\n",
    "        self.model = RIUNet()\n",
    "        self.loss_fn = CrossEntropyLoss(reduction='none')\n",
    "        self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=20)\n",
    "        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=20)\n",
    "        self.debugging = debugging\n",
    "        self.debugging_hook = debugging_hook\n",
    "        self.proj_viz_tfm = proj_viz_tfm\n",
    "\n",
    "        self.automatic_optimization = not debugging\n",
    "        \n",
    "        for n, m in self.model.named_modules():\n",
    "            assert not hasattr(m, 'name')\n",
    "            m.name = n\n",
    "\n",
    "    def step_routine(self, batch, batch_idx, stage, metric):\n",
    "        img, label, mask = batch\n",
    "        label[~mask] = 0\n",
    "\n",
    "        norm_img = self.input_norm(img)\n",
    "        pred = self.model(norm_img)\n",
    "        \n",
    "        loss = self.loss_fn(pred, label)\n",
    "        loss = loss[mask]\n",
    "        max_loss = loss.max()\n",
    "        loss = loss.mean()\n",
    "\n",
    "        pred_f = torch.permute(pred, (0, 2, 3, 1))\n",
    "        pred_f = torch.flatten(pred_f, 0, -2)\n",
    "        mask_f = torch.flatten(mask)\n",
    "        pred_m = pred_f[mask_f, :]\n",
    "        label_m = label[mask]\n",
    "        metric(pred_m, label_m)\n",
    "\n",
    "        self.log(f\"{stage}_loss_max\", max_loss)\n",
    "        self.log(f\"{stage}_acc_step\", metric)\n",
    "        self.log(f\"{stage}_loss\", loss)\n",
    "\n",
    "        if stage==\"train\" and self.debugging:\n",
    "            wandb_logger = self.logger.experiment\n",
    "            if self.global_step % 100 == 0:\n",
    "                with torch.nn.modules.module.register_module_forward_hook(\n",
    "                    self.debugging_hook(wandb_logger, self.global_step)\n",
    "                ):\n",
    "                    self.model(norm_img)  # run forward pass on current batch to log activations\n",
    "            if self.global_step % 500 == 0:\n",
    "                pred_np = pred[0].detach().cpu().numpy().argmax(0)\n",
    "                label_np = label[0].detach().cpu().numpy()\n",
    "                mask_np = mask[0].detach().cpu().numpy()\n",
    "                _, label_img, _ = self.proj_viz_tfm(None, label_np, mask_np)\n",
    "                _, pred_img, _ = self.proj_viz_tfm(None, pred_np, mask_np)\n",
    "                img_cmp = np.concatenate((pred_img, label_img), axis=0)\n",
    "                img_cmp = wandb.Image(img_cmp)\n",
    "                wandb_logger.log({\"examples\": img_cmp}, step=self.global_step)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.step_routine(batch, batch_idx, \"train\", self.train_accuracy)\n",
    "        if not self.debugging:\n",
    "            return loss\n",
    "        optimizer = self.optimizers()\n",
    "        optimizer.zero_grad()\n",
    "        self.manual_backward(loss)\n",
    "        wandb_logger = self.logger.experiment\n",
    "        step = self.global_step\n",
    "        p_old = {}\n",
    "        for n, p in self.model.named_parameters():\n",
    "            p_old[n] = p.detach().clone()\n",
    "        optimizer.step()\n",
    "        for n, p in self.model.named_parameters():\n",
    "            optim_step = p.detach() - p_old[n]\n",
    "            wandb_logger.log({\"ud/\" + str(n): (optim_step.std()/(p_old[n].std() + 1e-5)).log10()}, step=step)\n",
    "        lr_scheduler = self.lr_schedulers()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.log('train_acc_epoch', self.train_accuracy)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.step_routine(batch, batch_idx, \"val\", self.val_accuracy)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.log('val_acc_epoch', self.val_accuracy)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = 1e-3\n",
    "        optimizer = Adam(self.model.parameters(), lr=lr, eps=1e-5)\n",
    "        lr_scheduler = OneCycleLR(optimizer, lr, self.total_steps)\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4ecf63-72c5-4532-8198-f32aea044af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
