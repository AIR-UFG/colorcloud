{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11c69cc-d200-4710-addf-8062129cfe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec6d3ae-45a2-486d-9fab-a7b1b854babc",
   "metadata": {},
   "source": [
    "# biasutti2019riu\n",
    "\n",
    "> Module that implements the model from [RIU-Net: Embarrassingly simple semantic segmentation of 3D LiDAR point cloud](https://arxiv.org/abs/1905.08748)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0028db0e-08f5-4c13-a6fc-d50701159819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp biasutti2019riu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b407d042-2215-4712-9e2c-d951a8cf0fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Module, Sequential, Conv2d, BatchNorm2d, ReLU, ModuleList, ConvTranspose2d\n",
    "from torch.nn.init import kaiming_normal_, constant_, zeros_\n",
    "from torch.nn.modules.module import register_module_forward_hook\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from lightning import LightningModule\n",
    "from torchmetrics.classification import Accuracy\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import wandb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c660e6-07a3-4e55-b6f1-df2f356fb2f8",
   "metadata": {},
   "source": [
    "## RIUNet architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4818cc4f-4ebb-454d-9ab1-edeb6f5b22e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Block(Sequential):\n",
    "    \"Convolutional block repeatedly used in the RIU-Net encoder and decoder.\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__(OrderedDict([\n",
    "            (f'conv1', Conv2d(in_channels, out_channels, 3, 1, 1, bias=False, padding_mode='circular')),\n",
    "            (f'bn1', BatchNorm2d(out_channels, momentum=0.01)),\n",
    "            (f'relu1', ReLU()),\n",
    "            (f'conv2', Conv2d(out_channels, out_channels, 3, 1, 1, bias=False, padding_mode='circular')),\n",
    "            (f'bn2', BatchNorm2d(out_channels, momentum=0.01)),\n",
    "            (f'relu2', ReLU()),\n",
    "        ]))\n",
    "        self.init_params()\n",
    "    \n",
    "    def init_params(self):\n",
    "        for n, p in self.named_parameters():\n",
    "            if re.search('conv\\d\\.weight', n):\n",
    "                kaiming_normal_(p, nonlinearity='relu')\n",
    "            if re.search('bn\\d\\.bias', n):\n",
    "                constant_(p, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2893cb46-6646-4581-8683-5ffaa8425c1b",
   "metadata": {},
   "source": [
    "It implements the following architecture:\n",
    "\n",
    "```{mermaid}\n",
    "flowchart LR\n",
    "  A((\"\n",
    "  Input\n",
    "  (bs, in_c, h, w)\")) --> B[\"\n",
    "  Conv(3x3)\n",
    "  in_c -> out_c\"]\n",
    "  B --> C[\"BatchNorm2d\"]\n",
    "  C --> D[\"ReLU\"]\n",
    "  D --> E[\"\n",
    "  Conv(3x3)\n",
    "  out_c -> out_c\"]\n",
    "  E --> F[\"BatchNorm2d\"]\n",
    "  F --> G[\"ReLU\"]\n",
    "  G --> H((\"\n",
    "  Output\n",
    "  (bs, out_c, h, w)\"))\n",
    "```\n",
    "\n",
    "Here is an example on how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb46d424-aa62-4686-b746-b5f144063376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 64, 512]) == (1, 64, 64, 512)\n"
     ]
    }
   ],
   "source": [
    "bs, in_c, out_c, h, w = 1, 5, 64, 64, 512\n",
    "inp = torch.randn(bs, in_c, h, w)\n",
    "\n",
    "b = Block(in_c, out_c)\n",
    "outp = b(inp)\n",
    "assert outp.shape == (bs, out_c, h, w)\n",
    "print(outp.shape, f'== ({bs}, {out_c}, {h}, {w})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4489ac6-b568-4ec6-9f10-322f94bc911e",
   "metadata": {},
   "source": [
    "It initializes the weights from the conv layers following the [kaiming_normal_](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b453ff41-54bd-4c8b-81cf-75e58d906f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO: plot histograms of weights showing the gaussians following the kaiming algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78e0b1d-382d-42c7-b4c9-b872a1959755",
   "metadata": {},
   "source": [
    "It initializes the biases from the batch norm layers a little bit above 0 to reduce the probability of dead neurons at the beginning of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1e51fb-d9ee-48f3-a4c9-b6a26e8788a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Encoder(Module):\n",
    "    \"RIU-Net encoder architecture.\"\n",
    "    def __init__(self, channels=(5, 64, 128, 256, 512, 1024)):\n",
    "        super().__init__()\n",
    "        self.blocks = ModuleList(\n",
    "            [Block(channels[i], channels[i+1]) for i in range(len(channels)-1)]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        enc_features = []\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            enc_features.append(x)\n",
    "            x = F.max_pool2d(x, 2)\n",
    "        return enc_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cf6a19-277f-43a2-ad10-efb3a96164fb",
   "metadata": {},
   "source": [
    "It implements the following architecture:\n",
    "\n",
    "```{mermaid}\n",
    "flowchart LR\n",
    "  A((\"\n",
    "  Input\n",
    "  (bs, 5, h, w)\")) --> B[\"\n",
    "  Block\n",
    "  5 -> 64\"]\n",
    "  B --> C[\"MaxPool(2x2)\"]\n",
    "  C --> D[\"\n",
    "  Block\n",
    "  64 -> 128\"]\n",
    "  D --> E[\"MaxPool(2x2)\"]\n",
    "  E --> F[\"\n",
    "  Block\n",
    "  128 -> 256\"]\n",
    "  F --> G[\"MaxPool(2x2)\"]\n",
    "  G --> H[\"\n",
    "  Block\n",
    "  256 -> 512\"]\n",
    "  H --> I[\"MaxPool(2x2)\"]\n",
    "  I --> J[\"\n",
    "  Block\n",
    "  512 -> 1024\"]\n",
    "  B --> L((\"\n",
    "  Output\n",
    "  [(bs, 64, h, w),\n",
    "  (bs, 128, h/2, w/2),\n",
    "  (bs, 256, h/4, w/4),\n",
    "  (bs, 512, h/8, w/8),\n",
    "  (bs, 1024, h/16, w/16)]\"))\n",
    "  D --> L\n",
    "  F --> L\n",
    "  H --> L\n",
    "  J --> L\n",
    "```\n",
    "\n",
    "Here is an example on how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0961c1d1-19f7-402e-bdb5-e1b12dcff13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 64, 64, 512]),\n",
       " torch.Size([1, 128, 32, 256]),\n",
       " torch.Size([1, 256, 16, 128]),\n",
       " torch.Size([1, 512, 8, 64]),\n",
       " torch.Size([1, 1024, 4, 32])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = Encoder()\n",
    "outp = enc(inp)\n",
    "[o.shape for o in outp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e039c48-213f-4411-8d6d-884ecb8f7dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Decoder(Module):\n",
    "    \"RIU-Net decoder architecture.\"\n",
    "    def __init__(self, channels=(1024, 512, 256, 128, 64)):\n",
    "        super().__init__()\n",
    "        self.upconvs = ModuleList(\n",
    "            [ConvTranspose2d(channels[i], channels[i+1], 6, 2, 2, bias=False) for i in range(len(channels)-1)]\n",
    "        )\n",
    "        self.blocks = ModuleList(\n",
    "            [Block(channels[i], channels[i+1]) for i in range(len(channels)-1)]\n",
    "        )\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        for n, p in self.named_parameters():\n",
    "            if re.search('upconvs\\.\\d\\.weight', n):\n",
    "                kaiming_normal_(p, nonlinearity='linear')\n",
    "    \n",
    "    def forward(self, enc_features):\n",
    "        x = enc_features[-1]\n",
    "        for i, (upconv, block) in enumerate(zip(self.upconvs, self.blocks)):\n",
    "            x = upconv(x)\n",
    "            x = torch.cat([x, enc_features[-(i+2)]], dim=1)\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30318b56-525f-4c6d-8368-9ea7ce3fef25",
   "metadata": {},
   "source": [
    "It implements the following architecture:\n",
    "\n",
    "```{mermaid}\n",
    "flowchart LR\n",
    "  A((\"\n",
    "  Input\n",
    "  [(bs, 1024, h/16, w/16),\n",
    "  (bs, 512, h/8, w/8),\n",
    "  (bs, 256, h/4, w/4),\n",
    "  (bs, 128, h/2, w/2),\n",
    "  (bs, 64, h, w)]\")) --> B[\"\n",
    "  ConvTranspose(2x2)\n",
    "  1024 -> 512\"]\n",
    "  B --> C[\"\n",
    "  Block\n",
    "  concat(512,512) -> 512\"]\n",
    "  A --> C\n",
    "  C --> D[\"\n",
    "  ConvTranspose(2x2)\n",
    "  512 -> 256\"]\n",
    "  D --> E[\"\n",
    "  Block\n",
    "  concat(256,256) -> 256\"]\n",
    "  A --> E\n",
    "  E --> F[\"\n",
    "  ConvTranspose(2x2)\n",
    "  256 -> 128\"]\n",
    "  F --> G[\"\n",
    "  Block\n",
    "  concat(128,128) -> 128\"]\n",
    "  A --> G\n",
    "  G --> H[\"\n",
    "  ConvTranspose(2x2)\n",
    "  128 -> 64\"]\n",
    "  H --> I[\"\n",
    "  Block\n",
    "  concat(64,64) -> 64\"]\n",
    "  A --> I\n",
    "  I --> J((\"\n",
    "  Output\n",
    "  (bs, 64, h, w)\"))\n",
    "```\n",
    "\n",
    "Here is an example on how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95b30e7-5000-462b-8a1e-c46c9ad5f210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 64, 512]) == (1, 64, 64, 512)\n"
     ]
    }
   ],
   "source": [
    "dec = Decoder()\n",
    "fts = dec(outp)\n",
    "assert fts.shape == (bs, out_c, h, w)\n",
    "print(fts.shape, f'== ({bs}, {out_c}, {h}, {w})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2b5b8a-c437-4e65-8027-182568366723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RIUNet(Module):\n",
    "    \"RIU-Net complete architecture.\"\n",
    "    def __init__(self, in_channels=5, hidden_channels=(64, 128, 256, 512, 1024), n_classes=20):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.input_norm = BatchNorm2d(in_channels, affine=False, momentum=None)\n",
    "        self.backbone = Sequential(OrderedDict([\n",
    "            (f'enc', Encoder((in_channels, *hidden_channels))),\n",
    "            (f'dec', Decoder(hidden_channels[::-1]))\n",
    "        ]))\n",
    "        self.head = Conv2d(hidden_channels[0], n_classes, 1)\n",
    "\n",
    "    def init_params(self):\n",
    "        for n, p in self.named_parameters():\n",
    "            if re.search('head\\.weight', n):\n",
    "                kaiming_normal_(p, nonlinearity='linear')\n",
    "            if re.search('head\\.bias', n):\n",
    "                zeros_(p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_norm(x)\n",
    "        features = self.backbone(x)\n",
    "        prediction = self.head(features)\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f598dd-e2ca-4b49-90b9-8641cd2e9f99",
   "metadata": {},
   "source": [
    "It implements the following architecture:\n",
    "\n",
    "```{mermaid}\n",
    "flowchart LR\n",
    "  A((\"\n",
    "  Input\n",
    "  (bs, 5, h, w)\")) --> B[\"Encoder\"]\n",
    "  B --> C[\"Decoder\"]\n",
    "  C --> D[\"\n",
    "  Conv(1x1)\n",
    "  64 -> 20\"]\n",
    "  D --> E((\"\n",
    "  Output\n",
    "  (bs, 20, h, w)\"))\n",
    "```\n",
    "\n",
    "Here is an example on how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffcce5f-3640-4f92-9f8d-f2e47389dbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 64, 512]) == (1, 20, 64, 512)\n"
     ]
    }
   ],
   "source": [
    "n_classes=20\n",
    "model = RIUNet()\n",
    "logits = model(inp)\n",
    "assert logits.shape == (bs, n_classes, h, w)\n",
    "print(logits.shape, f'== ({bs}, {n_classes}, {h}, {w})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfe44e7-8055-48f7-8b0e-1a63dffafb9e",
   "metadata": {},
   "source": [
    "## LightningModule for standard experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f858a2-2370-4e8a-8cb7-16f72cbc8ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def log_activations(logger, step, model, img):\n",
    "    \"Function that uses a Pytorch forward hook to log properties of activations for debugging purposes.\"\n",
    "    def debugging_hook(module, inp, out):\n",
    "        if 'relu' in module.name:\n",
    "            acts = out.detach()\n",
    "            min_count = (acts < 1e-1).sum((0, 2, 3))\n",
    "            shape = acts.shape\n",
    "            total_count = shape[0]*shape[2]*shape[3]\n",
    "            rate = min_count/total_count\n",
    "            logger.log({\"max_dead_rate/\" + str(module.name): rate.max()}, step=step)\n",
    "    with register_module_forward_hook(debugging_hook):\n",
    "        model(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e597c761-ed8b-4ad6-87c2-b6d091d3eb11",
   "metadata": {},
   "source": [
    "TODO: needs proper documentation with code examples for *log_activations* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c92661-d1e9-43f3-8875-54fdd2f5248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SemanticSegmentationTask(LightningModule):\n",
    "    \"Lightning Module to standardize experiments with semantic segmentation tasks.\"\n",
    "    def __init__(self, model, loss_fn, viz_tfm, total_steps, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.viz_tfm = viz_tfm\n",
    "        self.lr = lr\n",
    "        self.total_steps = total_steps\n",
    "        self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=model.n_classes)\n",
    "        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=model.n_classes)\n",
    "        self.automatic_optimization = False\n",
    "        \n",
    "        for n, m in self.model.named_modules():\n",
    "            assert not hasattr(m, 'name')\n",
    "            m.name = n\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.model.parameters(), lr=self.lr, eps=1e-5)\n",
    "        lr_scheduler = OneCycleLR(optimizer, self.lr, self.total_steps)\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler}\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.step(batch, batch_idx, \"train\", self.train_accuracy)\n",
    "        if self.global_step % int(0.01*self.total_steps) == 0:\n",
    "            log_activations(self.logger.experiment, self.global_step, self.model, batch[0])\n",
    "        self.manual_optimization(loss)\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        self.log('train_acc_epoch', self.train_accuracy)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.step(batch, batch_idx, \"val\", self.val_accuracy)\n",
    "    \n",
    "    def step(self, batch, batch_idx, stage, metric):\n",
    "        img, label, mask = batch\n",
    "        label[~mask] = 0\n",
    "        \n",
    "        pred = self.model(img)\n",
    "        \n",
    "        loss = self.loss_fn(pred, label)\n",
    "        loss = loss[mask]\n",
    "        loss = loss.mean()\n",
    "\n",
    "        pred_f = torch.permute(pred, (0, 2, 3, 1)) # N,C,H,W -> N,H,W,C\n",
    "        pred_f = torch.flatten(pred_f, 0, -2)      # N,H,W,C -> N*H*W,C\n",
    "        mask_f = torch.flatten(mask)               # N,H,W   -> N*H*W\n",
    "        pred_m = pred_f[mask_f, :]\n",
    "        label_m = label[mask]\n",
    "        metric(pred_m, label_m)\n",
    "        \n",
    "        self.log(f\"{stage}_acc_step\", metric)\n",
    "        self.log(f\"{stage}_loss_step\", loss)\n",
    "        \n",
    "        if self.global_step % int(0.25*self.total_steps) == 0:\n",
    "            pred_np = pred[0].detach().cpu().numpy().argmax(0)\n",
    "            label_np = label[0].detach().cpu().numpy()\n",
    "            mask_np = mask[0].detach().cpu().numpy()\n",
    "            _, pred_img, _ = self.viz_tfm(None, pred_np, mask_np)\n",
    "            _, label_img, _ = self.viz_tfm(None, label_np, mask_np)\n",
    "            img_cmp = np.concatenate((pred_img, label_img), axis=0)\n",
    "            img_cmp = wandb.Image(img_cmp)\n",
    "            self.logger.experiment.log({f\"{stage}_examples\": img_cmp}, step=self.global_step)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def manual_optimization(self, loss):\n",
    "        optimizer = self.optimizers()\n",
    "        optimizer.zero_grad()\n",
    "        self.manual_backward(loss)\n",
    "        \n",
    "        p_old = {}\n",
    "        for n, p in self.model.named_parameters():\n",
    "            p_old[n] = p.detach().clone()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        for n, p in self.model.named_parameters():\n",
    "            optim_step = p.detach() - p_old[n]\n",
    "            self.logger.experiment.log({\"ud/\" + str(n): (optim_step.std()/(p_old[n].std() + 1e-5)).log10()}, step=self.global_step)\n",
    "        \n",
    "        lr_scheduler = self.lr_schedulers()\n",
    "        lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4ecf63-72c5-4532-8198-f32aea044af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
