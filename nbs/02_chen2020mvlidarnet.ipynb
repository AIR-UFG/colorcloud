{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af70967c-150b-4143-8212-882fc40d832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef7c288-d1ee-4743-8bf5-10c06bc1032b",
   "metadata": {},
   "source": [
    "# chen2020mvlidarnet\n",
    "\n",
    "> Module that implements the models from [MVLidarNet: Real-Time Multi-Class Scene Understanding for Autonomous Driving Using Multiple Views](https://arxiv.org/pdf/2006.05518)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89858be-462a-49f9-9fbb-5134c627f448",
   "metadata": {},
   "source": [
    "**(UNDER CONSTRUCTION...)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caee6fc-20c9-4775-9fa1-b563ec19c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp chen2020mvlidarnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8bfcb5-5ab7-43ed-a392-2d95071627c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "from torch.nn import Module, Sequential, Conv2d, BatchNorm2d, ReLU, ConvTranspose2d, MaxPool2d\n",
    "from torch.nn.init import kaiming_normal_, constant_, zeros_, normal_\n",
    "from collections import OrderedDict\n",
    "from lightning import LightningModule\n",
    "from torchmetrics.classification import Accuracy\n",
    "from torchmetrics.segmentation import MeanIoU\n",
    "from torch.nn.modules.module import register_module_forward_hook\n",
    "import re\n",
    "import wandb\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caabe04b-af08-49dc-9208-fff6f18446c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## MVLidarNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b395c858-045c-433e-aa86-0c2981a97596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ConvBNReLU(Sequential):\n",
    "    \"Sequential composition of convolution, batch normalization and ReLU.\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, has_ReLU=True):\n",
    "        sequence = [\n",
    "            ('conv', Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)),\n",
    "            ('bn', BatchNorm2d(out_channels, momentum=0.01))\n",
    "        ]\n",
    "        self.nonlinearity = 'linear'\n",
    "        if has_ReLU:\n",
    "            self.nonlinearity = 'relu'\n",
    "            sequence += [('relu', ReLU())]\n",
    "        super().__init__(OrderedDict(sequence))\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        for n, p in self.named_parameters():\n",
    "            if re.search('conv\\.weight', n):\n",
    "                kaiming_normal_(p, nonlinearity=self.nonlinearity)\n",
    "            if re.search('bn\\.bias', n):\n",
    "                constant_(p, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f742ca-86bc-49cc-b9b8-dcbb430b8fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 64, 2048]) == (1, 64, 64, 2048)\n"
     ]
    }
   ],
   "source": [
    "bs, in_c, out_c, h, w = 1, 5, 64, 64, 2048\n",
    "inp = torch.randn(bs, in_c, h, w)\n",
    "\n",
    "b = ConvBNReLU(in_c, out_c, 3, 1, 1)\n",
    "outp = b(inp)\n",
    "assert outp.shape == (bs, out_c, h, w)\n",
    "print(outp.shape, f'== ({bs}, {out_c}, {h}, {w})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae31d1e-d4db-4573-8e62-2bd6e8f662d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class InceptionV2(Module):\n",
    "    \"InceptionV2 Block from [Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/pdf/1512.00567).\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        out_channels = out_channels//4\n",
    "        \n",
    "        # 1x1\n",
    "        self.b1 = ConvBNReLU(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        # pool -> 1x1\n",
    "        self.b2 = Sequential(OrderedDict([\n",
    "            ('pool', MaxPool2d(kernel_size=3 , stride=1 , padding=1)),\n",
    "            ('cbr', ConvBNReLU(in_channels, out_channels, kernel_size=1, stride=1, padding=0))\n",
    "        ]))\n",
    "        \n",
    "        # 1x1 -> 3x3\n",
    "        self.b3 = Sequential(OrderedDict([\n",
    "            ('cbr1', ConvBNReLU(in_channels, out_channels, kernel_size=1, stride=1, padding=0)),\n",
    "            ('cbr2', ConvBNReLU(out_channels, out_channels, kernel_size=3, stride=1, padding=1))\n",
    "        ]))\n",
    "        \n",
    "        # 1x1 -> 3x3 -> 3x3\n",
    "        self.b4 = Sequential(OrderedDict([\n",
    "            ('cbr1', ConvBNReLU(in_channels, out_channels, kernel_size=1, stride=1, padding=0)),\n",
    "            ('cbr2', ConvBNReLU(out_channels, out_channels, kernel_size=3, stride=1, padding=1)),\n",
    "            ('cbr3', ConvBNReLU(out_channels, out_channels, kernel_size=3, stride=1, padding=1))\n",
    "        ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        branch1 = self.b1(x)\n",
    "        branch2 = self.b2(x)\n",
    "        branch3 = self.b3(x)\n",
    "        branch4 = self.b4(x)\n",
    "        return torch.cat((branch1, branch2, branch3, branch4), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04bb943-8cc7-4d0f-b6bb-848fbea59d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 64, 2048]) == (1, 64, 64, 2048)\n"
     ]
    }
   ],
   "source": [
    "b = InceptionV2(in_c, out_c)\n",
    "outp = b(inp)\n",
    "assert outp.shape == (bs, out_c, h, w)\n",
    "print(outp.shape, f'== ({bs}, {out_c}, {h}, {w})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2a7831-de9b-4ab2-9bdb-8a616522fc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class InceptionBlock(Sequential):\n",
    "    \"Sequential composition of InceptionV2 modules.\"\n",
    "    def __init__(self, in_channels, out_channels, n_modules, has_pool=False):\n",
    "        modules_in_block = []\n",
    "        for i in range(n_modules):\n",
    "            modules_in_block += [(f'incept{i+1}', InceptionV2(in_channels, out_channels))]\n",
    "            in_channels = out_channels\n",
    "        if has_pool:\n",
    "            modules_in_block += [('pool', MaxPool2d(3, 2, 1))] # kernel_size = 3, because of inception v2 paper table 1\n",
    "        super().__init__(OrderedDict(modules_in_block))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda788e1-8d6c-47e8-8dcf-2304a43b48b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 64, 2048]) == (1, 64, 64, 2048)\n"
     ]
    }
   ],
   "source": [
    "b = InceptionBlock(in_c, out_c, 2)\n",
    "outp = b(inp)\n",
    "assert outp.shape == (bs, out_c, h, w)\n",
    "print(outp.shape, f'== ({bs}, {out_c}, {h}, {w})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c78d99-a00a-40a2-a435-13e434171ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Encoder(Module):\n",
    "    \"MVLidarNet encoder architecture.\"\n",
    "    def __init__(self, in_channels=5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.trunk1 = ConvBNReLU(in_channels, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.trunk2 = ConvBNReLU(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.trunk3 = Sequential(\n",
    "            ConvBNReLU(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            MaxPool2d(3, 2, 1) # kernel_size = 3, because of inception v2 paper table 1\n",
    "        )\n",
    "        \n",
    "        self.block1 = InceptionBlock(in_channels=128, out_channels=64, n_modules=2)\n",
    "        self.block2 = InceptionBlock(in_channels=64, out_channels=64, n_modules=2, has_pool=True)\n",
    "        self.block3 = InceptionBlock(in_channels=64, out_channels=128, n_modules=3, has_pool=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        enc_features = [x]\n",
    "        \n",
    "        x = self.trunk1(x)\n",
    "        x = self.trunk2(x)\n",
    "        x = self.trunk3(x)\n",
    "        x = self.block1(x)\n",
    "        enc_features.append(x)\n",
    "        \n",
    "        x = self.block2(x)\n",
    "        enc_features.append(x)\n",
    "        \n",
    "        x = self.block3(x)\n",
    "        enc_features.append(x)\n",
    "        \n",
    "        return enc_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4553e9-3660-4f58-80b0-ab826acb0e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 5, 64, 2048]),\n",
       " torch.Size([1, 64, 32, 1024]),\n",
       " torch.Size([1, 64, 16, 512]),\n",
       " torch.Size([1, 128, 8, 256])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = Encoder()\n",
    "outp = enc(inp)\n",
    "[o.shape for o in outp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ad0c1a-b7c5-4e09-a56c-c2c2f131bc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Decoder(Module):\n",
    "    \"MVLidarNet decoder architecture.\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.up1a = ConvTranspose2d(128, 256, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.up1c = ConvBNReLU(256+64, 256, kernel_size=1, stride=1, padding=0)\n",
    "        self.up1d = ConvBNReLU(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.up2a = ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.up2c = ConvBNReLU(128+64, 128, kernel_size=1, stride=1, padding=0)\n",
    "        self.up2d = ConvBNReLU(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.up3a = ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.up3b = ConvBNReLU(64, 64, kernel_size=1, stride=1, padding=0)\n",
    "        self.up3c = ConvBNReLU(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        for n, p in self.named_parameters():\n",
    "            if re.search('up\\da\\.weight', n):\n",
    "                kaiming_normal_(p, nonlinearity='linear')\n",
    "    \n",
    "    def forward(self, enc_features):\n",
    "        x = enc_features[-1]\n",
    "        \n",
    "        x = self.up1a(x, output_size=enc_features[-2].size())\n",
    "        up1b = torch.cat((x, enc_features[-2]), dim=1)\n",
    "        x = self.up1c(up1b)\n",
    "        x = self.up1d(x)\n",
    "        \n",
    "        x = self.up2a(x, output_size=enc_features[-3].size())\n",
    "        up2b = torch.cat((x, enc_features[-3]), dim=1)\n",
    "        x = self.up2c(up2b)\n",
    "        x = self.up2d(x)\n",
    "        \n",
    "        x = self.up3a(x, output_size=enc_features[-4].size())\n",
    "        x = self.up3b(x)\n",
    "        x = self.up3c(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f88b1a-1341-430d-988a-0fdcd3690715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 64, 2048]) == (1, 64, 64, 2048)\n"
     ]
    }
   ],
   "source": [
    "dec = Decoder()\n",
    "fts = dec(outp)\n",
    "assert fts.shape == (bs, out_c, h, w)\n",
    "print(fts.shape, f'== ({bs}, {out_c}, {h}, {w})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296e4c68-ab4b-4e3f-ad81-4423f39fe93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MVLidarNet(Module):\n",
    "    \"MVLidarNet semantic segmentation architecture.\"\n",
    "    def __init__(self, in_channels=5, n_classes=7):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.input_norm = BatchNorm2d(in_channels, affine=False, momentum=None)\n",
    "        self.backbone = Sequential(OrderedDict([\n",
    "            ('enc', Encoder(in_channels)),\n",
    "            ('dec', Decoder())\n",
    "        ]))\n",
    "        \n",
    "        self.classhead1 = ConvBNReLU(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.classhead2 = ConvBNReLU(64, n_classes, kernel_size=1, stride=1, padding=0, has_ReLU=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_norm(x)\n",
    "        features = self.backbone(x)\n",
    "        x = self.classhead1(features)\n",
    "        prediction = self.classhead2(x)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90474418-8770-46ed-ac61-03b032161aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 64, 2048]) == (1, 7, 64, 2048)\n"
     ]
    }
   ],
   "source": [
    "n_classes=7\n",
    "model = MVLidarNet()\n",
    "logits = model(inp)\n",
    "assert logits.shape == (bs, n_classes, h, w)\n",
    "print(logits.shape, f'== ({bs}, {n_classes}, {h}, {w})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d3d815-b1f9-4607-8db0-4694b8b9b76f",
   "metadata": {},
   "source": [
    "## LightningModule for standard experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436f2907-8076-488b-88b0-5d0d5a7bd7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def log_activations(logger, step, model, img):\n",
    "    \"Function that uses a Pytorch forward hook to log properties of activations for debugging purposes.\"\n",
    "    def debugging_hook(module, inp, out):            \n",
    "        if hasattr(module, 'name') and 'relu' in module.name:\n",
    "            acts = out.detach()\n",
    "            \n",
    "            min_count = (acts < 1e-1).sum((0, 2, 3))\n",
    "            shape = acts.shape\n",
    "            total_count = shape[0]*shape[2]*shape[3]\n",
    "            rate = min_count/total_count\n",
    "            logger.log({\"max_dead_rate/\" + str(module.name): rate.max()}, step=step)\n",
    "            \n",
    "            #acts_flat = acts.cpu().view(-1,)\n",
    "            #acts_hist = np.histogram(acts_flat.log1p(), 100)\n",
    "            #logger.log({\"relu_hist/\" + str(module.name): wandb.Histogram(np_histogram=acts_hist)}, step=step)\n",
    "            \n",
    "    with register_module_forward_hook(debugging_hook):\n",
    "        model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8866414-f2ef-49d4-baa8-b261be41123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def log_imgs(pred, label, mask, viz_tfm, logger, stage, step, save_image_locally=True):\n",
    "    \"\"\"\n",
    "    Logs predicted and ground truth images during model training/evaluation, \n",
    "    optionally saving them locally.\n",
    "    \"\"\"\n",
    "    def to_numpy(tensor):\n",
    "        return tensor[0].detach().cpu().numpy()\n",
    "    \n",
    "    pred_np = to_numpy(pred).argmax(0)\n",
    "    label_np = to_numpy(label)\n",
    "    mask_np = to_numpy(mask)\n",
    "\n",
    "    altered_pred_np = pred_np.copy()\n",
    "    # Set matching predictions to 0\n",
    "    altered_pred_np[altered_pred_np == label_np] = 0\n",
    "    \n",
    "    item = {\n",
    "        \"frame\": None,\n",
    "        \"label\": pred_np,\n",
    "        \"mask\": mask_np,\n",
    "        \"weight\": None\n",
    "    }\n",
    "    pred_img = viz_tfm(item)[\"label\"]\n",
    "\n",
    "    item[\"label\"] = altered_pred_np\n",
    "    altered_pred_img = viz_tfm(item)[\"label\"]\n",
    "    \n",
    "    item[\"label\"] = label_np\n",
    "    label_img = viz_tfm(item)[\"label\"]\n",
    "    \n",
    "    img_cmp = np.concatenate((label_img, altered_pred_img, pred_img), axis=0)\n",
    "    \n",
    "    img_cmp_wandb = wandb.Image(img_cmp)\n",
    "    logger.log({f\"{stage}_examples\": img_cmp_wandb}, step=step)\n",
    "\n",
    "    if save_image_locally:\n",
    "        img_cmp = img_cmp.astype(np.uint8)\n",
    "        img_cmp_pil = Image.fromarray(img_cmp)\n",
    "        save_path = f\"../../step_images/{stage}examples_step{step}.png\"\n",
    "        img_cmp_pil.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c878a3-6c59-47ca-88c6-78f1b3974a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class SemanticSegmentationTask(LightningModule):\n",
    "    \"Lightning Module to standardize experiments with semantic segmentation tasks.\"\n",
    "    def __init__(self, model, loss_fn, viz_tfm, total_steps, lr=5e-4):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.viz_tfm = viz_tfm\n",
    "        self.lr = lr\n",
    "        self.total_steps = total_steps\n",
    "        self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=model.n_classes)\n",
    "        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=model.n_classes)\n",
    "        self.train_miou = MeanIoU(num_classes=13)\n",
    "        self.val_miou = MeanIoU(num_classes=13)\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        self.step_idx = 0\n",
    "        \n",
    "        # for n, m in self.model.named_modules():\n",
    "        #     assert not hasattr(m, 'name')\n",
    "        #     m.name = n\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.model.parameters(), lr=self.lr, eps=1e-5)\n",
    "        lr_scheduler = OneCycleLR(optimizer, self.lr, self.total_steps)\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler}\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        stage = 'train'\n",
    "        logger = self.logger.experiment\n",
    "\n",
    "        step_01 = max(int(0.01 * self.total_steps), 1)\n",
    "        step_25 = max(int(0.25 * self.total_steps), 1)\n",
    "\n",
    "        metrics = {\"accuracy\": self.train_accuracy, \"miou\": self.train_miou}\n",
    "        loss, pred, label, mask = self.step(batch, batch_idx, stage, metrics)\n",
    "        \n",
    "        if self.step_idx % step_01 == 0:\n",
    "            log_activations(logger, self.step_idx, self.model, batch['frame'])\n",
    "        if self.step_idx % step_25 == 0:\n",
    "            log_imgs(pred, label, mask, self.viz_tfm, logger, stage, self.step_idx)\n",
    "        self.manual_optimization(loss)\n",
    "        self.step_idx += 1\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        self.log('train_acc_epoch', self.train_accuracy)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        stage = 'val'\n",
    "        logger = self.logger.experiment\n",
    "\n",
    "        metrics = {\"accuracy\": self.val_accuracy, \"miou\": self.val_miou}\n",
    "        _, pred, label, mask = self.step(batch, batch_idx, stage, metrics)\n",
    "        if batch_idx == 0:\n",
    "            log_imgs(pred, label, mask, self.viz_tfm, logger, stage, self.step_idx)\n",
    "    \n",
    "    def step(self, batch, batch_idx, stage, metrics):\n",
    "        item = batch\n",
    "        img = item['frame']\n",
    "        label = item['label']\n",
    "        mask = item['mask']\n",
    "\n",
    "        label[~mask] = 0\n",
    "        \n",
    "        pred = self.model(img)\n",
    "        \n",
    "        loss = self.loss_fn(pred, label)\n",
    "        loss = loss[mask]\n",
    "        loss = loss.mean()\n",
    "        \n",
    "        pred_f = torch.permute(pred, (0, 2, 3, 1)) # N,C,H,W -> N,H,W,C\n",
    "        pred_f = torch.flatten(pred_f, 0, -2)      # N,H,W,C -> N*H*W,C\n",
    "        mask_f = torch.flatten(mask)               # N,H,W   -> N*H*W\n",
    "        pred_m = pred_f[mask_f, :]\n",
    "        label_m = label[mask]\n",
    "        metrics[\"accuracy\"](pred_m, label_m)\n",
    "        \n",
    "        pred_labels = torch.argmax(pred, dim=1)\n",
    "        mask_miou = (label != 0)\n",
    "        pred_labels[~mask] = 0\n",
    "        metrics[\"miou\"](pred_labels, label)\n",
    "\n",
    "        self.log(f\"{stage}_acc_step\", metrics[\"accuracy\"])\n",
    "        self.log(f\"{stage}_mean_iou_step\", metrics[\"miou\"])\n",
    "        self.log(f\"{stage}_loss_step\", loss.log10())\n",
    "\n",
    "        return loss, pred, label, mask\n",
    "    \n",
    "    def manual_optimization(self, loss):\n",
    "        optimizer = self.optimizers()\n",
    "        optimizer.zero_grad()\n",
    "        self.manual_backward(loss)\n",
    "        \n",
    "        p_old = {}\n",
    "        for n, p in self.model.named_parameters():\n",
    "            p_old[n] = p.detach().clone()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        for n, p in self.model.named_parameters():\n",
    "            optim_step = p.detach() - p_old[n]\n",
    "            \n",
    "            #log_rate = optim_step.abs().log1p() - p_old[n].abs().log1p()\n",
    "            #log_rate_hist = np.histogram(log_rate.cpu().view(-1), 100)\n",
    "            #self.logger.experiment.log({\"log_update_rate_hist/\" + str(n): wandb.Histogram(np_histogram=log_rate_hist)}, step=self.step_idx)\n",
    "            \n",
    "            self.logger.experiment.log({\"ud/\" + str(n): ((optim_step.std())/(p_old[n].std() + 1e-5)).log10()}, step=self.step_idx)\n",
    "        \n",
    "        lr_scheduler = self.lr_schedulers()\n",
    "        lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c97882a9-4746-4ca2-9bf7-0d3e4b647cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
