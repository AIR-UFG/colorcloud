{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af70967c-150b-4143-8212-882fc40d832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef7c288-d1ee-4743-8bf5-10c06bc1032b",
   "metadata": {},
   "source": [
    "# chen2020mvlidarnet\n",
    "\n",
    "> Module that implements the models from [MVLidarNet: Real-Time Multi-Class Scene Understanding for Autonomous Driving Using Multiple Views](https://arxiv.org/pdf/2006.05518)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89858be-462a-49f9-9fbb-5134c627f448",
   "metadata": {},
   "source": [
    "**(UNDER CONSTRUCTION...)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caee6fc-20c9-4775-9fa1-b563ec19c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp chen2020mvlidarnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8bfcb5-5ab7-43ed-a392-2d95071627c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "from torch.nn import Module, Sequential, Conv2d, BatchNorm2d, ReLU, ConvTranspose2d, MaxPool2d\n",
    "from torch.nn.init import kaiming_normal_, constant_, zeros_\n",
    "from collections import OrderedDict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b395c858-045c-433e-aa86-0c2981a97596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ConvBNReLU(Sequential):\n",
    "    \"Sequential composition of convolution, batch normalization and ReLU.\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, has_ReLU=True):\n",
    "        sequence = [\n",
    "            ('conv', Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)),\n",
    "            ('bn', BatchNorm2d(out_channels, momentum=0.01))\n",
    "        ]\n",
    "        self.nonlinearity = 'linear'\n",
    "        if has_ReLU:\n",
    "            self.nonlinearity = 'relu'\n",
    "            sequence += [('relu', ReLU())]\n",
    "        super().__init__(OrderedDict(sequence))\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        for n, p in self.named_parameters():\n",
    "            if re.search('conv\\.weight', n):\n",
    "                kaiming_normal_(p, nonlinearity=self.nonlinearity)\n",
    "            if re.search('bn\\.bias', n):\n",
    "                constant_(p, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f742ca-86bc-49cc-b9b8-dcbb430b8fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 64, 2048]) == (1, 64, 64, 2048)\n"
     ]
    }
   ],
   "source": [
    "bs, in_c, out_c, h, w = 1, 5, 64, 64, 2048\n",
    "inp = torch.randn(bs, in_c, h, w)\n",
    "\n",
    "b = ConvBNReLU(in_c, out_c, 3, 1, 1)\n",
    "outp = b(inp)\n",
    "assert outp.shape == (bs, out_c, h, w)\n",
    "print(outp.shape, f'== ({bs}, {out_c}, {h}, {w})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae31d1e-d4db-4573-8e62-2bd6e8f662d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class InceptionV2(Module):\n",
    "    \"InceptionV2 Block from [Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/pdf/1512.00567).\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        out_channels = out_channels//4\n",
    "        \n",
    "        # 1x1\n",
    "        self.b1 = ConvBNReLU(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        # pool -> 1x1\n",
    "        self.b2 = Sequential(OrderedDict([\n",
    "            ('pool', MaxPool2d(kernel_size=3 , stride=1 , padding=1)),\n",
    "            ('cbr', ConvBNReLU(in_channels, out_channels, kernel_size=1, stride=1, padding=0))\n",
    "        ]))\n",
    "        \n",
    "        # 1x1 -> 3x3\n",
    "        self.b3 = Sequential(OrderedDict([\n",
    "            ('cbr1', ConvBNReLU(in_channels, out_channels, kernel_size=1, stride=1, padding=0)),\n",
    "            ('cbr2', ConvBNReLU(out_channels, out_channels, kernel_size=3, stride=1, padding=1))\n",
    "        ]))\n",
    "        \n",
    "        # 1x1 -> 3x3 -> 3x3\n",
    "        self.b4 = Sequential(OrderedDict([\n",
    "            ('cbr1', ConvBNReLU(in_channels, out_channels, kernel_size=1, stride=1, padding=0)),\n",
    "            ('cbr2', ConvBNReLU(out_channels, out_channels, kernel_size=3, stride=1, padding=1)),\n",
    "            ('cbr3', ConvBNReLU(out_channels, out_channels, kernel_size=3, stride=1, padding=1))\n",
    "        ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        branch1 = self.b1(x)\n",
    "        branch2 = self.b2(x)\n",
    "        branch3 = self.b3(x)\n",
    "        branch4 = self.b4(x)\n",
    "        return torch.cat((branch1, branch2, branch3, branch4), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04bb943-8cc7-4d0f-b6bb-848fbea59d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 64, 2048]) == (1, 64, 64, 2048)\n"
     ]
    }
   ],
   "source": [
    "b = InceptionV2(in_c, out_c)\n",
    "outp = b(inp)\n",
    "assert outp.shape == (bs, out_c, h, w)\n",
    "print(outp.shape, f'== ({bs}, {out_c}, {h}, {w})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2a7831-de9b-4ab2-9bdb-8a616522fc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class InceptionBlock(Sequential):\n",
    "    \"Sequential composition of InceptionV2 modules.\"\n",
    "    def __init__(self, in_channels, out_channels, n_modules, has_pool=False):\n",
    "        modules_in_block = []\n",
    "        for i in range(n_modules):\n",
    "            modules_in_block += [(f'incept{i+1}', InceptionV2(in_channels, out_channels))]\n",
    "            in_channels = out_channels\n",
    "        if has_pool:\n",
    "            modules_in_block += [('pool', MaxPool2d(3, 2, 1))] # kernel_size = 3, because of inception v2 paper table 1\n",
    "        super().__init__(OrderedDict(modules_in_block))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda788e1-8d6c-47e8-8dcf-2304a43b48b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 64, 2048]) == (1, 64, 64, 2048)\n"
     ]
    }
   ],
   "source": [
    "b = InceptionBlock(in_c, out_c, 2)\n",
    "outp = b(inp)\n",
    "assert outp.shape == (bs, out_c, h, w)\n",
    "print(outp.shape, f'== ({bs}, {out_c}, {h}, {w})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c78d99-a00a-40a2-a435-13e434171ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Encoder(Module):\n",
    "    \"MVLidarNet encoder architecture.\"\n",
    "    def __init__(self, in_channels=5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.trunk1 = ConvBNReLU(in_channels, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.trunk2 = ConvBNReLU(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.trunk3 = Sequential(\n",
    "            ConvBNReLU(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            MaxPool2d(3, 2, 1) # kernel_size = 3, because of inception v2 paper table 1\n",
    "        )\n",
    "        \n",
    "        self.block1 = InceptionBlock(in_channels=128, out_channels=64, n_modules=2)\n",
    "        self.block2 = InceptionBlock(in_channels=64, out_channels=64, n_modules=2, has_pool=True)\n",
    "        self.block3 = InceptionBlock(in_channels=64, out_channels=128, n_modules=3, has_pool=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        enc_features = [x]\n",
    "        \n",
    "        x = self.trunk1(x)\n",
    "        x = self.trunk2(x)\n",
    "        x = self.trunk3(x)\n",
    "        x = self.block1(x)\n",
    "        enc_features.append(x)\n",
    "        \n",
    "        x = self.block2(x)\n",
    "        enc_features.append(x)\n",
    "        \n",
    "        x = self.block3(x)\n",
    "        enc_features.append(x)\n",
    "        \n",
    "        return enc_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4553e9-3660-4f58-80b0-ab826acb0e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 5, 64, 2048]),\n",
       " torch.Size([1, 64, 32, 1024]),\n",
       " torch.Size([1, 64, 16, 512]),\n",
       " torch.Size([1, 128, 8, 256])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = Encoder()\n",
    "outp = enc(inp)\n",
    "[o.shape for o in outp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ad0c1a-b7c5-4e09-a56c-c2c2f131bc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Decoder(Module):\n",
    "    \"MVLidarNet decoder architecture.\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.up1a = ConvTranspose2d(128, 256, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.up1c = ConvBNReLU(256+64, 256, kernel_size=1, stride=1, padding=0)\n",
    "        self.up1d = ConvBNReLU(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.up2a = ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.up2c = ConvBNReLU(128+64, 128, kernel_size=1, stride=1, padding=0)\n",
    "        self.up2d = ConvBNReLU(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.up3a = ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.up3b = ConvBNReLU(64, 64, kernel_size=1, stride=1, padding=0)\n",
    "        self.up3c = ConvBNReLU(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        for n, p in self.named_parameters():\n",
    "            if re.search('up\\da\\.weight', n):\n",
    "                kaiming_normal_(p, nonlinearity='linear')\n",
    "    \n",
    "    def forward(self, enc_features):\n",
    "        x = enc_features[-1]\n",
    "        \n",
    "        x = self.up1a(x, output_size=enc_features[-2].size())\n",
    "        up1b = torch.cat((x, enc_features[-2]), dim=1)\n",
    "        x = self.up1c(up1b)\n",
    "        x = self.up1d(x)\n",
    "        \n",
    "        x = self.up2a(x, output_size=enc_features[-3].size())\n",
    "        up2b = torch.cat((x, enc_features[-3]), dim=1)\n",
    "        x = self.up2c(up2b)\n",
    "        x = self.up2d(x)\n",
    "        \n",
    "        x = self.up3a(x, output_size=enc_features[-4].size())\n",
    "        x = self.up3b(x)\n",
    "        x = self.up3c(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f88b1a-1341-430d-988a-0fdcd3690715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 64, 2048]) == (1, 64, 64, 2048)\n"
     ]
    }
   ],
   "source": [
    "dec = Decoder()\n",
    "fts = dec(outp)\n",
    "assert fts.shape == (bs, out_c, h, w)\n",
    "print(fts.shape, f'== ({bs}, {out_c}, {h}, {w})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296e4c68-ab4b-4e3f-ad81-4423f39fe93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MVLidarNet(Module):\n",
    "    \"MVLidarNet semantic segmentation architecture.\"\n",
    "    def __init__(self, in_channels=5, n_classes=7):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.input_norm = BatchNorm2d(in_channels, affine=False, momentum=None)\n",
    "        self.backbone = Sequential(OrderedDict([\n",
    "            ('enc', Encoder(in_channels)),\n",
    "            ('dec', Decoder())\n",
    "        ]))\n",
    "        \n",
    "        self.classhead1 = ConvBNReLU(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.classhead2 = ConvBNReLU(64, n_classes, kernel_size=1, stride=1, padding=0, has_ReLU=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_norm(x)\n",
    "        features = self.backbone(x)\n",
    "        x = self.classhead1(features)\n",
    "        prediction = self.classhead2(x)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90474418-8770-46ed-ac61-03b032161aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 64, 2048]) == (1, 7, 64, 2048)\n"
     ]
    }
   ],
   "source": [
    "n_classes=7\n",
    "model = MVLidarNet()\n",
    "logits = model(inp)\n",
    "assert logits.shape == (bs, n_classes, h, w)\n",
    "print(logits.shape, f'== ({bs}, {n_classes}, {h}, {w})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97882a9-4746-4ca2-9bf7-0d3e4b647cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
