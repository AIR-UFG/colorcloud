{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e6b0541-5754-4aec-872a-04075bd1025b",
   "metadata": {},
   "source": [
    "# UFGsim2024infufg\n",
    "> Module to handle loading, preprocessing and postprocessing of the data from the simulated world for UFG's dataset made by the AIR-UFG perception team\n",
    ">\n",
    "> Still needs proper documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133aa0c5-ef61-4842-8460-4a15a57b5464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp UFGsim2024infufg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61cfbf1-6c48-42bb-8f40-136f89ffe6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "import torch\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch import nn\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from lightning import LightningDataModule\n",
    "from torchvision.transforms import v2\n",
    "from colorcloud.behley2019iccv import SphericalProjection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2172e4c5-4077-4b61-ae57-0dbbf5333379",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This is a conversion tool of the simulated dataset for UFG's autonomous driving perception software project as a pytorch [Dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n",
    "\n",
    "Some important notes:\n",
    "1. The Dataset:\n",
    "   > - Currently there is 13 labels representing vehicles, buildings and vegetation. You can check them in the yaml file.\n",
    "   > - Since the dataset is built in a simulated world, the point clouds are converted from rosbags into PCD files and then from PCD to '.bin' files for proper pre-processing. In case you want to know how this was done, the tool used for the conversion can be found in the [Point Cloud Convertion Tools](https://github.com/AIR-UFG/pointcloud_conversion_tools) repository.\n",
    "   > - The current lidar shape used for the Dataset construction is (h,w) = (16x440).\n",
    "2. The dataset files **must** be organized as follows:\n",
    "    - data\n",
    "      - laser_scans\n",
    "          - laser0\n",
    "            - pcd1.bin\n",
    "            - pcd2.bin\n",
    "            - ...\n",
    "          - laser1\n",
    "            - pcd1.bin\n",
    "            - pcd2.bin\n",
    "            - ...\n",
    "          - ...\n",
    "      - ufg-sim.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7fccfe-da14-4c67-aed9-a8e1de5ad7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class UFGSimDataset(Dataset):\n",
    "    \"Load the UFGSim dataset ina pytorch Dataset object.\"\n",
    "    def __init__(self, data_path, split='train', transform=None):\n",
    "        data_path = Path(data_path)\n",
    "        yaml_path = data_path/'ufg-sim.yaml'\n",
    "        self.ufgsim_velodyne_path = data_path/'laser_scans'\n",
    "\n",
    "        with open(yaml_path, 'r') as file:\n",
    "            metadata = yaml.safe_load(file)\n",
    "\n",
    "        lasers = metadata['split'][split]\n",
    "        self.labels_dict = metadata['labels']\n",
    "\n",
    "        ufgsim_velodyne_fns = []\n",
    "        for laser in lasers:\n",
    "            ufgsim_velodyne_fns += list(self.ufgsim_velodyne_path.rglob(f'laser{laser}/*.bin'))\n",
    "  \n",
    "        self.frame_ids = [fn.stem for fn in sorted(ufgsim_velodyne_fns)]\n",
    "        self.frame_lasers = [fn.parts[-2] for fn in sorted(ufgsim_velodyne_fns)]\n",
    "\n",
    "        self.content = metadata['content']\n",
    "        max_key = sorted(self.content.keys())[-1]\n",
    "        self.content_np = np.zeros((max_key+1,), dtype=np.float32)\n",
    "        for k, v in self.content.items():\n",
    "            self.content_np[k] = v\n",
    "\n",
    "        self.learning_map = metadata['learning_map']\n",
    "        max_key = sorted(self.learning_map.keys())[-1]\n",
    "        self.learning_map_np = np.zeros((max_key+1,), dtype=int)\n",
    "        for k, v in self.learning_map.items():\n",
    "            self.learning_map_np[k] = v\n",
    "        \n",
    "        self.learning_map_inv = metadata['learning_map_inv']\n",
    "        self.learning_map_inv_np = np.zeros((len(self.learning_map_inv),))\n",
    "        self.content_sum_np = np.zeros_like(self.learning_map_inv_np, dtype=np.float32)\n",
    "        for k, v in self.learning_map_inv.items():\n",
    "            self.learning_map_inv_np[k] = v\n",
    "            self.content_sum_np[k] = self.content_np[self.learning_map_np == k].sum()\n",
    "        \n",
    "        self.color_map_bgr = metadata['color_map']\n",
    "        max_key = sorted(self.color_map_bgr.keys())[-1]\n",
    "        self.color_map_rgb_np = np.zeros((max_key+1,3))\n",
    "        for k,v in self.color_map_bgr.items():\n",
    "            self.color_map_rgb_np[k] = np.array(v[::-1], np.float32)\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.is_test = (split == 'test')\n",
    "\n",
    "\n",
    "    def learning_remap(self, remapping_rules):\n",
    "        new_map_np = np.zeros_like(self.learning_map_np, dtype=int)\n",
    "        max_key = sorted(remapping_rules.values())[-1]\n",
    "        new_map_inv_np = np.zeros((max_key+1,), dtype=int)\n",
    "        for k, v in remapping_rules.items():\n",
    "            new_map_np[self.learning_map_np == k] = v\n",
    "            if new_map_inv_np[v] == 0:\n",
    "                new_map_inv_np[v] = self.learning_map_inv_np[k]\n",
    "        self.learning_map_np = new_map_np\n",
    "        self.learning_map_inv_np = new_map_inv_np\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.frame_ids)\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        frame_id = self.frame_ids[idx]\n",
    "        frame_laser = self.frame_lasers[idx]\n",
    "\n",
    "        frame_path = self.ufgsim_velodyne_path/frame_laser/(frame_id + '.bin')\n",
    "\n",
    "        with open(frame_path, 'rb') as f:\n",
    "            frame = np.fromfile(f, dtype=np.float32).reshape(-1, 4)\n",
    "            x_frame = frame[:, 0]\n",
    "            y_frame = frame[:, 1]\n",
    "            z_frame = frame[:, 2]\n",
    "            label = frame[:, 3].astype(np.uint32)\n",
    "\n",
    "        label = self.learning_map_np[label]\n",
    "        mask = label != 0\n",
    "        weight = 1./self.content_sum_np[label]\n",
    "\n",
    "        item = {\n",
    "            'frame': frame,\n",
    "            'label': label,\n",
    "            'mask': mask,\n",
    "            'weight': weight\n",
    "        }\n",
    "        if self.transform:\n",
    "            item = self.transform(item)\n",
    "        # return x_frame, y_frame, z_frame, label # in case of separated training \n",
    "        return item\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de188721-ee05-47e5-90f4-9b955d0ba56c",
   "metadata": {},
   "source": [
    "Similar to the [semantic-kitti-api](https://github.com/PRBonn/semantic-kitti-api) method, we use a yaml file for label description and annotation as folllows:\n",
    "> - `labels`: dictionary which maps numeric labels [...] to a string class. Example: `10: \"car\"`\n",
    "> - `color_map`: dictionary which maps numeric labels [...] to a bgr color for visualization. Example `10: [245, 150, 100] # car, blue-ish`\n",
    "> - `learning_map`: dictionary which maps each class label to its cross entropy equivalent, for learning. This is done to mask undesired classes, map different classes together, and because the cross entropy expects a value in [0, numclasses - 1]. [...] Examples:\n",
    ">   ```yaml\n",
    ">     0 : 0     # \"unlabeled\"\n",
    ">     1 : 0     # \"outlier\" to \"unlabeled\" -> gets ignored in training, with unlabeled\n",
    ">     10: 1     # \"car\"\n",
    ">     252: 1    # \"moving-car\" to \"car\" -> gets merged with static car class\n",
    ">   ```  \n",
    "> - `learning_map_inv`: dictionary with inverse of the previous mapping [...] (for saving point cloud predictions in original label format [and to use the color_map dictionary for visualization]). [...]\n",
    "> - `split`: contains 3 lists, with the sequence numbers for training, validation, and evaluation.\n",
    "\n",
    "Lastly, use the following code to get the training data in its original format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7796f2a-57b1-4f23-92ca-bd8da775cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "data_path = '../UFGSim/'\n",
    "ds = UFGSimDataset(data_path)\n",
    "test_ds = UFGSimDataset(data_path, split='test')\n",
    "val_ds = UFGSimDataset(data_path, split='valid')\n",
    "print(f'train size:\\t{len(ds)}\\ntest size:\\t{len(test_ds)}\\nvalid size:\\t{len(val_ds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c00edb3-9136-460f-a5a0-9014aa13cf3f",
   "metadata": {},
   "source": [
    "In case you added/removed sequences, you'll want to update the `content` field in the __yaml__ file. In order to do that see the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5725f59e-b89d-4d24-89db-2e800d193857",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calculate_frequencies(dataset):\n",
    "    class_frequencies = {i: 0 for i in range(0, 13)}\n",
    "    \n",
    "    for item in dataset:\n",
    "        labels = item['label']\n",
    "        # Flatten the label array to count occurrences\n",
    "        flattened_labels = labels.flatten()\n",
    "        # Count the occurrences of each class\n",
    "        unique, counts = np.unique(flattened_labels, return_counts=True)\n",
    "        # Update the frequency dictionary\n",
    "        for cls, count in zip(unique, counts):\n",
    "            class_frequencies[cls] += count\n",
    "\n",
    "    # change cases where frequency is 0 to 1\n",
    "    for key, item in class_frequencies.items():\n",
    "        if item == 0:\n",
    "            class_frequencies[key] = 1\n",
    "\n",
    "    class_frequencies = list(class_frequencies.values())\n",
    "    return class_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fc4414-a29d-417e-adb7-36474d8fa336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "train_freq = calculate_frequencies(ds)\n",
    "test_freq = calculate_frequencies(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a54ed-b333-443b-a755-4316e5496227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "combined_freq = [v1 + v2 for v1,v2 in zip(train_freq,test_freq)]\n",
    "combined_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16393005-f0fb-4cc9-9028-5023b827fac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "train_pcd = [item['frame'].shape[0] * item['frame'].shape[1] for item in ds]\n",
    "test_pcd = [item['frame'].shape[0] * item['frame'].shape[1] for item in test_ds]\n",
    "combined_pcd = sum(train_pcd) + sum(test_pcd)\n",
    "combined_pcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a3c39f-03cc-4882-873e-c137bd0c9508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "content = [v1 / combined_pcd for v1 in combined_freq]\n",
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2c7a76-761e-47dd-9b3a-302062ed5473",
   "metadata": {},
   "source": [
    "Now, check if the dictionary keys are as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef22a71d-3afb-4b85-a7de-1ff854c979de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "item = ds[128]\n",
    "frame = item['frame']\n",
    "frame[:,3], frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15f304f-5106-425a-b31d-7c5d85adfe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "label = item['label']\n",
    "label, label.shape, label.dtype, set(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7377760c-1b6a-48e4-a95d-88e06384f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "mask = item['mask']\n",
    "mask, mask.shape, mask.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4387c023-d51e-4f68-bedb-cbcd21e42d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "weights = item['weight']\n",
    "weights, weights.shape, weights.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb85a814-528e-4cb9-b9c1-b45f76fa330e",
   "metadata": {},
   "source": [
    "## Lidar Range Image Projections\n",
    "In order to process the 3D LiDAR data more efficiently, there is a famous approach to convert the 3D data into 2D data, and in the case of a rotating LiDAR scan, all point can be projected to an image using a process called **spherical projection**. \n",
    "\n",
    "This concept can be found in the following papers categorized by the projection algorithm and ordered chronologically:\n",
    "> - [SqueezeSeg: Convolutional Neural Nets with Recurrent CRF for Real-Time Road-Object Segmentation from 3D LiDAR Point Cloud](https://arxiv.org/pdf/1710.07368)\n",
    "> - [Range-Image: Incorporating sensor topology for LiDAR point cloud processing](https://hal.science/hal-01756975/document)\n",
    "> - [RIU-Net: Embarrassingly simple semantic segmentation of 3D LiDAR point cloud](https://arxiv.org/pdf/1905.08748)\n",
    "\n",
    "The code for this convertion as further explanations and inspirations used in this work can be seen in the [behley2019iccv pytorch module](https://github.com/AIR-UFG/colorcloud/blob/main/nbs/00_behley2019iccv.ipynb) also made by AIR-UFG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22453ebc-3dbe-4cbd-8088-d284af80e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class ProjectionSimTransform(nn.Module):\n",
    "    def __init__(self, projection):\n",
    "        super().__init__()\n",
    "        self.projection = projection\n",
    "        self.W = projection.W\n",
    "        self.H = projection.H\n",
    "\n",
    "    def forward(self, item):\n",
    "        frame = item['frame']\n",
    "        label = item['label']\n",
    "        mask = item['mask']\n",
    "       # weight = item['weight']\n",
    "\n",
    "        scan_xyz = frame[:,:3]\n",
    "        depth = np.linalg.norm(scan_xyz, 2, axis=1)\n",
    "\n",
    "        proj_x, proj_y, outliers = self.projection.get_xy_projections(scan_xyz, depth)\n",
    "\n",
    "        # filter outliers\n",
    "        if outliers is not None:\n",
    "            proj_x = proj_x[~outliers]\n",
    "            proj_y = proj_y[~outliers]\n",
    "            scan_xyz = scan_xyz[~outliers]\n",
    "            depth = depth[~outliers]\n",
    "            if label is not None:\n",
    "                label = label[~outliers]\n",
    "                mask = mask[~outliers]\n",
    "               # weight = weight[~outliers]\n",
    "\n",
    "        order = np.argsort(depth)[::-1]\n",
    "        info_list = [scan_xyz, depth[..., np.newaxis]]\n",
    "        if label is not None:\n",
    "           # info_list += [weight[..., np.newaxis]]\n",
    "            info_list += [mask[..., np.newaxis]]\n",
    "            info_list += [label[..., np.newaxis]]\n",
    "\n",
    "        scan_info = np.concatenate(info_list, axis=-1)\n",
    "        scan_info = scan_info[order]\n",
    "        proj_y = proj_y[order]\n",
    "        proj_x = proj_x[order]\n",
    "\n",
    "\n",
    "        projections_img = np.zeros((self.H, self.W, 2+len(info_list)), dtype=np.float32)\n",
    "        projections_img[:,:,-1] -= 1\n",
    "        projections_img[proj_y, proj_x] = scan_info\n",
    "\n",
    "        if label is not None:\n",
    "            frame_img = projections_img[:,:,:-2]\n",
    "            label_img = projections_img[:,:,-1].astype(int)\n",
    "            mask_img = projections_img[:,:,-2].astype(bool)\n",
    "            mask_img = mask_img & (label_img > -1)\n",
    "           # weight_img = projections_img[:,:,-3]\n",
    "\n",
    "        else:\n",
    "            frame_img = projections_img\n",
    "            label_img = None\n",
    "            mask_img = projections_img[:,:,-1] >= 0\n",
    "           # weight_img = None\n",
    "\n",
    "        item = {\n",
    "            'frame': frame_img,\n",
    "            'label': label_img,\n",
    "            'mask': mask_img,\n",
    "            #'weight': weight_img,\n",
    "        }\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a2f9cc-c730-42eb-b37b-563d03529969",
   "metadata": {},
   "source": [
    "With the projection algorithm described previously, we can construct a `ProjectionSimTransform` object that can be used with the `UFGSimDataset`. Here is an example on how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38294e80-1394-4d88-9d0c-b679288742db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "proj = SphericalProjection(fov_up_deg=15., fov_down_deg=-15., W=440, H=16)\n",
    "tfms = ProjectionSimTransform(proj)\n",
    "ds.set_transform(tfms)\n",
    "item = ds[128]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b57842-78d4-464c-bb24-60c24c4e506a",
   "metadata": {},
   "source": [
    "As we can see below, the data is still returned as numpy arrays, but their shapes have changed as follows:\n",
    "\n",
    "- frame: from a sequence of 4D float vectors (i.e. x, y, z, labels) to an image of HxW resolution with 4 channels with float numbers (i.e. x, y, z, depth);\n",
    "- label: from a sequence of integers to an image of HxW resolution with a single channel with an integer;\n",
    "- mask: from a sequence of booleans to an image of HxW resolution with a single channel with booleans indicating which pixels should be ignored during training and inference.\n",
    "\n",
    "There are a lot of pixels in these projection images that have no corresponding LiDAR readings to be projected. Hence, these pixels were filled with:\n",
    "\n",
    "- in frame_img: 0.\n",
    "- in label_img: -1\n",
    "\n",
    "The mask image provides a convenient boolean mask to easily identify pixels with valid LiDAR readings marked as 'True' and pixels with no LiDAR readings marked as 'False'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8581c839-cf9e-4bf5-9f34-1b50ad942e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "frame_img = item['frame']\n",
    "np.unique(frame_img[:2]), shframe_img.ape, frame_img.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d862d2-8d1a-496d-acc1-f7f3329c2002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "label_img = item['label']\n",
    "label_img[:2], label_img.shape, label_img.dtype, set(label_img.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6507d7b5-6621-4ecf-814a-4f9d8a68f4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "mask_img = item['mask']\n",
    "mask_img[:2], mask_img.shape, mask_img.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb8ccf9-7729-4965-8dea-5fec990918f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class ProjectionSimVizTransform(nn.Module):\n",
    "    def __init__(self, color_map_rgb_np, learning_map_inv_np):\n",
    "        super().__init__()\n",
    "        self.color_map_rgb_np = color_map_rgb_np\n",
    "        self.learning_map_inv_np = learning_map_inv_np\n",
    "\n",
    "    def scale(self, img, min_value, max_value):\n",
    "        img = img.clip(min_value, max_value)\n",
    "        return (255.*(img - min_value)/(max_value - min_value)).astype(int)\n",
    "\n",
    "    def forward(self, item):\n",
    "        frame_img = item['frame']\n",
    "        label_img = item['label']\n",
    "        mask_img = item['mask']\n",
    "        #weight_img = item['weight']\n",
    "        \n",
    "        normalized_frame_img = None\n",
    "        if frame_img is not None:\n",
    "            x = self.scale(frame_img[:,:,0], -100., 100.)\n",
    "            y = self.scale(frame_img[:,:,1], -100., 100.)\n",
    "            z = self.scale(frame_img[:,:,2], -31., 5.)\n",
    "            d = self.scale(frame_img[:,:,3], 0., 100.)\n",
    "            normalized_frame_img = np.stack((x, y, z, d), axis=-1)\n",
    "            normalized_frame_img[mask_img == False] *= 0\n",
    "\n",
    "        colored_label_img = None\n",
    "        if label_img is not None:\n",
    "            label_img[mask_img] = self.learning_map_inv_np[label_img[mask_img]]\n",
    "            colored_label_img = np.zeros(label_img.shape + (3,))\n",
    "            colored_label_img[mask_img] = self.color_map_rgb_np[label_img[mask_img]]\n",
    "            colored_label_img = colored_label_img.astype(int)\n",
    "\n",
    "        item = {\n",
    "            'frame': normalized_frame_img,\n",
    "            'label': colored_label_img,\n",
    "            'mask': mask_img,\n",
    "            #'weight': weight_img,          \n",
    "        }\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271f1ff0-5228-422f-8a2d-ca4cba43e57c",
   "metadata": {},
   "source": [
    "In order to plot comparable visualizations of the projections, we need to standardize the scale of the float channels from the frame projection and map the label integers to their corresponding RGB color defined in the `color_map` dict from the yaml file that was downloaded before. After some experimentation with several samples from the entire dataset, the hardcoded scale values in the `ProjectionSimVizTransform.forward` method seemed to work reasonably well.\n",
    "\n",
    "Here is an example on how to use and compose the previous transforms with the `torchvision.transforms.v2` module, and then visualize the resulting images with the `matplolib.pyplot` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecef3d67-cd14-4661-aa25-c87b5e1ae6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e5dc63-57d2-4a7c-a619-aa50d8dced3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def plot_projections(img, label, channels=['x', 'y', 'z', 'r', 'd'], channels_map = {\"x\": 0, \"y\": 1, \"z\": 2, \"r\": 3, \"d\": 4}):\n",
    "    num_channels = len(channels)\n",
    "    fig_size_vertical = 2*num_channels\n",
    "    fig, axs = plt.subplots(num_channels + 1, 1, figsize=(20, fig_size_vertical))\n",
    "    \n",
    "    for i, (ax, title) in enumerate(zip(axs, channels + ['label'])):\n",
    "        if i < num_channels:\n",
    "            j = channels_map[channels[i]]\n",
    "            ax.imshow(img[:, :, j])\n",
    "        else:\n",
    "            ax.imshow(label)\n",
    "        ax.set_title(title)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303e4285-bdb8-4e14-a2fa-ef2e832672b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "proj = SphericalProjection(fov_up_deg=15., fov_down_deg=-15., W=440, H=16) \n",
    "tfms = v2.Compose([\n",
    "    ProjectionSimTransform(proj),\n",
    "    ProjectionSimVizTransform(ds.color_map_rgb_np, ds.learning_map_inv_np),\n",
    "])\n",
    "ds.set_transform(tfms)\n",
    "item = ds[128]\n",
    "\n",
    "img = item['frame']\n",
    "label = item['label']\n",
    "channels_map = {\"x\": 0, \"y\": 1, \"z\": 2, \"d\": 3}\n",
    "\n",
    "plot_projections(img, label, channels=['x', 'y', 'z', 'd'], channels_map=channels_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d41cd16-a62f-45ba-8206-bff521f7b041",
   "metadata": {},
   "source": [
    "In the next cell, we can see how long it takes on average to sample an item from this dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b512750-706e-433e-848c-461bcb4541b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "%timeit item = ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358c1aaa-567a-45a9-aed5-36f18ccec972",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class ProjectionToTensorTransformSim(nn.Module):\n",
    "    \"Pytorch transform that converts the projections from np.array to torch.tensor. It also changes the frame image format from (H, W, C) to (C, H, W).\"\n",
    "    def forward(self, item):\n",
    "        frame_img = item['frame']\n",
    "        label_img = item['label']\n",
    "        mask_img = item['mask']\n",
    "       # weight_img = item['weight']\n",
    "        \n",
    "        frame_img = np.transpose(frame_img, (2, 0, 1))\n",
    "        frame_img = torch.from_numpy(frame_img).float()\n",
    "        if label_img is not None:\n",
    "            label_img = torch.from_numpy(label_img.astype(np.int64))\n",
    "          #  weight_img = torch.from_numpy(weight_img)\n",
    "        if mask_img is not None:\n",
    "            mask_img = torch.from_numpy(mask_img)\n",
    "        \n",
    "        item = {\n",
    "            'frame': frame_img,\n",
    "            'label': label_img,\n",
    "            'mask': mask_img,\n",
    "            #'weight': weight_img,\n",
    "        }\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8b3946-6c7f-4c9b-b4a0-44f38d7a887f",
   "metadata": {},
   "source": [
    "In order to use the dataset to train pytorch models, it is necessary to transform the data from `np.array` to `torch.tensor`. Lastly, for the frame images, it is a must to transpose/permute its axis from channels **last** to channels **first**: (H,W,C) -> (C, H, W).\n",
    "\n",
    "Here is an example on how to use it with the scan spherical projection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c383533a-e220-437e-a3f9-83236ab5a9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "proj = SphericalProjection(fov_up_deg=15., fov_down_deg=-15., W=440, H=16)\n",
    "tfms = v2.Compose([\n",
    "    ProjectionSimTransform(proj),\n",
    "    ProjectionToTensorTransformSim(),\n",
    "])\n",
    "ds.set_transform(tfms)\n",
    "item = ds[127]\n",
    "img = item['frame']\n",
    "label = item['label']\n",
    "mask = item['mask']\n",
    "#weight = item['weight']\n",
    "\n",
    "print(img.shape, img.type())\n",
    "print(label.shape, label.dtype)\n",
    "print(mask.shape, mask.type())\n",
    "#print(weight.shape, weight.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608b5112-7735-40fe-8118-743bd24f671e",
   "metadata": {},
   "source": [
    "Here is an example on how to combine the classes above to implement a `torch.utils.data.Dataloader` that iterates on batches of frame images, labels and masks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46878fa6-4027-42dd-8dcd-f889e618cb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "bs = 10\n",
    "dl = DataLoader(ds, bs, num_workers=0)\n",
    "for batch in dl:\n",
    "    item = batch\n",
    "    img = item['frame']\n",
    "    label = item['label']\n",
    "    mask = item['mask']\n",
    "   # weight = item['weight']\n",
    "    print(f\"Shape of img \\t [N, C, H, W]: \\t {img.shape}\")\n",
    "    print(f\"Shape of label \\t [N, H, W]: \\t {label.shape}\")\n",
    "    print(f\"Shape of mask \\t [N, H, W]: \\t {mask.shape}\")\n",
    "   # print(f\"Shape of weight \\t [N, H, W]: \\t {weight.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349c1e4e-a219-4b20-8a30-aacbeeaf5042",
   "metadata": {},
   "source": [
    "In the next cell, we can see how long it takes on average to iterate the dataloader for 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993b30d7-f136-4d4e-989c-4c8f4f4d0c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972d125f-b8b0-4e77-939f-dd743b91225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "for batch in tqdm(dl):\n",
    "    item = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f787b1fe-74cd-49df-8b50-2be305ae37a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "ds = UFGSimDataset(data_path)\n",
    "proj = SphericalProjection(fov_up_deg=15., fov_down_deg=-15., W=440, H=16)\n",
    "tfm = ProjectionSimTransform(proj)\n",
    "ds.set_transform(tfm)\n",
    "\n",
    "bs = 1000\n",
    "dl = DataLoader(ds, bs)\n",
    "item = next(iter(dl))\n",
    "img = item['frame']\n",
    "label = item['label']\n",
    "mask = item['mask']\n",
    "\n",
    "idx = ((label <= 0).sum(axis=1) > 60).sum(axis=1).argmax()\n",
    "\n",
    "tfms = v2.Compose([\n",
    "    ProjectionSimTransform(proj),\n",
    "    ProjectionSimVizTransform(ds.color_map_rgb_np, ds.learning_map_inv_np),\n",
    "])\n",
    "ds.set_transform(tfms)\n",
    "\n",
    "item = ds[idx]\n",
    "i = item['frame']\n",
    "l = item['label']\n",
    "channels_map = {\"x\": 0, \"y\": 1, \"z\": 2, \"d\": 3}\n",
    "plot_projections(i, l, channels=['x', 'y', 'z', 'd'], channels_map=channels_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca0414f-1884-4ca9-b575-e7b652927211",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "ds.set_transform(tfm)\n",
    "\n",
    "bs = 1000\n",
    "dl = DataLoader(ds, bs, shuffle=True)\n",
    "item = next(iter(dl))\n",
    "img = item['frame']\n",
    "label = item['label']\n",
    "mask = item['mask']\n",
    "\n",
    "sparsity = (label <= 0).sum(axis=(1,2))/label[0].numel()\n",
    "plt.hist(sparsity, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3e75b6-41a8-4ec8-9057-557a4fa6edca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "l = label.flatten()\n",
    "values, counts = np.unique(l, return_counts=True)\n",
    "plt.bar(values, np.log10(counts));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eeaade-5360-47a5-ba40-ef94d34c6058",
   "metadata": {},
   "source": [
    "## Reversing the projection\n",
    "\n",
    "Since this work is meant to be used to train an autonomous car in a simulated world to serve as validation for the real world training and the training of the LiDAR 3D data is done by projecting into an image, it is necessary to reverse the projection (2D projected image to 3D LiDAR point cloud) of the training in order to be used as data for the car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782d2169-a7ba-4f96-b4b3-f5b6b8f5cf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class ProjectionSimToPointCloud(nn.Module):\n",
    "    def __init__(self, proj_fov_up=15.0, proj_fov_down=-15.0):\n",
    "        super().__init__()\n",
    "        self.proj_fov_up = proj_fov_up\n",
    "        self.proj_fov_down = proj_fov_down\n",
    "        self.fov_up = self.proj_fov_up / 180.0 * np.pi\n",
    "        self.fov_down = self.proj_fov_down / 180.0 * np.pi\n",
    "\n",
    "    def forward(self, image):\n",
    "        h, w = image[:,:,3].shape\n",
    "        points = []\n",
    "\n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                pitch = (1 - i/h) * (self.fov_up + abs(self.fov_down)) - abs(self.fov_down)\n",
    "                yaw = ((j / w) * np.pi * 2) - np.pi\n",
    "                depth = image[:,:,3][i][j]\n",
    "\n",
    "                x = depth * np.cos(yaw)\n",
    "                y = -depth * np.sin(yaw)\n",
    "                z = depth * np.sin(pitch)\n",
    "                points.append([x, y, z])\n",
    "\n",
    "        points = np.array(points)\n",
    "        return torch.tensor(points, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f101e691-bfe9-4ee2-aa70-87943607d941",
   "metadata": {},
   "source": [
    "Notice that in order to do this process, the `depth` value is needed. This is because the depth value it obtained by the (x,y,z) values. That being said, inverting the `spherical projection` equations and with some trigonometry applied to the depth value, it should be possible to retrieve the original point cloud data.\n",
    "\n",
    "Here is an example of how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e67497-740e-4eff-a49c-ce8c75687171",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "ds = UFGSimDataset(data_path)\n",
    "frame, _, _ = ds[127]\n",
    "print(f'Original point cloud:\\t{frame.shape}')\n",
    "proj = SphericalProjection(fov_up_deg=15., fov_down_deg=-15., W=440, H=16)\n",
    "tfm = ProjectionSimTransform(proj)\n",
    "ds.set_transform(tfm)\n",
    "frame, _, _ = ds[127]\n",
    "print(f'Projected image:\\t{frame.shape}')\n",
    "b2p = ProjectionSimToPointCloud()\n",
    "pcd = b2p(frame)\n",
    "print(f'Reprojected to point cloud:\\t{pcd.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fb8b73-aac2-42a2-811d-80c3877f8f07",
   "metadata": {},
   "source": [
    "## LightningDataModules for benchmarking\n",
    "\n",
    "When benchmarking different semantic segmentation algorithms, the following LightningDataModule should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ed52bd-aac6-4b45-b735-0096f0e45560",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class SemanticSegmentationSimLDM(LightningDataModule):\n",
    "    \"Lightning DataModule to facilitate reproducibility of experiments.\"\n",
    "    def __init__(self, \n",
    "                 proj_style='spherical',\n",
    "                 proj_kargs={'fov_up_deg': 15.,'fov_down_deg': -15., 'W': 440, 'H': 16,},\n",
    "                 train_batch_size=8, \n",
    "                 eval_batch_size=16,\n",
    "                 num_workers=8\n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        proj_class = {\n",
    "        #'unfold': UnfoldingProjection,\n",
    "        'spherical': SphericalProjection\n",
    "        }\n",
    "        assert proj_style in proj_class.keys()\n",
    "        self.proj = proj_class[proj_style](**proj_kargs)\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.eval_batch_size = eval_batch_size\n",
    "        self.num_workers = num_workers\n",
    "    \n",
    "    def setup(self, stage: str):\n",
    "        data_path = '../UFGSim/'\n",
    "        tfms = v2.Compose([\n",
    "            ProjectionSimTransform(self.proj),\n",
    "            ProjectionToTensorTransformSim(),\n",
    "        ])\n",
    "        split = stage\n",
    "        if stage == 'fit':\n",
    "            split = 'train'\n",
    "        else:\n",
    "            split = 'test'\n",
    "        \n",
    "        ds = UFGSimDataset(data_path, split, transform=tfms)\n",
    "        if not hasattr(self, 'viz_tfm'):\n",
    "            self.viz_tfm = ProjectionSimVizTransform(ds.color_map_rgb_np, ds.learning_map_inv_np)\n",
    "        \n",
    "        if stage == \"fit\":\n",
    "            self.ds_train = ds\n",
    "            self.ds_val = UFGSimDataset(data_path, 'valid', tfms)\n",
    "        \n",
    "        if stage == \"test\":\n",
    "            self.ds_test = ds\n",
    "        if stage == \"predict\":\n",
    "            self.ds_predict = ds\n",
    "            \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.ds_train, batch_size=self.train_batch_size, num_workers=self.num_workers, shuffle=True, drop_last=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.ds_val, batch_size=self.eval_batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.ds_test, batch_size=self.eval_batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.ds_predict, batch_size=self.eval_batch_size, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1923fae-717a-4bf4-af58-5ee93300fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
