{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11c69cc-d200-4710-addf-8062129cfe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec6d3ae-45a2-486d-9fab-a7b1b854babc",
   "metadata": {},
   "source": [
    "# models\n",
    "\n",
    "> Module that implements different models processing point cloud data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc207aa6-bc9a-425f-b6cb-d07520a553a0",
   "metadata": {},
   "source": [
    "# UNDER CONSTRUCTION..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0028db0e-08f5-4c13-a6fc-d50701159819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b407d042-2215-4712-9e2c-d951a8cf0fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "from torch.nn import Module, Sequential, Conv2d, BatchNorm2d, ReLU, ModuleList, MaxPool2d, ConvTranspose2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4818cc4f-4ebb-454d-9ab1-edeb6f5b22e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Block(Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.net = Sequential(\n",
    "            Conv2d(in_channels, out_channels, 3, 1, 1, bias=False, padding_mode='circular'), \n",
    "            BatchNorm2d(out_channels), \n",
    "            ReLU(),\n",
    "            Conv2d(out_channels, out_channels, 3, 1, 1, bias=False, padding_mode='circular'), \n",
    "            BatchNorm2d(out_channels), \n",
    "            ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83d485e-4645-4b78-a94e-370a45b42359",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "from colorcloud.datatools import SemanticKITTIDataset, SphericalProjectionTransform, ToTensorTransform\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3c5db3-c464-4cec-bcbc-f1501fa75698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64, 1024])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "data_path = '../../Cloud2DImageConverter/point_clouds/semantic_kitti/'\n",
    "ds = SemanticKITTIDataset(data_path)\n",
    "\n",
    "tfms = v2.Compose([\n",
    "    SphericalProjectionTransform(fov_up_deg=4., fov_down_deg=-26., W=1024, H=64),\n",
    "    ToTensorTransform(),\n",
    "])\n",
    "ds.set_transform(tfms)\n",
    "img, label, mask = ds[0]\n",
    "\n",
    "b = Block(5, 64)\n",
    "activations = b(img.reshape(-1, *img.shape))\n",
    "activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1e51fb-d9ee-48f3-a4c9-b6a26e8788a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Encoder(Module):\n",
    "    def __init__(self, channels=(5, 64, 128, 256, 512, 1024)):\n",
    "        super().__init__()\n",
    "        self.blocks = ModuleList(\n",
    "            [Block(channels[i], channels[i+1]) for i in range(len(channels)-1)]\n",
    "        )\n",
    "        self.pool = MaxPool2d(2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        enc_features = []\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            enc_features.append(x)\n",
    "            x = self.pool(x)\n",
    "        return enc_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af6e533-a6be-40c5-9dd7-69b68c7f5824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 64, 64, 1024]),\n",
       " torch.Size([1, 128, 32, 512]),\n",
       " torch.Size([1, 256, 16, 256]),\n",
       " torch.Size([1, 512, 8, 128]),\n",
       " torch.Size([1, 1024, 4, 64])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "enc = Encoder()\n",
    "activations = enc(img.reshape(-1, *img.shape))\n",
    "[a.shape for a in activations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e039c48-213f-4411-8d6d-884ecb8f7dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Decoder(Module):\n",
    "    def __init__(self, channels=(1024, 512, 256, 128, 64)):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.upconvs = ModuleList(\n",
    "            [ConvTranspose2d(channels[i], channels[i+1], 2, 2) for i in range(len(channels)-1)]\n",
    "        )\n",
    "        self.blocks = ModuleList(\n",
    "            [Block(channels[i], channels[i+1]) for i in range(len(channels)-1)]\n",
    "        )\n",
    "    \n",
    "    def forward(self, enc_features):\n",
    "        x = enc_features[-1]\n",
    "        for i in range(len(self.channels)-1):\n",
    "            x = self.upconvs[i](x)\n",
    "            x = torch.cat([x, enc_features[-(i+2)]], dim=1)\n",
    "            x = self.blocks[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b81d76a-f335-4d17-bbed-a0685eb0987b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64, 1024])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "dec = Decoder()\n",
    "activations = dec(activations)\n",
    "activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2b5b8a-c437-4e65-8027-182568366723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class UNet(Module):\n",
    "    def __init__(self, in_channels=5, hidden_channels=(64, 128, 256, 512, 1024), n_classes=20):\n",
    "        super().__init__()\n",
    "        self.backbone = Sequential(\n",
    "            Encoder((in_channels, *hidden_channels)),\n",
    "            Decoder(hidden_channels[::-1])\n",
    "        )\n",
    "        self.head = Conv2d(hidden_channels[0], n_classes, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        prediction = self.head(features)\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7d8d7b-d855-48ec-83b3-ff34bcac6dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 64, 1024])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "model = UNet()\n",
    "logits = model(img.reshape(-1, *img.shape))\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4ecf63-72c5-4532-8198-f32aea044af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
