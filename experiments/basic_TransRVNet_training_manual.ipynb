{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967697e7-371b-4ecc-8a32-20299f7498cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255556ac-018a-4b5d-804c-0b08dc792f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from colorcloud.behley2019iccv import plot_projections\n",
    "from colorcloud.cheng2023TransRVNet import TransVRNet\n",
    "from colorcloud.cheng2023TransRVNet import TransRVNet_loss\n",
    "from colorcloud.cheng2023TransRVNet import RandomRotationTransform\n",
    "from colorcloud.cheng2023TransRVNet import RandomDroppingPointsTransform\n",
    "from colorcloud.cheng2023TransRVNet import RandomSingInvertingTransform\n",
    "from colorcloud.behley2019iccv import SemanticKITTIDataset\n",
    "from colorcloud.behley2019iccv import ProjectionTransform\n",
    "from colorcloud.behley2019iccv import ProjectionToTensorTransform\n",
    "from colorcloud.behley2019iccv import SphericalProjection\n",
    "from colorcloud.behley2019iccv import ProjectionVizTransform\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torchvision.transforms import v2\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from torchmetrics.classification import Accuracy\n",
    "from torchmetrics.segmentation import MeanIoU\n",
    "from torchmetrics.classification import Dice\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4fec74-752a-45cc-8094-a5dddc3accf6",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eed986-4cf7-4ee2-a181-1a4f00679964",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3809f11e",
   "metadata": {},
   "source": [
    "## Memory visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4f872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.memory._record_memory_history(max_entries=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57660195-b8be-45ea-82d4-ac361abb8612",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Convolutions' Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6686904-8719-4759-b840-e6204f25860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrciam_p = {\n",
    "    \"p1\": {\n",
    "        \"b1_in\": 1,\n",
    "        \"b1_out1\": 32,\n",
    "        \"b1_out2\": 32,\n",
    "        \"b1_out3\": 32,\n",
    "        \"b2_in\": 64,\n",
    "        \"b2_out\": 64,\n",
    "        \"b3_in\": 64,\n",
    "        \"b3_out1\": 64,\n",
    "        \"b3_out2\": 64,\n",
    "        \"b3_out3\": 64,\n",
    "        \"output\": 64,\n",
    "    },\n",
    "    \"p2\": {\n",
    "        \"b1_in\": 3,\n",
    "        \"b1_out1\": 32,\n",
    "        \"b1_out2\": 32,\n",
    "        \"b1_out3\": 32,\n",
    "        \"b2_in\": 64,\n",
    "        \"b2_out\": 64,\n",
    "        \"b3_in\": 64,\n",
    "        \"b3_out1\": 64,\n",
    "        \"b3_out2\": 64,\n",
    "        \"b3_out3\": 64,\n",
    "        \"output\": 64,\n",
    "    },\n",
    "    \"output_conv\": 192\n",
    "}\n",
    "\n",
    "encoder_p = {\n",
    "    \"module_1\": {\n",
    "        \"in_channels\": 192,\n",
    "        \"conv2_in_channels\": 128,\n",
    "        \"conv2_out_channels\": 256,\n",
    "        \"dilated_conv_out_channels\": 256,\n",
    "        \"residual_out_channels\": 256\n",
    "    },\n",
    "    \"module_2\": {\n",
    "        \"in_channels\": 256,\n",
    "        \"conv2_in_channels\": 256,\n",
    "        \"conv2_out_channels\": 256,\n",
    "        \"dilated_conv_out_channels\": 256,\n",
    "        \"residual_out_channels\": 256\n",
    "    } \n",
    "}\n",
    "\n",
    "decoder_p = {\n",
    "    \"in_channels\": 264,\n",
    "    \"conv2_in_channels\": 128,\n",
    "    \"dilated_conv_in_channels\": 128,\n",
    "    \"dilated_conv_out_channels\": 64,\n",
    "    \"output\": 32\n",
    "}\n",
    "\n",
    "p_bntm = {\n",
    "    \"window_size\": (4,4),\n",
    "    \"embed_dim\": int(encoder_p[\"module_2\"][\"residual_out_channels\"]/8)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d890da",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9819793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_projections(img, label):\n",
    "    _, axs = plt.subplots(6, 1, figsize=(20,10), layout='compressed')\n",
    "    for i, (ax, title) in enumerate(zip(axs, ['x', 'y', 'z', 'reflectance', 'depth', 'label'])): # type: ignore\n",
    "        if i < 5:\n",
    "            ax.imshow(img[:,:,i])\n",
    "        else:\n",
    "            ax.imshow(label)\n",
    "        ax.set_title(title)\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083794e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/workspace/data'\n",
    "train_dataset = SemanticKITTIDataset(data_path=data_path, split='train')\n",
    "print(\"Size of train dataset: \", len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c202e933",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = SphericalProjection(fov_up_deg=4., fov_down_deg=-26., W=1024, H=64)\n",
    "scaling_values = {\n",
    "    \"x\" : {\"min\": -150., \"max\":150.},\n",
    "    \"y\" : {\"min\": -150., \"max\":150.},\n",
    "    \"z\" : {\"min\": -12., \"max\":30.},\n",
    "    \"r\" : {\"min\": 0., \"max\":1.},\n",
    "    \"d\" : {\"min\": 0., \"max\":130.}\n",
    "}\n",
    "aug_tfms = v2.Compose([\n",
    "    RandomDroppingPointsTransform(),\n",
    "    RandomRotationTransform(),\n",
    "    RandomSingInvertingTransform(),\n",
    "    ProjectionTransform(proj),\n",
    "    ProjectionVizTransform(train_dataset.color_map_rgb_np, train_dataset.learning_map_inv_np, scaling_values),\n",
    "])\n",
    "\n",
    "train_dataset.set_transform(aug_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc661738",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = train_dataset[0]\n",
    "img = item['frame']\n",
    "label = item['label']\n",
    "\n",
    "plot_projections(img, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b36022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_imgs(pred, label, mask, stage, step):\n",
    "    \"\"\"\n",
    "    Logs predicted and ground truth images during model training/evaluation, \n",
    "    optionally saving them locally.\n",
    "    \"\"\"\n",
    "\n",
    "    viz_tfm = ProjectionVizTransform(train_dataset.color_map_rgb_np, \n",
    "                                     train_dataset.learning_map_inv_np, \n",
    "                                     scaling_values)\n",
    "    def to_numpy(tensor):\n",
    "        return tensor[0].detach().cpu().numpy()\n",
    "    \n",
    "    pred_np = to_numpy(pred).argmax(0)\n",
    "    label_np = to_numpy(label)\n",
    "    mask_np = to_numpy(mask)\n",
    "\n",
    "    altered_pred_np = pred_np.copy()\n",
    "    # Set matching predictions to 0\n",
    "    altered_pred_np[altered_pred_np == label_np] = 0\n",
    "    \n",
    "    item = {\n",
    "        \"frame\": None,\n",
    "        \"label\": pred_np,\n",
    "        \"mask\": mask_np,\n",
    "        \"weight\": None\n",
    "    }\n",
    "    pred_img = viz_tfm(item)[\"label\"]\n",
    "\n",
    "    item[\"label\"] = altered_pred_np\n",
    "    altered_pred_img = viz_tfm(item)[\"label\"]\n",
    "    \n",
    "    item[\"label\"] = label_np\n",
    "    label_img = viz_tfm(item)[\"label\"]\n",
    "    \n",
    "    img_cmp = np.concatenate((label_img, altered_pred_img, pred_img), axis=0)\n",
    "    \n",
    "    img_cmp = img_cmp.astype(np.uint8)\n",
    "    img_cmp_pil = Image.fromarray(img_cmp)\n",
    "    save_path = f\"../../data/imagens/transRVNet/manual_{stage}examples_step{step}.png\"\n",
    "    img_cmp_pil.save(save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341b3269",
   "metadata": {},
   "source": [
    "# Pytorch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b957e27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/workspace/data'\n",
    "train_dataset   = SemanticKITTIDataset(data_path=data_path, split='train')\n",
    "val_dataset     = SemanticKITTIDataset(data_path=data_path, split='valid')\n",
    "test_dataset    = SemanticKITTIDataset(data_path=data_path, split='test' )\n",
    "\n",
    "print(\"Size of train dataset:   \", len(train_dataset))\n",
    "print(\"Size of val dataset:     \", len(val_dataset  ))\n",
    "print(\"Size of test dataset:    \", len(test_dataset ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c5042",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = SphericalProjection(fov_up_deg=4., fov_down_deg=-26., W=1024, H=64)\n",
    "scaling_values = {\n",
    "    \"x\" : {\"min\": -150., \"max\":150.},\n",
    "    \"y\" : {\"min\": -150., \"max\":150.},\n",
    "    \"z\" : {\"min\": -10., \"max\":30.},\n",
    "    \"r\" : {\"min\": 0., \"max\":1.},\n",
    "    \"d\" : {\"min\": 0., \"max\":130.}\n",
    "}\n",
    "aug_tfms = v2.Compose([\n",
    "    RandomDroppingPointsTransform(),\n",
    "    RandomRotationTransform(),\n",
    "    RandomSingInvertingTransform(),\n",
    "    ProjectionTransform(proj),\n",
    "    ProjectionToTensorTransform(),\n",
    "])\n",
    "\n",
    "train_dataset.set_transform(aug_tfms)\n",
    "val_dataset.set_transform(aug_tfms)\n",
    "test_dataset.set_transform(aug_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a848edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size,\n",
    "    num_workers=2,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    "    # pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size,\n",
    "    num_workers=2,\n",
    "    shuffle=False\n",
    "    # pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    # pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a193e990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps\n",
    "n_epochs = 1\n",
    "train_steps = len(train_loader) // batch_size\n",
    "val_steps = len(val_loader) // batch_size\n",
    "# test_steps = len(test_loader) // batch_size\n",
    "total_steps = train_steps * n_epochs\n",
    "\n",
    "H = {\"train_loss\": [], \"val_loss\": []} # store loss history\n",
    "\n",
    "model = TransVRNet(mrciam_p, encoder_p, decoder_p, p_bntm).to(device)\n",
    "# loss function\n",
    "loss_func = TransRVNet_loss(device=device)\n",
    "# optimizer\n",
    "opt = AdamW(model.parameters(), lr=1e-1, weight_decay=0.0001)\n",
    "lr_scheduler = OneCycleLR(opt, max_lr=0.002, div_factor=1, final_div_factor=10, steps_per_epoch=total_steps, epochs=30) # mudar esse parametro epochs para n_epochs dps\n",
    "# dropout\n",
    "dropout = torch.nn.Dropout(p=0.2, inplace=False)\n",
    "\n",
    "\n",
    "###########################################################\n",
    "####################### metrics ###########################\n",
    "###########################################################\n",
    "accuracy = Accuracy(task=\"multiclass\", num_classes=model.n_classes, average=None).to(device)\n",
    "accuracy_dict = {\"train\": [], \"val\": []}\n",
    "\n",
    "# Mean IoU\n",
    "miou = MeanIoU(num_classes=model.n_classes, per_class=True).to(device)  # Ajuste aqui\n",
    "miou_dict = {\"train\": [], \"val\": []}\n",
    "\n",
    "dice = Dice(num_classes=model.n_classes).to(device)\n",
    "dice_dict = {\"train\": [], \"val\": []}\n",
    "\n",
    "# Multiclass F1 Score\n",
    "mcf1s = MulticlassF1Score(num_classes=model.n_classes, average=None).to(device)\n",
    "mcf1s_dict = {\"train\": [], \"val\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2d8bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "current_step = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=\"Training  \", leave=True):\n",
    "        current_step += 1\n",
    "        train_item = {key: value.to(device) for key, value in batch.items()}\n",
    "        img = train_item['frame']\n",
    "        label = train_item['label'].to(torch.int64)\n",
    "        mask = train_item['mask']\n",
    "\n",
    "        label[~mask] = 0\n",
    "\n",
    "        # Separate channels\n",
    "        xyz = img[:, :3, :, :]\n",
    "        reflectance = img[:, 3, :, :].unsqueeze(1)\n",
    "        depth = img[:, 4, :, :].unsqueeze(1)\n",
    "\n",
    "        pred = model(xyz, depth, reflectance)\n",
    "\n",
    "        # Apply dropout\n",
    "        pred = dropout(pred)\n",
    "\n",
    "        train_loss = loss_func(pred, label, mask)\n",
    "\n",
    "        ########################################\n",
    "        # aqui pra baixo n precisa para o treino\n",
    "        # pred_f = torch.permute(pred, (0, 2, 3, 1)) # N,C,H,W -> N,H,W,C\n",
    "        # pred_f = torch.flatten(pred_f, 0, -2)      # N,H,W,C -> N*H*W,C\n",
    "        # mask_f = torch.flatten(mask)               # N,H,W   -> N*H*W\n",
    "        # pred_m = pred_f[mask_f, :]\n",
    "        # label_m = label[mask]\n",
    "\n",
    "        # current_train_acc = accuracy(pred_m, label_m)\n",
    "        # accuracy_dict[\"train\"].append(current_train_acc.cpu().detach().numpy())\n",
    "        # pred_labels = torch.argmax(pred, dim=1).to(device)\n",
    "        # mask_miou = (label != 0)\n",
    "        # pred_labels[~mask] = 0\n",
    "        # current_train_miou = miou(pred_labels, label)\n",
    "        # miou_dict[\"train\"].append(current_train_miou.cpu().detach().numpy())\n",
    "        # current_train_dice = dice(pred_labels, label)\n",
    "        # dice_dict[\"train\"].append(current_train_dice.cpu().detach().numpy())\n",
    "        # current_train_mcf1s = mcf1s(pred_labels, label)\n",
    "        # mcf1s_dict[\"train\"].append(current_train_mcf1s.cpu().detach().numpy())\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        train_loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_train_loss += train_loss\n",
    "\n",
    "        if(((current_step) % 500) == 0 or current_step == 1):\n",
    "            log_imgs(pred, label, mask, 'train', current_step)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\", leave=True):\n",
    "            val_item = {key: value.to(device) for key, value in batch.items()}\n",
    "            img = val_item['frame']\n",
    "            label = val_item['label'].to(torch.int64)\n",
    "            mask = val_item['mask']\n",
    " \n",
    "            label[~mask] = 0\n",
    "\n",
    "            # Separate channels\n",
    "            xyz = img[:, :3, :, :]\n",
    "            reflectance = img[:, 3, :, :].unsqueeze(1)\n",
    "            depth = img[:, 4, :, :].unsqueeze(1)\n",
    "\n",
    "            pred = model(xyz, depth, reflectance)\n",
    "\n",
    "            val_loss = loss_func(pred, label, mask)\n",
    "\n",
    "            ########################################\n",
    "            # # aqui pra baixo n precisa para o treino\n",
    "            # pred_f = torch.permute(pred, (0, 2, 3, 1)) # N,C,H,W -> N,H,W,C\n",
    "            # pred_f = torch.flatten(pred_f, 0, -2)      # N,H,W,C -> N*H*W,C\n",
    "            # mask_f = torch.flatten(mask)               # N,H,W   -> N*H*W\n",
    "            # pred_m = pred_f[mask_f, :]\n",
    "            # label_m = label[mask]\n",
    "\n",
    "            # current_val_acc = accuracy(pred_m, label_m)\n",
    "            # accuracy_dict[\"val\"].append(current_val_acc.cpu().detach().numpy())\n",
    "            # pred_labels = torch.argmax(pred, dim=1).to(device)\n",
    "            # mask_miou = (label != 0)\n",
    "            # pred_labels[~mask] = 0\n",
    "            # current_val_miou = miou(pred_labels, label)\n",
    "            # miou_dict[\"val\"].append(current_val_miou.cpu().detach().numpy())\n",
    "            # current_val_dice = dice(pred_labels, label)\n",
    "            # dice_dict[\"val\"].append(current_val_dice.cpu().detach().numpy())\n",
    "            # current_val_mcf1s = mcf1s(pred_labels, label)\n",
    "            # mcf1s_dict[\"val\"].append(current_val_mcf1s.cpu().detach().numpy())\n",
    "        \n",
    "            total_val_loss += val_loss\n",
    "\n",
    "        if(((current_step) % 500) == 0 or current_step == 1):\n",
    "            log_imgs(pred, label, mask, 'test', epoch+1)\n",
    "\n",
    "    avg_train_loss = total_train_loss / train_steps\n",
    "    avg_val_loss = total_val_loss / val_steps\n",
    "\n",
    "    # Store loss history for graphical visualization\n",
    "    H[\"train_loss\"].append(avg_train_loss.cpu().detach().numpy()) \n",
    "    H[\"val_loss\"].append(avg_val_loss.cpu().detach().numpy())\n",
    "\n",
    "    print(\"CURRENT EPOCH: {}/{}\".format(epoch + 1, n_epochs))\n",
    "    print(\"Train loss:    {:.10f}, Val loss {:.4f}\".format(avg_train_loss, avg_val_loss))\n",
    "\n",
    "    # model_name = f\"../../data/modelos/{epoch}_transRVNet_torch.pt\"\n",
    "    # torch.save(model, model_name)\n",
    "\n",
    "    torch.cuda.empty_cache()  # Opcional: libera cache da GPU após cada época\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Training took {:.2f}s\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb41d93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.memory._dump_snapshot(\"../../data/profile.pkl\")\n",
    "# torch.cuda.memory._record_memory_history(enabled=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2560ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    \"unlabeled\", \"car\", \"bicycle\", \"motorcycle\", \"truck\", \"other-vehicle\",\n",
    "    \"person\", \"bicyclist\", \"motorcyclist\", \"road\", \"parking\", \"sidewalk\",\n",
    "    \"other-ground\", \"building\", \"fence\", \"vegetation\", \"trunk\", \"terrain\",\n",
    "    \"pole\", \"traffic-sign\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55024c6f",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc18867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar o tensor em uma lista e criar pares classe-valor\n",
    "current_train_acc_cpu = current_train_acc.cpu().numpy()  # Mover para CPU para manipulação\n",
    "results = zip(classes, current_train_acc_cpu)\n",
    "\n",
    "# Exibir resultados formatados\n",
    "print(\"Accuracy por classe on training:\")\n",
    "for cls, acc in results:\n",
    "    print(f\"{cls:>15}: {acc:.4f}\")\n",
    "\n",
    "# Transformar o tensor em uma lista e criar pares classe-valor\n",
    "current_val_acc_cpu = current_val_acc.cpu().numpy()  # Mover para CPU para manipulação\n",
    "results = zip(classes, current_val_acc_cpu)\n",
    "\n",
    "# Exibir resultados formatados\n",
    "print(\"Accuracy por classe on val:\")\n",
    "for cls, acc in results:\n",
    "    print(f\"{cls:>15}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332abc05",
   "metadata": {},
   "source": [
    "### Mean IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1dca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar o tensor em uma lista e criar pares classe-valor\n",
    "current_train_miou_cpu = current_train_miou.cpu().numpy()  # Mover para CPU para manipulação\n",
    "results = zip(classes, current_train_miou_cpu)\n",
    "\n",
    "# Exibir resultados formatados\n",
    "print(\"Mean IOU por classe on training:\")\n",
    "for cls, acc in results:\n",
    "    print(f\"{cls:>15}: {acc:.4f}\")\n",
    "\n",
    "# Transformar o tensor em uma lista e criar pares classe-valor\n",
    "current_val_miou_cpu = current_val_miou.cpu().numpy()  # Mover para CPU para manipulação\n",
    "results = zip(classes, current_val_miou_cpu)\n",
    "\n",
    "# Exibir resultados formatados\n",
    "print(\"Mean IOU por classe on val:\")\n",
    "for cls, acc in results:\n",
    "    print(f\"{cls:>15}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0fb7dd",
   "metadata": {},
   "source": [
    "### Dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1568f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Dice: {:.4f} on training and {:.4f} on testing\".format(current_train_dice, current_val_dice))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081a50ec",
   "metadata": {},
   "source": [
    "### F1 Macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a2e2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar o tensor em uma lista e criar pares classe-valor\n",
    "current_train_mcf1s_cpu = current_train_mcf1s.cpu().numpy()  # Mover para CPU para manipulação\n",
    "results = zip(classes, current_train_mcf1s_cpu)\n",
    "\n",
    "# Exibir resultados formatados\n",
    "print(\"F1 Macro por classe on training:\")\n",
    "for cls, acc in results:\n",
    "    print(f\"{cls:>15}: {acc:.4f}\")\n",
    "\n",
    "# Transformar o tensor em uma lista e criar pares classe-valor\n",
    "current_val_mcf1s_cpu = current_val_mcf1s.cpu().numpy()  # Mover para CPU para manipulação\n",
    "results = zip(classes, current_val_mcf1s_cpu)\n",
    "\n",
    "# Exibir resultados formatados\n",
    "print(\"F1 Macro por classe on val:\")\n",
    "for cls, acc in results:\n",
    "    print(f\"{cls:>15}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e97e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_name = \"transRVNet_torch.pt\"\n",
    "torch.save(model, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f596aa",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3952bb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(H[\"val_loss\"], label=\"val_loss\")\n",
    "plt.title(\"Training Loss on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766cfb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "\n",
    "train_accuracy = [x.sum()/20 for x in accuracy_dict[\"train\"]]\n",
    "val_accuracy = [x.sum()/20 for x in accuracy_dict[\"val\"]]\n",
    "\n",
    "plt.plot(train_accuracy, label=\"train_accuracy\")\n",
    "plt.plot(val_accuracy, label=\"val_accuracy\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Steps #\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1a3859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean IoU\n",
    "# train_miou = [x.cpu().numpy() for x in miou_dict[\"train\"]]\n",
    "# val_miou = [x.cpu().numpy() for x in miou_dict[\"val\"]]\n",
    "\n",
    "train_miou = [x.sum()/20 for x in miou_dict[\"train\"]]\n",
    "val_miou = [x.sum()/20 for x in miou_dict[\"val\"]]\n",
    "\n",
    "plt.plot(train_miou, label=\"train_miou\")\n",
    "plt.plot(val_miou, label=\"val_miou\")\n",
    "plt.title(\"Mean IOU\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"miou\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1732b6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_miou = [x for x in miou_dict[\"train\"]]\n",
    "val_miou = [x for x in miou_dict[\"val\"]]\n",
    "\n",
    "# Calcula a média do Mean IoU por classe para o conjunto de treino e validação\n",
    "mean_train_miou = np.mean(train_miou, axis=0)\n",
    "mean_val_miou = np.mean(val_miou, axis=0)\n",
    "\n",
    "# Calcula a média do Mean IoU por classe para o conjunto de treino e validação\n",
    "# mean_train_miou = mean_train_miou[:13]\n",
    "# mean_val_miou = mean_val_miou[:13]\n",
    "\n",
    "# Número de classes\n",
    "num_classes = mean_train_miou.shape[0]\n",
    "\n",
    "# Plotando o gráfico de barras para Mean IoU de treino e validação\n",
    "plt.figure(figsize=(12, 6))\n",
    "bar_width = 0.35\n",
    "index = np.arange(num_classes)\n",
    "\n",
    "# Barras para treino e validação\n",
    "plt.bar(index, mean_train_miou, bar_width, label='Treino', color='salmon', edgecolor='black')\n",
    "plt.bar(index + bar_width, mean_val_miou, bar_width, label='Validação', color='skyblue', edgecolor='black')\n",
    "\n",
    "# Configuração dos eixos e títulos\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Mean IoU')\n",
    "plt.title('Mean IoU Médio por Classe para Treino e Validação')\n",
    "plt.xticks(index + bar_width / 2, classes, rotation=45)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05e5dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dice\n",
    "train_dice = [x for x in dice_dict[\"train\"]]\n",
    "val_dice = [x for x in dice_dict[\"val\"]]\n",
    "\n",
    "plt.plot(train_dice, label=\"train_dice\")\n",
    "plt.plot(val_dice, label=\"val_dice\")\n",
    "plt.title(\"Dice\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"dice score\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b54ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 Macro\n",
    "train_mcf1s = [x.sum()/20 for x in mcf1s_dict[\"train\"]]\n",
    "val_mcf1s = [x.sum()/20 for x in mcf1s_dict[\"val\"]]\n",
    "\n",
    "plt.plot(train_mcf1s, label=\"train_mcf1s\")\n",
    "plt.plot(val_mcf1s, label=\"val_mcf1s\")\n",
    "plt.title(\"F1 Macro\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"mcf1s score\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3704b582",
   "metadata": {},
   "source": [
    "## Vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bd60d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('transRVNet_torch.pt', weights_only=False).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3028f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def expand_to_64(img):\n",
    "    N, C, H, W = img.shape\n",
    "    assert H == 16, \"A altura inicial da imagem deve ser 16.\"\n",
    "    expanded_img = torch.zeros((N, C, 64, W), device=img.device, dtype=img.dtype)\n",
    "    for i in range(H):\n",
    "        expanded_img[:, :, i * 4, :] = img[:, :, i, :]\n",
    "    return expanded_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37c7bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def reduce_to_16(img):\n",
    "    N, C, H, W = img.shape\n",
    "    assert H == 64, \"A altura da imagem deve ser 64 para ser reduzida para 16.\"\n",
    "    reduced_img = img.view(N, C, 16, 4, W).mean(dim=3)\n",
    "    return reduced_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5e7788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_imgs(pred, label, mask, num_batch):\n",
    "    def to_numpy(tensor):\n",
    "        return tensor[0].detach().cpu().numpy()\n",
    "\n",
    "    scaling_values = {\n",
    "    \"x\" : {\"min\": -150., \"max\":150.},\n",
    "    \"y\" : {\"min\": -150., \"max\":150.},\n",
    "    \"z\" : {\"min\": -10., \"max\":30.},\n",
    "    \"r\" : {\"min\": 0., \"max\":1.},\n",
    "    \"d\" : {\"min\": 0., \"max\":130.}\n",
    "    }\n",
    "    \n",
    "    viz_tfm = ProjectionVizTransform(test_dataset.color_map_rgb_np, test_dataset.learning_map_inv_np, scaling_values)\n",
    "    pred_np = to_numpy(pred).argmax(0)\n",
    "    label_np = to_numpy(label)\n",
    "    mask_np = to_numpy(mask)\n",
    "\n",
    "    altered_pred_np = pred_np.copy()\n",
    "    # Set matching predictions to 0\n",
    "    altered_pred_np[altered_pred_np == label_np] = 0\n",
    "    \n",
    "    item = {\n",
    "        \"frame\": None,\n",
    "        \"label\": pred_np,\n",
    "        \"mask\": mask_np,\n",
    "        \"weight\": None\n",
    "    }\n",
    "    pred_img = viz_tfm(item)[\"label\"]\n",
    "\n",
    "    item[\"label\"] = altered_pred_np\n",
    "    altered_pred_img = viz_tfm(item)[\"label\"]\n",
    "    \n",
    "    item[\"label\"] = label_np\n",
    "    label_img = viz_tfm(item)[\"label\"]\n",
    "    \n",
    "    img_cmp = np.concatenate((label_img, altered_pred_img, pred_img), axis=0)\n",
    "    \n",
    "    img_cmp = img_cmp.astype(np.uint8)\n",
    "    img_cmp_pil = Image.fromarray(img_cmp)\n",
    "    save_path = f\"../../data/imagens/{num_batch}examples_with_loss.png\"\n",
    "    img_cmp_pil.save(save_path)\n",
    "     \n",
    "\n",
    "def inference_with_metrics(model, test_loader, device, i_type=None):\n",
    "    model.eval()\n",
    "\n",
    "    total_accuracy = 0\n",
    "    total_miou = 0\n",
    "    total_dice = 0\n",
    "    total_mcf1s = 0\n",
    "\n",
    "    first = 0\n",
    "    \n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\", leave=True):\n",
    "            num_batches += 1\n",
    "\n",
    "            test_item = {key: value.to(device) for key, value in batch.items()}\n",
    "            img = test_item['frame']\n",
    "            label = test_item['label'].to(torch.int64)\n",
    "            mask = test_item['mask']\n",
    "\n",
    "            label[~mask] = 0\n",
    "\n",
    "            # Separate channels\n",
    "            xyz = img[:, :3, :, :]\n",
    "            reflectance = img[:, 3, :, :].unsqueeze(1)\n",
    "            depth = img[:, 4, :, :].unsqueeze(1)\n",
    "\n",
    "            #img = expand_to_64(img)\n",
    "            if i_type:\n",
    "                img = F.interpolate(img, size=(64,1024), mode=i_type)\n",
    "\n",
    "            pred = model(xyz, depth, reflectance)\n",
    "\n",
    "            if i_type:\n",
    "                #pred = pred[:, :, ::4, :]\n",
    "                pred = reduce_to_16(pred)\n",
    "            #pred = reduce_to_16(pred)\n",
    "            #pred = F.interpolate(pred.float(), size=(16,1024), mode='nearest')\n",
    "            \n",
    "            # if num_batches == 1:\n",
    "            #     print(f\"pred: {pred.shape}\\n mask: {mask.shape}\")\n",
    "                \n",
    "\n",
    "            if (num_batches % 1) == 0:\n",
    "                save_imgs(pred, label, mask, num_batches)\n",
    "\n",
    "            pred_f = torch.permute(pred, (0, 2, 3, 1)) # N,C,H,W -> N,H,W,C\n",
    "            pred_f = torch.flatten(pred_f, 0, -2)      # N,H,W,C -> N*H*W,C\n",
    "            mask_f = torch.flatten(mask)               # N,H,W   -> N*H*W\n",
    "            pred_m = pred_f[mask_f, :]\n",
    "            label_m = label[mask]\n",
    "\n",
    "            # import pdb\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            pred_labels = torch.argmax(pred, dim=1).to(device)\n",
    "            mask_miou = (label != 0)\n",
    "            pred_labels[~mask] = 0\n",
    "\n",
    "            current_test_acc = accuracy(pred_m, label_m)\n",
    "            current_test_miou = miou(pred_labels, label)\n",
    "            current_test_dice = dice(pred_labels, label)\n",
    "            current_test_mcf1s = mcf1s(pred_labels, label)\n",
    "\n",
    "            total_accuracy += current_test_acc\n",
    "            total_miou += current_test_miou\n",
    "            total_dice += current_test_dice\n",
    "            total_mcf1s += current_test_mcf1s\n",
    "\n",
    "    avg_accuracy = total_accuracy / num_batches\n",
    "    avg_miou = total_miou / num_batches\n",
    "    avg_dice = total_dice / num_batches\n",
    "    avg_mcf1s = total_mcf1s / num_batches\n",
    "\n",
    "    return avg_accuracy, avg_miou, avg_dice, avg_mcf1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5063275",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_type = [\"normal\", \"bilinear\", \"nearest\"]\n",
    "classes = [\"unlabeled\", \"car\", \"person\", \"road\", \"terrain\", \"vegetation\"]\n",
    "\n",
    "for i_type in inference_type:\n",
    "    if i_type == \"normal\":\n",
    "        avg_acc, avg_miou, avg_dice, avg_f1_macro = inference_with_metrics(model, test_loader, device)\n",
    "    # else:\n",
    "        # avg_acc, avg_miou, avg_dice, avg_f1_macro = inference_with_metrics(model, test_loader, device, i_type)\n",
    "\n",
    "    print(i_type)\n",
    "    for iou, item in zip(avg_miou, classes): # type: ignore\n",
    "        print(f\"{item}: {iou*100:.1f}\")\n",
    "    mean = sum(avg_miou[1:6]/len(avg_miou[1:6])) # type: ignore\n",
    "    print(f\"\\nmean {mean*100:.1f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f223f3d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
