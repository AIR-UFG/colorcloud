{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a16075c-e886-41bd-a374-bf2ecf7f2fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "663e75fa-bd3b-4424-bd5d-e9c259be79e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorcloud.behley2019iccv import SphericalProjection, ProjectionToTensorTransform\n",
    "from colorcloud.UFGsim2024infufg import UFGSimDataset, ProjectionSimTransform\n",
    "from colorcloud.biasutti2019riu import RIUNet\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b72646f1-8ded-4e47-b8fc-51fc467a8b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../UFGSim'\n",
    "ds = UFGSimDataset(data_path)\n",
    "\n",
    "proj = SphericalProjection(fov_up_deg=15., fov_down_deg=-15., W=440, H=16)\n",
    "tfms = v2.Compose([\n",
    "    ProjectionSimTransform(proj),\n",
    "    ProjectionToTensorTransform(),\n",
    "])\n",
    "ds.set_transform(tfms)\n",
    "\n",
    "bs = 3\n",
    "dl = DataLoader(ds, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adc434dc-f122-4cde-b7dd-5a5ebe8d2624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a1e0665-3f66-4845-b1e7-130818a06fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIUNet(\n",
      "  (input_norm): BatchNorm2d(4, eps=1e-05, momentum=None, affine=False, track_running_stats=True)\n",
      "  (backbone): Sequential(\n",
      "    (enc): Encoder(\n",
      "      (blocks): ModuleList(\n",
      "        (0): Block(\n",
      "          (conv1): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=circular)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=circular)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "        )\n",
      "        (1): Block(\n",
      "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=circular)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=circular)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "        )\n",
      "        (2): Block(\n",
      "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=circular)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=circular)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "        )\n",
      "        (3): Block(\n",
      "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=circular)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=circular)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dec): Decoder(\n",
      "      (upconvs): ModuleList(\n",
      "        (0): ConvTranspose2d(512, 256, kernel_size=(6, 6), stride=(2, 2), padding=(2, 2), bias=False)\n",
      "        (1): ConvTranspose2d(256, 128, kernel_size=(6, 6), stride=(2, 2), padding=(2, 2), bias=False)\n",
      "        (2): ConvTranspose2d(128, 64, kernel_size=(6, 6), stride=(2, 2), padding=(2, 2), bias=False)\n",
      "      )\n",
      "      (blocks): ModuleList(\n",
      "        (0): Block(\n",
      "          (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=circular)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=circular)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "        )\n",
      "        (1): Block(\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=circular)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=circular)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "        )\n",
      "        (2): Block(\n",
      "          (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=circular)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU()\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=circular)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head): Conv2d(64, 13, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = RIUNet(in_channels=4, hidden_channels=(64, 128, 256, 512), n_classes=13).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a64f654a-ce27-4e78-bc90-26b1a96a5202",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8487c9-27af-44fb-9086-a5c9d3aeb5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.999776\n",
      "loss: 1.864565\n",
      "loss: 1.284580\n",
      "loss: 0.928722\n",
      "loss: 0.754632\n",
      "loss: 0.662107\n",
      "loss: 0.623375\n",
      "loss: 0.624365\n",
      "loss: 0.621404\n",
      "loss: 0.617286\n",
      "loss: 0.643031\n",
      "loss: 0.628119\n",
      "loss: 0.655748\n",
      "loss: 0.665729\n",
      "loss: 0.686211\n",
      "loss: 0.638563\n",
      "loss: 0.522562\n",
      "loss: 0.461450\n",
      "loss: 0.448920\n",
      "loss: 0.421206\n",
      "loss: 0.438708\n",
      "loss: 0.440686\n",
      "loss: 0.456340\n",
      "loss: 0.474012\n",
      "loss: 0.450407\n",
      "loss: 0.411703\n",
      "loss: 0.388037\n",
      "loss: 0.437849\n",
      "loss: 0.468884\n",
      "loss: 0.387367\n",
      "loss: 0.354124\n",
      "loss: 0.308671\n",
      "loss: 0.307039\n",
      "loss: 0.297971\n",
      "loss: 0.312220\n",
      "loss: 0.298699\n",
      "loss: 0.324711\n",
      "loss: 0.341010\n",
      "loss: 0.391715\n",
      "loss: 0.358085\n",
      "loss: 0.330169\n",
      "loss: 0.336429\n",
      "loss: 0.368509\n",
      "loss: 0.346900\n",
      "loss: 0.332161\n",
      "loss: 0.331838\n",
      "loss: 0.334887\n",
      "loss: 0.381763\n",
      "loss: 0.371034\n",
      "loss: 0.378392\n",
      "loss: 0.420559\n",
      "loss: 0.903443\n",
      "loss: 1.272106\n",
      "loss: 0.972197\n",
      "loss: 0.757011\n",
      "loss: 0.612080\n",
      "loss: 0.492289\n",
      "loss: 0.440999\n",
      "loss: 0.352335\n",
      "loss: 0.269848\n",
      "loss: 0.194311\n",
      "loss: 0.146679\n",
      "loss: 0.122284\n",
      "loss: 0.100552\n",
      "loss: 0.077998\n",
      "loss: 0.064796\n",
      "loss: 0.056068\n",
      "loss: 0.060575\n",
      "loss: 0.301072\n",
      "loss: 0.105423\n",
      "loss: 0.078956\n",
      "loss: 0.077059\n",
      "loss: 0.078493\n",
      "loss: 0.096543\n",
      "loss: 0.098823\n",
      "loss: 0.105434\n",
      "loss: 0.102633\n",
      "loss: 0.124547\n",
      "loss: 0.122680\n",
      "loss: 0.113334\n",
      "loss: 0.120007\n",
      "loss: 0.133272\n",
      "loss: 0.128159\n",
      "loss: 0.133481\n",
      "loss: 0.105969\n",
      "loss: 0.104372\n",
      "loss: 0.102044\n",
      "loss: 0.086158\n",
      "loss: 0.099690\n",
      "loss: 0.100133\n",
      "loss: 0.100670\n",
      "loss: 0.105992\n",
      "loss: 0.090875\n",
      "loss: 0.104721\n",
      "loss: 0.100252\n",
      "loss: 0.106455\n",
      "loss: 0.091282\n",
      "loss: 0.082781\n",
      "loss: 0.074733\n",
      "loss: 0.066572\n",
      "loss: 0.070050\n",
      "loss: 0.069578\n",
      "loss: 0.072341\n",
      "loss: 0.067106\n",
      "loss: 0.077368\n",
      "loss: 0.081879\n",
      "loss: 0.077596\n",
      "loss: 0.076560\n",
      "loss: 0.075202\n",
      "loss: 0.083987\n",
      "loss: 0.095098\n",
      "loss: 0.081393\n",
      "loss: 0.083088\n",
      "loss: 0.107628\n",
      "loss: 0.141720\n",
      "loss: 0.140166\n",
      "loss: 0.142713\n",
      "loss: 0.169210\n",
      "loss: 0.146824\n",
      "loss: 0.126101\n",
      "loss: 0.148986\n",
      "loss: 0.151370\n",
      "loss: 0.146717\n",
      "loss: 0.182790\n",
      "loss: 0.190612\n",
      "loss: 0.192607\n",
      "loss: 0.118086\n",
      "loss: 0.070864\n",
      "loss: 0.044536\n",
      "loss: 0.035642\n",
      "loss: 0.027351\n",
      "loss: 0.024907\n",
      "loss: 0.024496\n",
      "loss: 0.018149\n",
      "loss: 0.015813\n",
      "loss: 0.018491\n",
      "loss: 0.044017\n",
      "loss: 0.029088\n",
      "loss: 0.020753\n",
      "loss: 0.024331\n",
      "loss: 0.034448\n",
      "loss: 0.045176\n",
      "loss: 0.050236\n",
      "loss: 0.054369\n",
      "loss: 0.054295\n",
      "loss: 0.074841\n",
      "loss: 0.080946\n",
      "loss: 0.082519\n",
      "loss: 0.079405\n",
      "loss: 0.062606\n",
      "loss: 0.051872\n",
      "loss: 0.051293\n",
      "loss: 0.048659\n",
      "loss: 0.041985\n",
      "loss: 0.041220\n",
      "loss: 0.037369\n",
      "loss: 0.038433\n",
      "loss: 0.046744\n",
      "loss: 0.049128\n",
      "loss: 0.053440\n",
      "loss: 0.046486\n",
      "loss: 0.055418\n",
      "loss: 0.052937\n",
      "loss: 0.050132\n",
      "loss: 0.042141\n",
      "loss: 0.038090\n",
      "loss: 0.038794\n",
      "loss: 0.035230\n",
      "loss: 0.037048\n",
      "loss: 0.036078\n",
      "loss: 0.037379\n",
      "loss: 0.034102\n",
      "loss: 0.038486\n",
      "loss: 0.039764\n",
      "loss: 0.038146\n",
      "loss: 0.034623\n",
      "loss: 0.035515\n",
      "loss: 0.041339\n",
      "loss: 0.044431\n",
      "loss: 0.039893\n",
      "loss: 0.040415\n",
      "loss: 0.049864\n",
      "loss: 0.051951\n",
      "loss: 0.056275\n",
      "loss: 0.061342\n",
      "loss: 0.071764\n",
      "loss: 0.068412\n",
      "loss: 0.060652\n",
      "loss: 0.079923\n",
      "loss: 0.082145\n",
      "loss: 0.063626\n",
      "loss: 0.072807\n",
      "loss: 0.060660\n",
      "loss: 0.049185\n",
      "loss: 0.038045\n",
      "loss: 0.027849\n",
      "loss: 0.021314\n",
      "loss: 0.016581\n",
      "loss: 0.011451\n",
      "loss: 0.010444\n",
      "loss: 0.008763\n",
      "loss: 0.007265\n",
      "loss: 0.006407\n",
      "loss: 0.007605\n",
      "loss: 0.014632\n",
      "loss: 0.009804\n",
      "loss: 0.009243\n",
      "loss: 0.012321\n",
      "loss: 0.021043\n",
      "loss: 0.028670\n",
      "loss: 0.033991\n",
      "loss: 0.034361\n",
      "loss: 0.031411\n",
      "loss: 0.042988\n",
      "loss: 0.039927\n",
      "loss: 0.035192\n",
      "loss: 0.039684\n",
      "loss: 0.031481\n",
      "loss: 0.028541\n",
      "loss: 0.031611\n",
      "loss: 0.029356\n",
      "loss: 0.023929\n",
      "loss: 0.026168\n",
      "loss: 0.022308\n",
      "loss: 0.024402\n",
      "loss: 0.032374\n",
      "loss: 0.035159\n",
      "loss: 0.045427\n",
      "loss: 0.031714\n",
      "loss: 0.028095\n",
      "loss: 0.032842\n",
      "loss: 0.028247\n",
      "loss: 0.027448\n",
      "loss: 0.023670\n",
      "loss: 0.025333\n",
      "loss: 0.028718\n",
      "loss: 0.022644\n",
      "loss: 0.019982\n",
      "loss: 0.020891\n",
      "loss: 0.019472\n",
      "loss: 0.021199\n",
      "loss: 0.022180\n",
      "loss: 0.020940\n",
      "loss: 0.018762\n",
      "loss: 0.018666\n",
      "loss: 0.024400\n",
      "loss: 0.025217\n",
      "loss: 0.024604\n",
      "loss: 0.021970\n",
      "loss: 0.026989\n",
      "loss: 0.025824\n",
      "loss: 0.026305\n",
      "loss: 0.027741\n",
      "loss: 0.030138\n",
      "loss: 0.029332\n",
      "loss: 0.029071\n",
      "loss: 0.036351\n",
      "loss: 0.038476\n",
      "loss: 0.033087\n",
      "loss: 0.043482\n",
      "loss: 0.036830\n",
      "loss: 0.035896\n",
      "loss: 0.026299\n",
      "loss: 0.019077\n",
      "loss: 0.014401\n",
      "loss: 0.013614\n",
      "loss: 0.007747\n",
      "loss: 0.006362\n",
      "loss: 0.004379\n",
      "loss: 0.003329\n",
      "loss: 0.002782\n",
      "loss: 0.003589\n",
      "loss: 0.006811\n",
      "loss: 0.005306\n",
      "loss: 0.005618\n",
      "loss: 0.007256\n",
      "loss: 0.013366\n",
      "loss: 0.018493\n",
      "loss: 0.023648\n",
      "loss: 0.023565\n",
      "loss: 0.021491\n",
      "loss: 0.028659\n",
      "loss: 0.031427\n",
      "loss: 0.042485\n",
      "loss: 0.048702\n",
      "loss: 0.033701\n",
      "loss: 0.026064\n",
      "loss: 0.025626\n",
      "loss: 0.019472\n",
      "loss: 0.015155\n",
      "loss: 0.015881\n",
      "loss: 0.013654\n",
      "loss: 0.015249\n",
      "loss: 0.019710\n",
      "loss: 0.020350\n",
      "loss: 0.023619\n",
      "loss: 0.018945\n",
      "loss: 0.017913\n",
      "loss: 0.022078\n",
      "loss: 0.017372\n",
      "loss: 0.016905\n",
      "loss: 0.015505\n",
      "loss: 0.013794\n",
      "loss: 0.012596\n",
      "loss: 0.011445\n",
      "loss: 0.012113\n",
      "loss: 0.012503\n",
      "loss: 0.012057\n",
      "loss: 0.012985\n",
      "loss: 0.013231\n",
      "loss: 0.012540\n",
      "loss: 0.011600\n",
      "loss: 0.010983\n",
      "loss: 0.015752\n",
      "loss: 0.015540\n",
      "loss: 0.015417\n",
      "loss: 0.012596\n",
      "loss: 0.014507\n",
      "loss: 0.014164\n",
      "loss: 0.015159\n",
      "loss: 0.014460\n",
      "loss: 0.015608\n",
      "loss: 0.014965\n",
      "loss: 0.014787\n",
      "loss: 0.018353\n",
      "loss: 0.020143\n",
      "loss: 0.018365\n",
      "loss: 0.022727\n",
      "loss: 0.018501\n",
      "loss: 0.014825\n",
      "loss: 0.013293\n",
      "loss: 0.009692\n",
      "loss: 0.006479\n",
      "loss: 0.004791\n",
      "loss: 0.003536\n",
      "loss: 0.003169\n",
      "loss: 0.002474\n",
      "loss: 0.001715\n",
      "loss: 0.001543\n",
      "loss: 0.002105\n",
      "loss: 0.004198\n",
      "loss: 0.003233\n",
      "loss: 0.003853\n",
      "loss: 0.004629\n",
      "loss: 0.008718\n",
      "loss: 0.012009\n",
      "loss: 0.016101\n",
      "loss: 0.016153\n",
      "loss: 0.015173\n",
      "loss: 0.026315\n",
      "loss: 0.037828\n",
      "loss: 0.033915\n",
      "loss: 0.025761\n",
      "loss: 0.021212\n",
      "loss: 0.016795\n",
      "loss: 0.016201\n",
      "loss: 0.015393\n",
      "loss: 0.010908\n",
      "loss: 0.010664\n",
      "loss: 0.009052\n",
      "loss: 0.009414\n",
      "loss: 0.012247\n",
      "loss: 0.012386\n",
      "loss: 0.013801\n",
      "loss: 0.014139\n",
      "loss: 0.013565\n",
      "loss: 0.016105\n",
      "loss: 0.013189\n",
      "loss: 0.011949\n",
      "loss: 0.012019\n",
      "loss: 0.010833\n",
      "loss: 0.008545\n",
      "loss: 0.007079\n",
      "loss: 0.007449\n",
      "loss: 0.007596\n",
      "loss: 0.007392\n",
      "loss: 0.008289\n",
      "loss: 0.008424\n",
      "loss: 0.008035\n",
      "loss: 0.007438\n",
      "loss: 0.006738\n",
      "loss: 0.010204\n",
      "loss: 0.009957\n",
      "loss: 0.010198\n",
      "loss: 0.008330\n",
      "loss: 0.008940\n",
      "loss: 0.008777\n",
      "loss: 0.008883\n",
      "loss: 0.008866\n",
      "loss: 0.009105\n",
      "loss: 0.008812\n",
      "loss: 0.008320\n",
      "loss: 0.010677\n",
      "loss: 0.012121\n",
      "loss: 0.011468\n",
      "loss: 0.013232\n",
      "loss: 0.010463\n",
      "loss: 0.008556\n",
      "loss: 0.008186\n",
      "loss: 0.006199\n",
      "loss: 0.004254\n",
      "loss: 0.003475\n",
      "loss: 0.002324\n",
      "loss: 0.002048\n",
      "loss: 0.001608\n",
      "loss: 0.001119\n",
      "loss: 0.000985\n",
      "loss: 0.001304\n",
      "loss: 0.002723\n",
      "loss: 0.002184\n",
      "loss: 0.002946\n",
      "loss: 0.003331\n",
      "loss: 0.005989\n",
      "loss: 0.008094\n",
      "loss: 0.011030\n",
      "loss: 0.011440\n",
      "loss: 0.009729\n",
      "loss: 0.013144\n",
      "loss: 0.011716\n",
      "loss: 0.011684\n",
      "loss: 0.011226\n",
      "loss: 0.008753\n",
      "loss: 0.006973\n",
      "loss: 0.008167\n",
      "loss: 0.007912\n",
      "loss: 0.006738\n",
      "loss: 0.007152\n",
      "loss: 0.005852\n",
      "loss: 0.006449\n",
      "loss: 0.008121\n",
      "loss: 0.007907\n",
      "loss: 0.008325\n",
      "loss: 0.006484\n",
      "loss: 0.007130\n",
      "loss: 0.008252\n",
      "loss: 0.007034\n",
      "loss: 0.007213\n",
      "loss: 0.006235\n",
      "loss: 0.005457\n",
      "loss: 0.004999\n",
      "loss: 0.004471\n",
      "loss: 0.004772\n",
      "loss: 0.004954\n",
      "loss: 0.004823\n",
      "loss: 0.005625\n",
      "loss: 0.005572\n",
      "loss: 0.005467\n",
      "loss: 0.004880\n",
      "loss: 0.004537\n",
      "loss: 0.006726\n",
      "loss: 0.006651\n",
      "loss: 0.006727\n",
      "loss: 0.005417\n",
      "loss: 0.005993\n",
      "loss: 0.005636\n",
      "loss: 0.005679\n",
      "loss: 0.005896\n",
      "loss: 0.005627\n",
      "loss: 0.005683\n",
      "loss: 0.005236\n",
      "loss: 0.006781\n",
      "loss: 0.007768\n",
      "loss: 0.007603\n",
      "loss: 0.008288\n",
      "loss: 0.006700\n",
      "loss: 0.005784\n",
      "loss: 0.005626\n",
      "loss: 0.004317\n",
      "loss: 0.002728\n",
      "loss: 0.001974\n",
      "loss: 0.001478\n",
      "loss: 0.001336\n",
      "loss: 0.001132\n",
      "loss: 0.000767\n",
      "loss: 0.000705\n",
      "loss: 0.000858\n",
      "loss: 0.001870\n",
      "loss: 0.001589\n",
      "loss: 0.002382\n",
      "loss: 0.002578\n",
      "loss: 0.004265\n",
      "loss: 0.005602\n",
      "loss: 0.007540\n",
      "loss: 0.008400\n",
      "loss: 0.007053\n",
      "loss: 0.009506\n",
      "loss: 0.008332\n",
      "loss: 0.007933\n",
      "loss: 0.007632\n",
      "loss: 0.005676\n",
      "loss: 0.004824\n",
      "loss: 0.005698\n",
      "loss: 0.005592\n",
      "loss: 0.005100\n",
      "loss: 0.005245\n",
      "loss: 0.004187\n",
      "loss: 0.004368\n",
      "loss: 0.005959\n",
      "loss: 0.005609\n",
      "loss: 0.006102\n",
      "loss: 0.004668\n",
      "loss: 0.005105\n",
      "loss: 0.005850\n",
      "loss: 0.004881\n",
      "loss: 0.005032\n",
      "loss: 0.004225\n",
      "loss: 0.003704\n",
      "loss: 0.003518\n",
      "loss: 0.003179\n",
      "loss: 0.003391\n",
      "loss: 0.003517\n",
      "loss: 0.003429\n",
      "loss: 0.004008\n",
      "loss: 0.004073\n",
      "loss: 0.003872\n",
      "loss: 0.003544\n",
      "loss: 0.003171\n",
      "loss: 0.004781\n",
      "loss: 0.004839\n",
      "loss: 0.004784\n",
      "loss: 0.003712\n",
      "loss: 0.004115\n",
      "loss: 0.003979\n",
      "loss: 0.003948\n",
      "loss: 0.004207\n",
      "loss: 0.003898\n",
      "loss: 0.003954\n",
      "loss: 0.003618\n",
      "loss: 0.004637\n",
      "loss: 0.005431\n",
      "loss: 0.005490\n",
      "loss: 0.005779\n",
      "loss: 0.004795\n",
      "loss: 0.004186\n",
      "loss: 0.004168\n",
      "loss: 0.003194\n",
      "loss: 0.002009\n",
      "loss: 0.001463\n",
      "loss: 0.001105\n",
      "loss: 0.000981\n",
      "loss: 0.000858\n",
      "loss: 0.000569\n",
      "loss: 0.000535\n",
      "loss: 0.000610\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    for batch in dl:\n",
    "        img, label, mask = batch\n",
    "        img, label, mask = img.to(device), label.to(device), mask.to(device)\n",
    "        label[~mask] = 0\n",
    "        \n",
    "        # Compute prediction error\n",
    "        pred = model(img)\n",
    "        loss = loss_fn(pred, label)\n",
    "        loss = loss[mask].mean()\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"loss: {loss.item():>7f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
