[
  {
    "objectID": "behley2019iccv.html",
    "href": "behley2019iccv.html",
    "title": "behley2019iccv",
    "section": "",
    "text": "This is an adaptation of the code from the semantic-kitti-api as a pytorch Dataset.\nSummarizing quotes from the paper:\n\n\n\n\n“we annotated all 22 sequences of the odometry benchmark of the KITTI Vision Benchmark consisting of over 43 000 scans” ### 3. The SemanticKITTI Dataset\n“showing inner city traffic, residential areas, but also highway scenes and countryside roads around Karlsruhe, Germany”\n“splitting sequences 00 to 10 as training set, and 11 to 21 as test set. For consistency […], we adopt the same division for our training and test set [as the KITTI Vision Benchmark]”\n“we provide 23201 full 3D scans for training and 20351 for testing”\n“we do not distinguish between persons riding a vehicle and the vehicle, but label the vehicle and the person as either bicyclist or motorcyclist”\n“we have 28 classes, where 6 classes are assigned the attribute moving or non-moving, and one outlier class is included for erroneous laser measurements caused by reflections or other effects” #### 3.1 Labeling process\n“we first register and loop close the sequences using an off-the-shelf laser-based SLAM system”\n“For three sequences, we had to manually add loop closure constraints to get correctly loop closed trajectories”\n“We subdivide the sequence of point clouds into tiles of 100 m by 100 m […] This enables us to label all scans consistently even when we encounter temporally distant loop closures”\n“An annotator needs on average 4.5 hours per tile, when labeling residential areas […], and needs on average 1.5 hours for labeling a highway tile.”\n“the whole dataset comprises 518 tiles” ### 4. Evaluation of Semantic Segmentation #### 4.1 Single Scan Experiments\n“we use 25 instead of 28 classes, ignoring outlier, other-structure, and other-object during training and inference”\n“we cannot expect to distinguish moving from non-moving objects with a single scan […] We therefore combine the moving classes with the corresponding non-moving class resulting in a total number of 19 classes for training and evaluation”\n“In the case of a rotating LiDAR, all points of a single turn can be projected to an image by using a spherical projection”\n“classes with few examples, like motorcyclists and trucks, seem to be more difficult”\n“classes with only a small number of points in a single point cloud, like bicycles and poles, are hard classes”\n\n\n\nsource\n\n\n\n\n SemanticKITTIDataset (data_path, split='train', transform=None,\n                       remapping_rules=None)\n\nLoad the SemanticKITTI data in a pytorch Dataset object.\n\n\nExported source\nclass SemanticKITTIDataset(Dataset):\n    \"Load the SemanticKITTI data in a pytorch Dataset object.\"\n    def __init__(self, data_path, split='train', transform=None, remapping_rules=None):\n        data_path = Path(data_path)\n        yaml_path = data_path/'semantic-kitti.yaml'\n        self.velodyne_path = data_path/'data_odometry_velodyne/dataset/sequences'\n        self.labels_path = data_path/'data_odometry_labels/dataset/sequences'\n\n        with open(yaml_path, 'r') as file:\n            metadata = yaml.safe_load(file)\n        \n        if split != 'none':\n            sequences = metadata['split'][split]\n            velodyne_fns = []\n            for seq in sequences:\n                velodyne_fns += list(self.velodyne_path.rglob(f'*{seq:02}/velodyne/*.bin'))\n            \n            self.frame_ids = [fn.stem for fn in velodyne_fns]\n            self.frame_sequences = [fn.parts[-3] for fn in velodyne_fns]\n        self.split = split\n        \n        self.labels_dict = metadata['labels']\n        \n        self.content = metadata['content']\n        max_key = sorted(self.content.keys())[-1]\n        self.content_np = np.zeros((max_key+1,), dtype=np.float32)\n        for k, v in self.content.items():\n            self.content_np[k] = v\n        \n        self.learning_map = metadata['learning_map']\n        self.learning_map_np = np.zeros((max_key+1,), dtype=np.uint32)\n        for k, v in self.learning_map.items():\n            self.learning_map_np[k] = v\n        \n        self.learning_map_inv = metadata['learning_map_inv']\n        self.learning_map_inv_np = np.zeros((len(self.learning_map_inv),), dtype=np.uint32)\n        content_sum_np = np.zeros_like(self.learning_map_inv_np, dtype=np.float32)\n        for k, v in self.learning_map_inv.items():\n            self.learning_map_inv_np[k] = v\n            content_sum_np[k] = self.content_np[self.learning_map_np == k].sum()\n        self.content_weights = 1./content_sum_np\n        \n        self.color_map_bgr = metadata['color_map']\n        self.color_map_rgb_np = np.zeros((max_key+1, 3), dtype=np.float32)\n        for k, v in self.color_map_bgr.items():\n            self.color_map_rgb_np[k] = np.array(v[::-1], np.float32)\n        \n        self.transform = transform\n        self.is_test = (split == 'test')\n\n        if remapping_rules is not None:\n            self.learning_remap(remapping_rules)\n    \n    def learning_remap(self, remapping_rules):\n        new_map_np = np.zeros_like(self.learning_map_np, dtype=np.uint32)\n        max_key = sorted(remapping_rules.values())[-1]\n        new_map_inv_np = np.zeros((max_key+1,), dtype=np.uint32)\n        for k, v in remapping_rules.items():\n            new_map_np[self.learning_map_np == k] = v\n            if new_map_inv_np[v] == 0:\n                new_map_inv_np[v] = self.learning_map_inv_np[k]\n        \n        new_content_sum_np = np.zeros_like(new_map_inv_np, dtype=np.float32)\n        for k in range(len(new_map_inv_np)):\n            new_content_sum_np[k] = self.content_np[new_map_np == k].sum()\n        \n        self.learning_map_np = new_map_np\n        self.learning_map_inv_np = new_map_inv_np\n        self.content_weights = 1./new_content_sum_np\n    \n    def set_transform(self, transform):\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.frame_ids)\n\n    def __getitem__(self, idx):\n        assert self.split != 'none'\n        \n        frame_id = self.frame_ids[idx]\n        frame_sequence = self.frame_sequences[idx]\n        \n        frame_path = self.velodyne_path/frame_sequence/'velodyne'/(frame_id + '.bin')\n        with open(frame_path, 'rb') as f:\n            frame = np.fromfile(f, dtype=np.float32).reshape(-1, 4)\n        \n        label = None\n        mask = None\n        if not self.is_test:\n            label_path = self.labels_path/frame_sequence/'labels'/(frame_id + '.label')\n            with open(label_path, 'rb') as f:\n                label = np.fromfile(f, dtype=np.uint32)\n                label = label & 0xFFFF\n            label = self.learning_map_np[label]\n            mask = label != 0   # see the field *learning_ignore* in the yaml file\n        \n        item = {\n            'frame': frame,\n            'label': label,\n            'mask': mask\n        }\n        if self.transform:\n            item = self.transform(item)\n        \n        return item\n\n\nTo use the SemanticKITTIDataset class, start by downloading the required files from the provided links on their official website. Then, extract these files into a folder data, at the root of your workspace:\n\ndata_odometry_velodyne\ndata_odometry_labels\n\nThen, download one of the following files with the metadata from the semantic-kitti-api to the same folder:\n\nsemantic-kitti.yaml (preferred one)\nsemantic-kitti-all.yaml\nsemantic-kitti-coarse.yaml\nsemantic-kitti-mos.yaml\n\nSummarizing quote from the semantic-kitti-api about the content of these files:\n\n\nlabels: dictionary which maps numeric labels […] to a string class. Example: 10: \"car\"\ncolor_map: dictionary which maps numeric labels […] to a bgr color for visualization. Example 10: [245, 150, 100] # car, blue-ish\ncontent: […] a ratio to the number of total points in the dataset. […] used to calculate the weights for the cross entropy […] (in order to handle class imbalance).\nlearning_map: dictionary which maps each class label to its cross entropy equivalent, for learning. This is done to mask undesired classes, map different classes together, and because the cross entropy expects a value in [0, numclasses - 1]. […] Examples:\n  0 : 0     # \"unlabeled\"\n  1 : 0     # \"outlier\" to \"unlabeled\" -&gt; gets ignored in training, with unlabeled\n  10: 1     # \"car\"\n  252: 1    # \"moving-car\" to \"car\" -&gt; gets merged with static car class\nlearning_map_inv: dictionary with inverse of the previous mapping […] (for saving point cloud predictions in original label format [and to use the color_map dictionary for visualization]). […]\nlearning_ignore: dictionary that contains for each cross entropy class if it will be ignored during training and evaluation or not. For example, class unlabeled gets ignored in both training and evaluation.\nsplit: contains 3 lists, with the sequence numbers for training, validation, and evaluation.\n\n\nThe SemanticKITTIDataset class stores this metadata in the following attributes:\n\nlabels:\n\nself.labels_dict: the original dictionary\n\ncolor_map:\n\nself.color_map_bgr: the original dictionary\nself.color_map_rgb_np: the colors converted into RGB format and the dictionary converted into a np.array for performance improvements\n\ncontent:\n\nself.content: the original dictionary\nself.content_np: the dictionary converted into a np.array for performance improvements\nself.content_weights: the calculated weights as the inverse of the original content ratios to use with the cross entropy in order to reduce problems due to class imbalance\n\nlearning_map:\n\nself.learning_map: the original dictionary\nself.learning_map_np: the dictionary converted into a np.array for performance improvements\n\nlearning_map_inv:\n\nself.learning_map_inv: the original dictionary\nself.learning_map_inv_np: the dictionary converted into a np.array for performance improvements\n\n\nLastly, use the following code to get the training data in its original format:\n\ndata_path = '/workspace/data'\nds = SemanticKITTIDataset(data_path)\nval_ds = SemanticKITTIDataset(data_path, split='valid')\ntest_ds = SemanticKITTIDataset(data_path, split='test')\nprint(f'train size:\\t{len(ds)}\\nval size:\\t{len(val_ds)}\\ntest size:\\t{len(test_ds)}')\n\ntrain size: 19130\nval size:   4071\ntest size:  20351\n\n\nWithout setting any transforms, the data is simply read into numpy arrays and stored into a dictionary with the following key-value pairs:\n\nframe: the point cloud readings in a particular timestamp as a sequence of 4D float vectors (i.e. x, y, z, reflectance);\nlabel: for the training set, a sequence of integer labels for each point in the corresponding frame (these labels take into consideration the information in the previously downloaded yaml file);\nmask: for the training set, a sequence of booleans for each point in the corresponding frame indicating if the point sould not be ignored during training;\n\nThe mask is hardcoded to ignore only the unlabeled class.\n\n\n\n\n\n\nWarning\n\n\n\nIf you are using the semantic-kitti-coarse.yaml, remember to update the mask to ignore the dynamic object class as well.\n\n\n\nitem = ds[128]\nframe = item['frame']\nframe, frame.shape\n\n(array([[35.963676  ,  0.06396142,  1.4249582 ,  0.28      ],\n        [42.710087  ,  0.21486942,  1.6528604 ,  0.11      ],\n        [36.184536  ,  0.2918239 ,  1.4328097 ,  0.15      ],\n        ...,\n        [ 3.793152  , -1.3955092 , -1.7389623 ,  0.4       ],\n        [ 4.0309873 , -1.4794569 , -1.8559116 ,  0.07      ],\n        [ 3.8513408 , -1.3985021 , -1.7649618 ,  0.        ]],\n       dtype=float32),\n (126063, 4))\n\n\n\nlabel = item['label']\nlabel, label.shape, label.dtype, set(label)\n\n(array([15, 15, 15, ...,  9,  9,  9], dtype=uint32),\n (126063,),\n dtype('uint32'),\n {0, 1, 2, 3, 9, 11, 13, 14, 15, 16, 17, 18, 19})\n\n\n\nmask = item['mask']\nmask, mask.shape, mask.dtype\n\n(array([ True,  True,  True, ...,  True,  True,  True]),\n (126063,),\n dtype('bool'))",
    "crumbs": [
      "behley2019iccv"
    ]
  },
  {
    "objectID": "behley2019iccv.html#semantic-kitti-api",
    "href": "behley2019iccv.html#semantic-kitti-api",
    "title": "behley2019iccv",
    "section": "",
    "text": "This is an adaptation of the code from the semantic-kitti-api as a pytorch Dataset.\nSummarizing quotes from the paper:\n\n\n\n\n“we annotated all 22 sequences of the odometry benchmark of the KITTI Vision Benchmark consisting of over 43 000 scans” ### 3. The SemanticKITTI Dataset\n“showing inner city traffic, residential areas, but also highway scenes and countryside roads around Karlsruhe, Germany”\n“splitting sequences 00 to 10 as training set, and 11 to 21 as test set. For consistency […], we adopt the same division for our training and test set [as the KITTI Vision Benchmark]”\n“we provide 23201 full 3D scans for training and 20351 for testing”\n“we do not distinguish between persons riding a vehicle and the vehicle, but label the vehicle and the person as either bicyclist or motorcyclist”\n“we have 28 classes, where 6 classes are assigned the attribute moving or non-moving, and one outlier class is included for erroneous laser measurements caused by reflections or other effects” #### 3.1 Labeling process\n“we first register and loop close the sequences using an off-the-shelf laser-based SLAM system”\n“For three sequences, we had to manually add loop closure constraints to get correctly loop closed trajectories”\n“We subdivide the sequence of point clouds into tiles of 100 m by 100 m […] This enables us to label all scans consistently even when we encounter temporally distant loop closures”\n“An annotator needs on average 4.5 hours per tile, when labeling residential areas […], and needs on average 1.5 hours for labeling a highway tile.”\n“the whole dataset comprises 518 tiles” ### 4. Evaluation of Semantic Segmentation #### 4.1 Single Scan Experiments\n“we use 25 instead of 28 classes, ignoring outlier, other-structure, and other-object during training and inference”\n“we cannot expect to distinguish moving from non-moving objects with a single scan […] We therefore combine the moving classes with the corresponding non-moving class resulting in a total number of 19 classes for training and evaluation”\n“In the case of a rotating LiDAR, all points of a single turn can be projected to an image by using a spherical projection”\n“classes with few examples, like motorcyclists and trucks, seem to be more difficult”\n“classes with only a small number of points in a single point cloud, like bicycles and poles, are hard classes”\n\n\n\nsource\n\n\n\n\n SemanticKITTIDataset (data_path, split='train', transform=None,\n                       remapping_rules=None)\n\nLoad the SemanticKITTI data in a pytorch Dataset object.\n\n\nExported source\nclass SemanticKITTIDataset(Dataset):\n    \"Load the SemanticKITTI data in a pytorch Dataset object.\"\n    def __init__(self, data_path, split='train', transform=None, remapping_rules=None):\n        data_path = Path(data_path)\n        yaml_path = data_path/'semantic-kitti.yaml'\n        self.velodyne_path = data_path/'data_odometry_velodyne/dataset/sequences'\n        self.labels_path = data_path/'data_odometry_labels/dataset/sequences'\n\n        with open(yaml_path, 'r') as file:\n            metadata = yaml.safe_load(file)\n        \n        if split != 'none':\n            sequences = metadata['split'][split]\n            velodyne_fns = []\n            for seq in sequences:\n                velodyne_fns += list(self.velodyne_path.rglob(f'*{seq:02}/velodyne/*.bin'))\n            \n            self.frame_ids = [fn.stem for fn in velodyne_fns]\n            self.frame_sequences = [fn.parts[-3] for fn in velodyne_fns]\n        self.split = split\n        \n        self.labels_dict = metadata['labels']\n        \n        self.content = metadata['content']\n        max_key = sorted(self.content.keys())[-1]\n        self.content_np = np.zeros((max_key+1,), dtype=np.float32)\n        for k, v in self.content.items():\n            self.content_np[k] = v\n        \n        self.learning_map = metadata['learning_map']\n        self.learning_map_np = np.zeros((max_key+1,), dtype=np.uint32)\n        for k, v in self.learning_map.items():\n            self.learning_map_np[k] = v\n        \n        self.learning_map_inv = metadata['learning_map_inv']\n        self.learning_map_inv_np = np.zeros((len(self.learning_map_inv),), dtype=np.uint32)\n        content_sum_np = np.zeros_like(self.learning_map_inv_np, dtype=np.float32)\n        for k, v in self.learning_map_inv.items():\n            self.learning_map_inv_np[k] = v\n            content_sum_np[k] = self.content_np[self.learning_map_np == k].sum()\n        self.content_weights = 1./content_sum_np\n        \n        self.color_map_bgr = metadata['color_map']\n        self.color_map_rgb_np = np.zeros((max_key+1, 3), dtype=np.float32)\n        for k, v in self.color_map_bgr.items():\n            self.color_map_rgb_np[k] = np.array(v[::-1], np.float32)\n        \n        self.transform = transform\n        self.is_test = (split == 'test')\n\n        if remapping_rules is not None:\n            self.learning_remap(remapping_rules)\n    \n    def learning_remap(self, remapping_rules):\n        new_map_np = np.zeros_like(self.learning_map_np, dtype=np.uint32)\n        max_key = sorted(remapping_rules.values())[-1]\n        new_map_inv_np = np.zeros((max_key+1,), dtype=np.uint32)\n        for k, v in remapping_rules.items():\n            new_map_np[self.learning_map_np == k] = v\n            if new_map_inv_np[v] == 0:\n                new_map_inv_np[v] = self.learning_map_inv_np[k]\n        \n        new_content_sum_np = np.zeros_like(new_map_inv_np, dtype=np.float32)\n        for k in range(len(new_map_inv_np)):\n            new_content_sum_np[k] = self.content_np[new_map_np == k].sum()\n        \n        self.learning_map_np = new_map_np\n        self.learning_map_inv_np = new_map_inv_np\n        self.content_weights = 1./new_content_sum_np\n    \n    def set_transform(self, transform):\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.frame_ids)\n\n    def __getitem__(self, idx):\n        assert self.split != 'none'\n        \n        frame_id = self.frame_ids[idx]\n        frame_sequence = self.frame_sequences[idx]\n        \n        frame_path = self.velodyne_path/frame_sequence/'velodyne'/(frame_id + '.bin')\n        with open(frame_path, 'rb') as f:\n            frame = np.fromfile(f, dtype=np.float32).reshape(-1, 4)\n        \n        label = None\n        mask = None\n        if not self.is_test:\n            label_path = self.labels_path/frame_sequence/'labels'/(frame_id + '.label')\n            with open(label_path, 'rb') as f:\n                label = np.fromfile(f, dtype=np.uint32)\n                label = label & 0xFFFF\n            label = self.learning_map_np[label]\n            mask = label != 0   # see the field *learning_ignore* in the yaml file\n        \n        item = {\n            'frame': frame,\n            'label': label,\n            'mask': mask\n        }\n        if self.transform:\n            item = self.transform(item)\n        \n        return item\n\n\nTo use the SemanticKITTIDataset class, start by downloading the required files from the provided links on their official website. Then, extract these files into a folder data, at the root of your workspace:\n\ndata_odometry_velodyne\ndata_odometry_labels\n\nThen, download one of the following files with the metadata from the semantic-kitti-api to the same folder:\n\nsemantic-kitti.yaml (preferred one)\nsemantic-kitti-all.yaml\nsemantic-kitti-coarse.yaml\nsemantic-kitti-mos.yaml\n\nSummarizing quote from the semantic-kitti-api about the content of these files:\n\n\nlabels: dictionary which maps numeric labels […] to a string class. Example: 10: \"car\"\ncolor_map: dictionary which maps numeric labels […] to a bgr color for visualization. Example 10: [245, 150, 100] # car, blue-ish\ncontent: […] a ratio to the number of total points in the dataset. […] used to calculate the weights for the cross entropy […] (in order to handle class imbalance).\nlearning_map: dictionary which maps each class label to its cross entropy equivalent, for learning. This is done to mask undesired classes, map different classes together, and because the cross entropy expects a value in [0, numclasses - 1]. […] Examples:\n  0 : 0     # \"unlabeled\"\n  1 : 0     # \"outlier\" to \"unlabeled\" -&gt; gets ignored in training, with unlabeled\n  10: 1     # \"car\"\n  252: 1    # \"moving-car\" to \"car\" -&gt; gets merged with static car class\nlearning_map_inv: dictionary with inverse of the previous mapping […] (for saving point cloud predictions in original label format [and to use the color_map dictionary for visualization]). […]\nlearning_ignore: dictionary that contains for each cross entropy class if it will be ignored during training and evaluation or not. For example, class unlabeled gets ignored in both training and evaluation.\nsplit: contains 3 lists, with the sequence numbers for training, validation, and evaluation.\n\n\nThe SemanticKITTIDataset class stores this metadata in the following attributes:\n\nlabels:\n\nself.labels_dict: the original dictionary\n\ncolor_map:\n\nself.color_map_bgr: the original dictionary\nself.color_map_rgb_np: the colors converted into RGB format and the dictionary converted into a np.array for performance improvements\n\ncontent:\n\nself.content: the original dictionary\nself.content_np: the dictionary converted into a np.array for performance improvements\nself.content_weights: the calculated weights as the inverse of the original content ratios to use with the cross entropy in order to reduce problems due to class imbalance\n\nlearning_map:\n\nself.learning_map: the original dictionary\nself.learning_map_np: the dictionary converted into a np.array for performance improvements\n\nlearning_map_inv:\n\nself.learning_map_inv: the original dictionary\nself.learning_map_inv_np: the dictionary converted into a np.array for performance improvements\n\n\nLastly, use the following code to get the training data in its original format:\n\ndata_path = '/workspace/data'\nds = SemanticKITTIDataset(data_path)\nval_ds = SemanticKITTIDataset(data_path, split='valid')\ntest_ds = SemanticKITTIDataset(data_path, split='test')\nprint(f'train size:\\t{len(ds)}\\nval size:\\t{len(val_ds)}\\ntest size:\\t{len(test_ds)}')\n\ntrain size: 19130\nval size:   4071\ntest size:  20351\n\n\nWithout setting any transforms, the data is simply read into numpy arrays and stored into a dictionary with the following key-value pairs:\n\nframe: the point cloud readings in a particular timestamp as a sequence of 4D float vectors (i.e. x, y, z, reflectance);\nlabel: for the training set, a sequence of integer labels for each point in the corresponding frame (these labels take into consideration the information in the previously downloaded yaml file);\nmask: for the training set, a sequence of booleans for each point in the corresponding frame indicating if the point sould not be ignored during training;\n\nThe mask is hardcoded to ignore only the unlabeled class.\n\n\n\n\n\n\nWarning\n\n\n\nIf you are using the semantic-kitti-coarse.yaml, remember to update the mask to ignore the dynamic object class as well.\n\n\n\nitem = ds[128]\nframe = item['frame']\nframe, frame.shape\n\n(array([[35.963676  ,  0.06396142,  1.4249582 ,  0.28      ],\n        [42.710087  ,  0.21486942,  1.6528604 ,  0.11      ],\n        [36.184536  ,  0.2918239 ,  1.4328097 ,  0.15      ],\n        ...,\n        [ 3.793152  , -1.3955092 , -1.7389623 ,  0.4       ],\n        [ 4.0309873 , -1.4794569 , -1.8559116 ,  0.07      ],\n        [ 3.8513408 , -1.3985021 , -1.7649618 ,  0.        ]],\n       dtype=float32),\n (126063, 4))\n\n\n\nlabel = item['label']\nlabel, label.shape, label.dtype, set(label)\n\n(array([15, 15, 15, ...,  9,  9,  9], dtype=uint32),\n (126063,),\n dtype('uint32'),\n {0, 1, 2, 3, 9, 11, 13, 14, 15, 16, 17, 18, 19})\n\n\n\nmask = item['mask']\nmask, mask.shape, mask.dtype\n\n(array([ True,  True,  True, ...,  True,  True,  True]),\n (126063,),\n dtype('bool'))",
    "crumbs": [
      "behley2019iccv"
    ]
  },
  {
    "objectID": "behley2019iccv.html#lidar-range-image-projections",
    "href": "behley2019iccv.html#lidar-range-image-projections",
    "title": "behley2019iccv",
    "section": "LiDAR Range Image Projections",
    "text": "LiDAR Range Image Projections\nAs referenced in the section 4.1 Single Scan Experiments of the SemanticKITTI paper: &gt; - “In the case of a rotating LiDAR, all points of a single turn can be projected to an image by using a spherical projection”\nFrom the best of our knowledge, the idea of applying projections to turn LiDAR point clouds into range images can be found in the following papers categorized by the projection algorithm and ordered chronologically:\n\nSperical/cylindrical projections\n\nSqueezeSeg: Convolutional Neural Nets with Recurrent CRF for Real-Time Road-Object Segmentation from 3D LiDAR Point Cloud\nRange-Image: Incorporating sensor topology for LiDAR point cloud processing\nRIU-Net: Embarrassingly simple semantic segmentation of 3D LiDAR point cloud\n\nScan Unfolding projection\n\nScan-based Semantic Segmentation of LiDAR Point Clouds: An Experimental Study\nEfficientLPS: Efficient LiDAR Panoptic Segmentation\n\n\n\nsource\n\nSphericalProjection\n\n SphericalProjection (fov_up_deg, fov_down_deg, W, H)\n\nCalculate yaw and pitch angles for each point and quantize these angles into image grid.\n\n\nExported source\nclass SphericalProjection:\n    \"Calculate yaw and pitch angles for each point and quantize these angles into image grid.\"\n    def __init__(self, fov_up_deg, fov_down_deg, W, H):\n        self.fov_up_rad = (fov_up_deg/180.)*np.pi\n        self.fov_down_rad = (fov_down_deg/180.)*np.pi\n        self.fov_rad = self.fov_up_rad - self.fov_down_rad\n        self.W = W\n        self.H = H\n    \n    def get_xy_projections(self, scan_xyz, depth):\n        # get angles of all points\n        yaw = np.arctan2(scan_xyz[:,1], scan_xyz[:,0])\n        pitch = np.arcsin(scan_xyz[:,2] / depth)\n        \n        # get projections in image coords (between [0.0, 1.0])\n        proj_x = 0.5*(1. + yaw/np.pi)\n        proj_y = (self.fov_up_rad - pitch)/self.fov_rad\n\n        # just making sure nothing wierd happened with the np.arctan2 function\n        assert proj_x.min() &gt;= 0.\n        assert proj_x.max() &lt;= 1.\n        # filter points outside the fov as outliers\n        outliers = (proj_y &lt; 0.)|(proj_y &gt;= 1.)\n        \n        # scale to image size using angular resolution (between [0.0, W/H])\n        proj_x *= self.W\n        proj_y *= self.H\n        \n        # round and clamp to use as indices (between [0, W/H - 1])\n        proj_x = np.floor(proj_x)\n        proj_x = np.clip(proj_x, 0, self.W - 1).astype(np.int32)\n        \n        proj_y = np.floor(proj_y)\n        proj_y = np.clip(proj_y, 0, self.H - 1).astype(np.int32)\n        \n        return proj_x, proj_y, outliers\n\n\nThe following images from the Medium article Spherical Projection for Point Clouds illustrate the process in the spherical projection calculation:\n\n\n\nFigure 1: Topology of the LiDAR sensor. In the image, the author shows an example with a LiDAR with 16 lasers.\n\n\n\n\n\nFigure 2: Spherical coordinate system and its relation to the projection image grid.\n\n\n\nsource\n\n\nUnfoldingProjection\n\n UnfoldingProjection (W, H)\n\nAssume the points are sorted in increasing yaw order and line number.\n\n\nExported source\nclass UnfoldingProjection:\n    \"Assume the points are sorted in increasing yaw order and line number.\"\n    def __init__(self, W, H):\n        self.W = W\n        self.H = H\n    \n    def get_xy_projections(self, scan_xyz, depth):\n        # get yaw angles of all points\n        yaw = np.arctan2(scan_xyz[:,1], scan_xyz[:,0])\n        \n        # rectify yaw value to be between ]0, 2*pi[\n        yaw[yaw &lt; 0] += 2.*np.pi\n        \n        # scale to image size\n        proj_x  = np.floor(self.W*0.5*yaw/np.pi).astype(np.int32)\n        \n        # just making sure nothing wierd happened with the np.arctan2 or the np.floor functions\n        assert proj_x.min() &gt;= 0\n        assert proj_x.max() &lt; self.W\n        \n        # find discontinuities (\"jumps\") from scan completing cycle\n        jump = yaw[1:] - yaw[:-1] &lt; -np.pi\n        jump = np.concatenate((np.zeros(1), jump))\n        \n        # every jump indicates a new scan row\n        proj_y = jump.cumsum().astype(np.int32)\n        \n        # for debugging only\n        if proj_y.max() &gt; self.H - 1:\n            print(proj_y.max())\n        assert proj_y.max() &lt;= self.H - 1\n        \n        return proj_x, proj_y, None\n\n\nIf the points are sorted in increasing yaw order and line number (which is the case in the SemanticKITTI dataset), then the projection lines can be found by making sure that discontinuities in yaw sequences come from scan completing cycles. That way, we can find these discontinuities and then quantize the yaw angles in the projection lines so they can be assigned to each pixel in the image grid.\nTo make sure the discontinuites happen from one scan line to another, we take into consideration that the vehicle used in the SemanticKITTI dataset has the following sensor configuration:\n\nWe also know that the LiDAR sensor rotates its lasers in a counter-clockwise direction and that a LiDAR frame starts and finishes when the lasers cross the x axis. Therefore, we calculate the yaw angles with the np.arctan2 function and then add \\(2\\pi\\) to the ones that are negative. The plot below, shows this trick on toy example data:\n\nfake_yaw_raw_data = np.concatenate((np.linspace(0.01,np.pi), np.linspace(-np.pi, -0.01)))\nfake_data_x = np.cos(fake_yaw_raw_data)\nfake_data_y = np.sin(fake_yaw_raw_data)\n\nplt.scatter(fake_data_y, fake_data_x);\n\n\n\n\n\n\n\n\n\nfake_yaw = np.arctan2(fake_data_y, fake_data_x)\nfake_yaw_corrected = np.arctan2(fake_data_y, fake_data_x)\nfake_yaw_corrected[fake_yaw_corrected &lt; 0] += 2.*np.pi\nplt.plot(fake_yaw)\nplt.plot(fake_yaw_corrected);\n\n\n\n\n\n\n\n\nFrom this trick, we can simply detect the end of each projection line by checking when the diference between adjacent angles is smaller than some threshold. After some experimentation with different samples, a threshold of \\(-\\pi\\) seems to be robust even if there are some missing readings from the LiDAR.\n\ntwo_lines = np.concatenate((fake_yaw_corrected, fake_yaw_corrected))\njump = two_lines[1:] - two_lines[:-1] &lt; -np.pi\nplt.plot(jump);\n\n\n\n\n\n\n\n\n\nsource\n\n\nProjectionTransform\n\n ProjectionTransform (projection)\n\nPytorch transform that turns a point cloud frame and its respective label and mask arrays into images in given projection style.\n\n\nExported source\nclass ProjectionTransform(nn.Module):\n    \"Pytorch transform that turns a point cloud frame and its respective label and mask arrays into images in given projection style.\"\n    def __init__(self, projection):\n        super().__init__()\n        self.projection = projection\n        self.W = projection.W\n        self.H = projection.H\n        \n    def forward(self, item):\n        frame = item['frame']\n        label = item['label']\n        mask = item['mask']\n        \n        # get point_cloud components\n        scan_xyz = frame[:,:3]\n        reflectance = frame[:, 3]\n\n        assert reflectance.max() &lt;= 1.\n        assert reflectance.min() &gt;= 0.\n\n        # get depths of all points\n        depth = np.linalg.norm(scan_xyz, 2, axis=1)\n\n        # get projections and outliers\n        proj_x, proj_y, outliers = self.projection.get_xy_projections(scan_xyz, depth)\n\n        # filter outliers\n        if outliers is not None:\n            proj_x = proj_x[~outliers]\n            proj_y = proj_y[~outliers]\n            scan_xyz = scan_xyz[~outliers]\n            reflectance = reflectance[~outliers]\n            depth = depth[~outliers]\n            if label is not None:\n                label = label[~outliers]\n                mask = mask[~outliers]\n        \n        # order in decreasing depth\n        order = np.argsort(depth)[::-1]\n        info_list = [\n            scan_xyz,\n            reflectance[..., np.newaxis],\n            depth[..., np.newaxis]\n        ]\n        if label is not None:\n            info_list += [mask[..., np.newaxis]]\n            info_list += [label[..., np.newaxis]]\n            \n        scan_info = np.concatenate(info_list, axis=-1)\n        scan_info = scan_info[order]\n        proj_y = proj_y[order]\n        proj_x = proj_x[order]\n        \n        # setup the image tensor\n        projections_img = np.zeros((self.H, self.W, 2+len(info_list)), dtype=np.float32)\n        projections_img[:,:,-1] -= 1 # this helps to identify points in the projection with no LiDAR readings\n        projections_img[proj_y, proj_x] = scan_info\n        \n        if label is not None:\n            frame_img = projections_img[:,:,:-2]\n            label_img = projections_img[:,:,-1].astype(np.int32)\n            mask_img = projections_img[:,:,-2].astype(bool)\n            mask_img = mask_img & (label_img &gt; -1)\n        else:\n            frame_img = projections_img\n            label_img = None\n            mask_img = projections_img[:,:,-1] &gt;= 0\n        \n        item = {\n            'frame': frame_img,\n            'label': label_img,\n            'mask': mask_img,\n        }\n        return item\n\n\nWith one of the projection algorithms described previously, we can construct a ProjectionTransform object that can be used with the SemanticKITTIDataset. Here is an example on how to use it:\n\nproj = SphericalProjection(fov_up_deg=3., fov_down_deg=-25., W=1024, H=64) # these values were taken from [https://github.com/PRBonn/semantic-kitti-api/blob/master/auxiliary/laserscan.py]\ntfms = ProjectionTransform(proj)\nds.set_transform(tfms)\nitem = ds[128]\n\nAs we can see below, the data is still returned as numpy arrays, but their shapes have changed as follows:\n\nframe: an image of HxW resolution with 5 channels with floats (i.e. x, y, z, reflectance, depth);\nlabel: an image of HxW resolution with a single channel with integers;\nmask: an image of HxW resolution with a single channel with booleans.\n\nThere are a lot of pixels in these projection images that have no corresponding LiDAR readings to be projected. Hence, these pixels were filled with:\n\nin frame: array([0., 0., 0., 0., 0.])\nin label: -1\nin mask: False\n\nThe mask image provides a convenient boolean mask to easily identify pixels with valid LiDAR readings marked as True and pixels with no LiDAR readings marked as False.\n\nframe_img = item['frame']\nframe_img[:2], frame_img.shape, frame_img.dtype\n\n(array([[[  0.       ,   0.       ,   0.       ,   0.       ,\n            0.       ],\n         [  0.       ,   0.       ,   0.       ,   0.       ,\n            0.       ],\n         [  0.       ,   0.       ,   0.       ,   0.       ,\n            0.       ],\n         ...,\n         [  0.       ,   0.       ,   0.       ,   0.       ,\n            0.       ],\n         [  0.       ,   0.       ,   0.       ,   0.       ,\n            0.       ],\n         [  0.       ,   0.       ,   0.       ,   0.       ,\n            0.       ]],\n \n        [[  0.       ,   0.       ,   0.       ,   0.       ,\n            0.       ],\n         [  0.       ,   0.       ,   0.       ,   0.       ,\n            0.       ],\n         [  0.       ,   0.       ,   0.       ,   0.       ,\n            0.       ],\n         ...,\n         [-51.240726 ,   0.7981246,   2.022291 ,   0.21     ,\n           51.286827 ],\n         [-50.624985 ,   0.3961611,   2.000314 ,   0.17     ,\n           50.66604  ],\n         [-51.28182  ,   0.2415634,   2.0237277,   0.13     ,\n           51.322304 ]]], dtype=float32),\n (64, 1024, 5),\n dtype('float32'))\n\n\n\nlabel_img = item['label']\nlabel_img[:2], label_img.shape, label_img.dtype, set(label_img.flatten())\n\n(array([[-1, -1, -1, ..., -1, -1, -1],\n        [-1, -1, -1, ...,  0,  0,  0]], dtype=int32),\n (64, 1024),\n dtype('int32'),\n {-1, 0, 1, 9, 11, 13, 14, 15, 16, 17, 18, 19})\n\n\n\nmask_img = item['mask']\nmask_img[:2], mask_img.shape, mask_img.dtype\n\n(array([[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]),\n (64, 1024),\n dtype('bool'))\n\n\n\nsource\n\n\nProjectionVizTransform\n\n ProjectionVizTransform (color_map_rgb_np, learning_map_inv_np,\n                         scaling_values)\n\nPytorch transform to preprocess projection images for proper visualization.\n\n\nExported source\nclass ProjectionVizTransform(nn.Module):\n    \"Pytorch transform to preprocess projection images for proper visualization.\"\n    def __init__(self, color_map_rgb_np, learning_map_inv_np, scaling_values):\n        super().__init__()\n        self.color_map_rgb_np = color_map_rgb_np\n        self.learning_map_inv_np = learning_map_inv_np\n        self.scaling_values = scaling_values\n    \n    def scale(self, img, min_value, max_value):\n        assert img.max() &lt;= max_value\n        assert img.min() &gt;= min_value\n        assert max_value &gt; min_value\n        \n        img = img.clip(min_value, max_value)\n        return (255.*(img - min_value)/(max_value - min_value)).astype(int)\n    \n    def forward(self, item):\n        frame_img = item['frame']\n        label_img = item['label']\n        mask_img = item['mask']\n        \n        normalized_frame_img = None\n        if frame_img is not None:\n            x = self.scale(frame_img[:,:,0], self.scaling_values[\"x\"][\"min\"], self.scaling_values[\"x\"][\"max\"])\n            y = self.scale(frame_img[:,:,1], self.scaling_values[\"y\"][\"min\"], self.scaling_values[\"y\"][\"max\"])\n            z = self.scale(frame_img[:,:,2], self.scaling_values[\"z\"][\"min\"], self.scaling_values[\"z\"][\"max\"])\n            r = self.scale(frame_img[:,:,3], self.scaling_values[\"r\"][\"min\"], self.scaling_values[\"r\"][\"max\"])\n            d = self.scale(frame_img[:,:,4], self.scaling_values[\"d\"][\"min\"], self.scaling_values[\"d\"][\"max\"])\n            normalized_frame_img = np.stack((x, y, z, r, d), axis=-1)\n            normalized_frame_img[mask_img == False] *= 0\n\n        colored_label_img = None\n        if label_img is not None:\n            label_img[mask_img] = self.learning_map_inv_np[label_img[mask_img]]\n            colored_label_img = np.zeros(label_img.shape + (3,))\n            colored_label_img[mask_img] = self.color_map_rgb_np[label_img[mask_img]]\n            colored_label_img = colored_label_img.astype(int)\n        \n        item = {\n            'frame': normalized_frame_img,\n            'label': colored_label_img,\n            'mask': mask_img,\n        }\n        return item\n\n\nIn order to plot comparable visualizations of the projections, we need to standardize the scale of the float channels from the frame projection and map the label integers to their corresponding RGB color defined in the color_map dict from the yaml file that was downloaded before.\nAfter experimenting with several samples from the dataset, the scale values in the table below were found to produce reasonable results. If you are using a dataset other than SemanticKITTI, these values may need to be adjusted accordingly.\n\n\n\nFeature\nMin\nMax\n\n\n\n\nx\n-100.0\n100.0\n\n\ny\n-100.0\n100.0\n\n\nz\n-31.0\n5.0\n\n\nr\n0.0\n1.0\n\n\nd\n0.0\n100.0\n\n\n\nHere is an example on how to use and compose the previous transforms with the torchvision.transforms.v2 module, and then visualize the resulting images with the matplolib.pyplot module:\n\nsource\n\n\nplot_projections\n\n plot_projections (img, label, channels_names)\n\n\n\nExported source\ndef plot_projections(img, label, channels_names):\n    assert len(channels_names) == img.shape[-1]\n    channels = [img[:,:,i] for i in range(len(channels_names))]\n    if label is not None:\n        channels_names.append('label')\n        channels.append(label)\n    fig, axs = plt.subplots(len(channels_names), 1, figsize=(20,10), layout='compressed')\n    for ax, channel, title in zip(axs, channels, channels_names):\n        ax.imshow(channel)\n        ax.set_title(title)\n        ax.axis('off')\n\n\n\nproj = SphericalProjection(fov_up_deg=3., fov_down_deg=-25., W=1024, H=64) # these values were taken from [https://github.com/PRBonn/semantic-kitti-api/blob/master/auxiliary/laserscan.py]\nscaling_values = {\n    \"x\" : {\"min\": -100., \"max\":100.},\n    \"y\" : {\"min\": -100., \"max\":100.},\n    \"z\" : {\"min\": -31., \"max\":5.},\n    \"r\" : {\"min\": 0., \"max\":1.},\n    \"d\" : {\"min\": 0., \"max\":100.}\n}\ntfms = v2.Compose([\n    ProjectionTransform(proj),\n    ProjectionVizTransform(ds.color_map_rgb_np, ds.learning_map_inv_np, scaling_values),\n])\nds.set_transform(tfms)\nitem = ds[128]\nimg = item['frame']\nlabel = item['label']\n\nplot_projections(img, label, ['x', 'y', 'z', 'r', 'd'])\n\n\n\n\n\n\n\n\n\nproj = UnfoldingProjection(W=1024, H=64)\nscaling_values = {\n    \"x\" : {\"min\": -100., \"max\":100.},\n    \"y\" : {\"min\": -100., \"max\":100.},\n    \"z\" : {\"min\": -31., \"max\":5.},\n    \"r\" : {\"min\": 0., \"max\":1.},\n    \"d\" : {\"min\": 0., \"max\":100.}\n}\ntfms = v2.Compose([\n    ProjectionTransform(proj),\n    ProjectionVizTransform(ds.color_map_rgb_np, ds.learning_map_inv_np, scaling_values),\n])\nds.set_transform(tfms)\nitem = ds[128]\nimg = item['frame']\nlabel = item['label']\n\nplot_projections(img, label, ['x', 'y', 'z', 'r', 'd'])\n\n\n\n\n\n\n\n\nIn the next cell, we can see how long it takes on average to sample an item from this dataset class.\n\n\n\n12.8 ms ± 373 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\nsource\n\n\nProjectionToTensorTransform\n\n ProjectionToTensorTransform (*args, **kwargs)\n\nPytorch transform that converts the projections from np.array to torch.tensor. It also changes the frame image format from (H, W, C) to (C, H, W).\n\n\nExported source\nclass ProjectionToTensorTransform(nn.Module):\n    \"Pytorch transform that converts the projections from np.array to torch.tensor. It also changes the frame image format from (H, W, C) to (C, H, W).\"\n    def forward(self, item):\n        frame_img = item['frame']\n        label_img = item['label']\n        mask_img = item['mask']\n        \n        frame_img = np.transpose(frame_img, (2, 0, 1))\n        frame_img = torch.from_numpy(frame_img).float()\n        if label_img is not None:\n            label_img = torch.from_numpy(label_img).long()\n        if mask_img is not None:\n            mask_img = torch.from_numpy(mask_img)\n\n        item = {\n            'frame': frame_img,\n            'label': label_img,\n            'mask': mask_img,\n        }\n        return item\n\n\nIn order to use the dataset to train pytorch models, it is necessary to transform the data from np.arrays to torch.tensors. Also, for the frame images, it is necessary to transpose (or permute in torch lingo) its axis from channels last (H, W, C) to channels first (C, H, W).\nHere is an example on how to use it with the scan unfolding projection:\n\nproj = UnfoldingProjection(W=1024, H=64)\ntfms = v2.Compose([\n    ProjectionTransform(proj),\n    ProjectionToTensorTransform(),\n])\nds.set_transform(tfms)\n\nitem = ds[0]\nimg = item['frame']\nlabel = item['label']\nmask = item['mask']\n\nprint(img.shape, img.type())\nprint(label.shape, label.type())\nprint(mask.shape, mask.type())\n\ntorch.Size([5, 64, 1024]) torch.FloatTensor\ntorch.Size([64, 1024]) torch.LongTensor\ntorch.Size([64, 1024]) torch.BoolTensor\n\n\nHere is an example on how to combine the classes above to implement a torch.utils.data.Dataloader that iterates on batches of frame, label, mask and weight images:\n\nbs = 10\ndl = DataLoader(ds, bs, num_workers=8)\nfor batch in dl:\n    item = batch\n    img = item['frame']\n    label = item['label']\n    mask = item['mask']\n    print(f\"Shape of img \\t [N, C, H, W]: \\t {img.shape}\")\n    print(f\"Shape of label \\t [N, H, W]: \\t {label.shape}\")\n    print(f\"Shape of mask \\t [N, H, W]: \\t {mask.shape}\")\n    break\n\nShape of img     [N, C, H, W]:   torch.Size([10, 5, 64, 1024])\nShape of label   [N, H, W]:      torch.Size([10, 64, 1024])\nShape of mask    [N, H, W]:      torch.Size([10, 64, 1024])\n\n\nIn the next cell, we can see how long it takes on average to iterate the dataloader for 1 epoch.\n\nfrom tqdm.auto import tqdm\n\n\nfor batch in tqdm(dl):\n    item = batch",
    "crumbs": [
      "behley2019iccv"
    ]
  },
  {
    "objectID": "behley2019iccv.html#remapping-the-labels",
    "href": "behley2019iccv.html#remapping-the-labels",
    "title": "behley2019iccv",
    "section": "Remapping the labels",
    "text": "Remapping the labels\nThe paper MVLidarNet: Real-Time Multi-Class Scene Understanding for Autonomous Driving Using Multiple Views proposes a different mapping of the labels for training than the one proposed by the yaml in the semantic-kitti-api. They only use the following 7 classes:\n\ncar\ntruck\nperson/pedestrians\ncyclist\nroad\nsidewalk\n\nFrom the preferred yaml file, we can use the following remapping rules dictionary and the method learning.remap from the dataset to apply the remapping:\n\nremapping_rules = {\n    1: 1,\n    4: 2,\n    6: 3,\n    7: 4,\n    2: 4,\n    9: 5,\n    11: 6\n}\nds.learning_remap(remapping_rules)\n\nscaling_values = {\n    \"x\" : {\"min\": -100., \"max\":100.},\n    \"y\" : {\"min\": -100., \"max\":100.},\n    \"z\" : {\"min\": -31., \"max\":5.},\n    \"r\" : {\"min\": 0., \"max\":1.},\n    \"d\" : {\"min\": 0., \"max\":100.}\n}\n\ntfms = v2.Compose([\n    ProjectionTransform(proj),\n    ProjectionVizTransform(ds.color_map_rgb_np, ds.learning_map_inv_np, scaling_values),\n])\nds.set_transform(tfms)\nitem = ds[128]\nimg = item['frame']\nlabel = item['label']\n\nplot_projections(img, label, ['x', 'y', 'z', 'r', 'd'])",
    "crumbs": [
      "behley2019iccv"
    ]
  },
  {
    "objectID": "behley2019iccv.html#exploratory-data-analysis-eda",
    "href": "behley2019iccv.html#exploratory-data-analysis-eda",
    "title": "behley2019iccv",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\nDuring training, we noticed some samples had significant portions of their projection images missing. We plotted an example of such image in the next cell.\n\nds = SemanticKITTIDataset(data_path)\nproj = UnfoldingProjection(W=1024, H=64)\ntfm = ProjectionTransform(proj)\nds.set_transform(tfm)\n\nbs = 1000\ndl = DataLoader(ds, bs)\nitem = next(iter(dl))\nimg = item['frame']\nlabel = item['label']\nmask = item['mask']\n\nidx = ((label &lt;= 0).sum(axis=1) &gt; 60).sum(axis=1).argmax()\n\nscaling_values = {\n    \"x\" : {\"min\": -100., \"max\":100.},\n    \"y\" : {\"min\": -100., \"max\":100.},\n    \"z\" : {\"min\": -31., \"max\":5.},\n    \"r\" : {\"min\": 0., \"max\":1.},\n    \"d\" : {\"min\": 0., \"max\":100.}\n}\ntfms = v2.Compose([\n    ProjectionTransform(proj),\n    ProjectionVizTransform(ds.color_map_rgb_np, ds.learning_map_inv_np, scaling_values),\n])\nds.set_transform(tfms)\n\nitem = ds[idx]\ni = item['frame']\nl = item['label']\nplot_projections(i, l, ['x', 'y', 'z', 'r', 'd'])\n\n\n\n\n\n\n\n\nThis level of sparsity could be detrimental to convolutional models, but the total number of samples where this happens seem to be very small with respect to the dataset size. From the best of our knowledge, we haven’t seen this explicitly reported anywhere before.\n\nds.set_transform(tfm)\n\nbs = 1000\ndl = DataLoader(ds, bs, shuffle=True)\nitem = next(iter(dl))\nimg = item['frame']\nlabel = item['label']\nmask = item['mask']\n\nsparsity = (label &lt;= 0).sum(axis=(1,2))/label[0].numel()\nplt.hist(sparsity, bins=100);\n\n\n\n\n\n\n\n\nIn the next cell we can see how unballanced the dataset is with the motorcyclist class (id 8) having 4 orders of magnitude less segmented pixels than classes such as vegetation (id 15) for example. This is reported with Figure 3 in the original paper.\n\nl = label.flatten()\nvalues, counts = np.unique(l, return_counts=True)\nplt.bar(values, np.log10(counts));",
    "crumbs": [
      "behley2019iccv"
    ]
  },
  {
    "objectID": "behley2019iccv.html#dataloaders-for-benchmarking",
    "href": "behley2019iccv.html#dataloaders-for-benchmarking",
    "title": "behley2019iccv",
    "section": "DataLoaders for benchmarking",
    "text": "DataLoaders for benchmarking\nWhen benchmarking different semantic segmentation algorithms, the following DataLoaders should be used.\n\n\n\n\n\n\nWarning\n\n\n\nData augmentation transforms can be applied before (aug_pre_tfms) or after (aug_post_tfms) the projection transforms.\n\n\n\nsource\n\nget_benchmarking_dls\n\n get_benchmarking_dls (data_path, proj_style='unfold', proj_kargs={'W':\n                       512, 'H': 64}, remapping_rules=None,\n                       train_batch_size=8, val_batch_size=16,\n                       num_workers=8, aug_pre_tfms=None,\n                       aug_post_tfms=None)\n\n\n\nExported source\ndef get_benchmarking_dls(\n    data_path,\n    proj_style='unfold',\n    proj_kargs={'W': 512, 'H': 64},\n    remapping_rules=None,\n    train_batch_size=8, \n    val_batch_size=16,\n    num_workers=8,\n    aug_pre_tfms=None,\n    aug_post_tfms=None\n):\n    proj_class = {\n        'unfold': UnfoldingProjection,\n        'spherical': SphericalProjection\n    }\n    assert proj_style in proj_class.keys()\n    proj = proj_class[proj_style](**proj_kargs)\n\n    val_tfms = v2.Compose([\n        ProjectionTransform(proj),\n        ProjectionToTensorTransform(),\n    ])\n    if aug_pre_tfms is None:\n        aug_pre_tfms = nn.Identity()\n    if aug_post_tfms is None:\n        aug_post_tfms = nn.Identity()\n    train_tfms = v2.Compose([\n        aug_pre_tfms,\n        val_tfms,\n        aug_post_tfms\n    ])\n    \n    train_ds = SemanticKITTIDataset(data_path, 'train', train_tfms, remapping_rules=remapping_rules)\n    train_dl = DataLoader(train_ds, batch_size=train_batch_size, num_workers=num_workers, shuffle=True, drop_last=True)\n    \n    val_ds = SemanticKITTIDataset(data_path, 'valid', val_tfms, remapping_rules=remapping_rules)\n    val_dl = DataLoader(val_ds, batch_size=val_batch_size, num_workers=num_workers)\n    \n    return train_dl, val_dl",
    "crumbs": [
      "behley2019iccv"
    ]
  },
  {
    "objectID": "chen2020mvlidarnet.html",
    "href": "chen2020mvlidarnet.html",
    "title": "chen2020mvlidarnet",
    "section": "",
    "text": "(UNDER CONSTRUCTION…)\n\nsource\n\nConvBNReLU\n\n ConvBNReLU (in_channels, out_channels, kernel_size, stride, padding,\n             has_ReLU=True)\n\nSequential composition of convolution, batch normalization and ReLU.\n\nbs, in_c, out_c, h, w = 1, 5, 64, 64, 2048\ninp = torch.randn(bs, in_c, h, w)\n\nb = ConvBNReLU(in_c, out_c, 3, 1, 1)\noutp = b(inp)\nassert outp.shape == (bs, out_c, h, w)\nprint(outp.shape, f'== ({bs}, {out_c}, {h}, {w})')\n\ntorch.Size([1, 64, 64, 2048]) == (1, 64, 64, 2048)\n\n\n\nsource\n\n\nInceptionV2\n\n InceptionV2 (in_channels, out_channels)\n\nInceptionV2 Block from Rethinking the Inception Architecture for Computer Vision.\n\nb = InceptionV2(in_c, out_c)\noutp = b(inp)\nassert outp.shape == (bs, out_c, h, w)\nprint(outp.shape, f'== ({bs}, {out_c}, {h}, {w})')\n\ntorch.Size([1, 64, 64, 2048]) == (1, 64, 64, 2048)\n\n\n\nsource\n\n\nInceptionBlock\n\n InceptionBlock (in_channels, out_channels, n_modules, has_pool=False)\n\nSequential composition of InceptionV2 modules.\n\nb = InceptionBlock(in_c, out_c, 2)\noutp = b(inp)\nassert outp.shape == (bs, out_c, h, w)\nprint(outp.shape, f'== ({bs}, {out_c}, {h}, {w})')\n\ntorch.Size([1, 64, 64, 2048]) == (1, 64, 64, 2048)\n\n\n\nsource\n\n\nEncoder\n\n Encoder (in_channels=5)\n\nMVLidarNet encoder architecture.\n\nenc = Encoder()\noutp = enc(inp)\n[o.shape for o in outp]\n\n[torch.Size([1, 5, 64, 2048]),\n torch.Size([1, 64, 32, 1024]),\n torch.Size([1, 64, 16, 512]),\n torch.Size([1, 128, 8, 256])]\n\n\n\nsource\n\n\nDecoder\n\n Decoder ()\n\nMVLidarNet decoder architecture.\n\ndec = Decoder()\nfts = dec(outp)\nassert fts.shape == (bs, out_c, h, w)\nprint(fts.shape, f'== ({bs}, {out_c}, {h}, {w})')\n\ntorch.Size([1, 64, 64, 2048]) == (1, 64, 64, 2048)\n\n\n\nsource\n\n\nMVLidarNet\n\n MVLidarNet (in_channels=5, n_classes=7)\n\nMVLidarNet semantic segmentation architecture.\n\nn_classes=7\nmodel = MVLidarNet()\nlogits = model(inp)\nassert logits.shape == (bs, n_classes, h, w)\nprint(logits.shape, f'== ({bs}, {n_classes}, {h}, {w})')\n\ntorch.Size([1, 7, 64, 2048]) == (1, 7, 64, 2048)",
    "crumbs": [
      "chen2020mvlidarnet"
    ]
  },
  {
    "objectID": "biasutti2019riu.html",
    "href": "biasutti2019riu.html",
    "title": "biasutti2019riu",
    "section": "",
    "text": "Summarizing quotes from the paper:",
    "crumbs": [
      "biasutti2019riu"
    ]
  },
  {
    "objectID": "biasutti2019riu.html#riunet-architecture",
    "href": "biasutti2019riu.html#riunet-architecture",
    "title": "biasutti2019riu",
    "section": "RIUNet architecture",
    "text": "RIUNet architecture\n\nsource\n\nBlock\n\n Block (in_channels, out_channels)\n\nConvolutional block repeatedly used in the RIU-Net encoder and decoder.\n\n\nExported source\nclass Block(Sequential):\n    \"Convolutional block repeatedly used in the RIU-Net encoder and decoder.\"\n    def __init__(self, in_channels, out_channels):\n        super().__init__(OrderedDict([\n            (f'conv1', Conv2d(in_channels, out_channels, 3, 1, 1, bias=False, padding_mode='circular')),\n            (f'bn1', BatchNorm2d(out_channels, momentum=0.01)),\n            (f'relu1', ReLU()),\n            (f'conv2', Conv2d(out_channels, out_channels, 3, 1, 1, bias=False, padding_mode='circular')),\n            (f'bn2', BatchNorm2d(out_channels, momentum=0.01)),\n            (f'relu2', ReLU()),\n        ]))\n        self.init_params()\n    \n    def init_params(self):\n        for n, p in self.named_parameters():\n            if re.search('conv\\d\\.weight', n):\n                kaiming_normal_(p, nonlinearity='relu')\n\n\nIt implements the following architecture:\n\n\n\n\n\nflowchart LR\n  A((\"\n  Input\n  (bs, in_c, h, w)\")) --&gt; B[\"\n  Conv(3x3)\n  in_c -&gt; out_c\"]\n  B --&gt; C[\"BatchNorm2d\"]\n  C --&gt; D[\"ReLU\"]\n  D --&gt; E[\"\n  Conv(3x3)\n  out_c -&gt; out_c\"]\n  E --&gt; F[\"BatchNorm2d\"]\n  F --&gt; G[\"ReLU\"]\n  G --&gt; H((\"\n  Output\n  (bs, out_c, h, w)\"))\n\n\n\n\n\n\nHere is an example on how to use it:\n\nbs, in_c, out_c, h, w = 1, 5, 64, 64, 512\ninp = torch.randn(bs, in_c, h, w)\n\nb = Block(in_c, out_c)\noutp = b(inp)\nassert outp.shape == (bs, out_c, h, w)\nprint(outp.shape, f'== ({bs}, {out_c}, {h}, {w})')\n\ntorch.Size([1, 64, 64, 512]) == (1, 64, 64, 512)\n\n\nIt initializes the weights from the conv layers following the kaiming_normal_ algorithm in fan_in mode as described in U-Net (page 5).\n\nfrom matplotlib import pyplot as plt\nimport matplotlib.colors as mcolors\n\n\ncolors_list = list(mcolors.TABLEAU_COLORS)\n\ndef plot_param_dists(net, param_re_pattern, nonlinearity_gain):\n    color_idx = 0\n    for n, p in net.named_parameters():\n        if re.search(param_re_pattern, n):\n            x_range = 1.1*p.data.max()\n            ## kaiming normal dist\n            fan_in = p.shape[1]*p.shape[2]*p.shape[3]\n            mu, sigma = 0., np.sqrt(nonlinearity_gain/fan_in)\n            x = np.linspace(-x_range, x_range, 100)\n            y = ((1./(np.sqrt(2*np.pi)*sigma))*np.exp(-0.5*((1./sigma)*(x - mu))**2))\n            plt.plot(x, y, '--', color=colors_list[color_idx], label='Expected '+n)\n            ## sampled weight dist\n            plt.hist(p.view(-1).data, 30, density=True, alpha=0.5, color=colors_list[color_idx], label='Actual '+n)\n            color_idx += 1\n    plt.legend();\n\n\nplot_param_dists(b, 'conv\\d\\.weight', 2.)\n\n\n\n\n\n\n\n\n\nfor n, p in b.named_parameters():\n    if re.search('conv\\d\\.weight', n):\n        fan_in = p.shape[1]*p.shape[2]*p.shape[3]\n        mu, sigma = 0., np.sqrt(2./fan_in)\n        p_data = p.view(-1).data\n        assert abs(mu - p_data.mean()) &lt; 1e-2\n        assert abs(sigma - p_data.std()) &lt; 1e-2\n\n\nsource\n\n\nEncoder\n\n Encoder (channels=(5, 64, 128, 256, 512, 1024))\n\nRIU-Net encoder architecture.\n\n\nExported source\nclass Encoder(Module):\n    \"RIU-Net encoder architecture.\"\n    def __init__(self, channels=(5, 64, 128, 256, 512, 1024)):\n        super().__init__()\n        self.blocks = ModuleList(\n            [Block(channels[i], channels[i+1]) for i in range(len(channels)-1)]\n        )\n    \n    def forward(self, x):\n        enc_features = []\n        for block in self.blocks:\n            x = block(x)\n            enc_features.append(x)\n            x = F.max_pool2d(x, 2)\n        return enc_features\n\n\nIt implements the following architecture:\n\n\n\n\n\nflowchart LR\n  A((\"\n  Input\n  (bs, 5, h, w)\")) --&gt; B[\"\n  Block\n  5 -&gt; 64\"]\n  B --&gt; C[\"MaxPool(2x2)\"]\n  C --&gt; D[\"\n  Block\n  64 -&gt; 128\"]\n  D --&gt; E[\"MaxPool(2x2)\"]\n  E --&gt; F[\"\n  Block\n  128 -&gt; 256\"]\n  F --&gt; G[\"MaxPool(2x2)\"]\n  G --&gt; H[\"\n  Block\n  256 -&gt; 512\"]\n  H --&gt; I[\"MaxPool(2x2)\"]\n  I --&gt; J[\"\n  Block\n  512 -&gt; 1024\"]\n  B --&gt; L((\"\n  Output\n  [(bs, 64, h, w),\n  (bs, 128, h/2, w/2),\n  (bs, 256, h/4, w/4),\n  (bs, 512, h/8, w/8),\n  (bs, 1024, h/16, w/16)]\"))\n  D --&gt; L\n  F --&gt; L\n  H --&gt; L\n  J --&gt; L\n\n\n\n\n\n\nHere is an example on how to use it:\n\nenc = Encoder()\noutp = enc(inp)\n[o.shape for o in outp]\n\n[torch.Size([1, 64, 64, 512]),\n torch.Size([1, 128, 32, 256]),\n torch.Size([1, 256, 16, 128]),\n torch.Size([1, 512, 8, 64]),\n torch.Size([1, 1024, 4, 32])]\n\n\nFor the decoder, the paper mentions the application of “up-convolutions”, which were first defined in U-Net as:\n\n“an upsampling of the feature map followed by a 2x2 convolution (“up-convolution”)”\n\nOur implementation changes the 2x2 convolution to a 3x3 one to avoid croping the feature maps to handle shape mismatches with the encoder skip connections.\n\nsource\n\n\nUpConv\n\n UpConv (in_channels, out_channels)\n\nUp-convolution operation adapted from U-Net.\n\n\nExported source\nclass UpConv(Sequential):\n    \"Up-convolution operation adapted from [U-Net](https://arxiv.org/abs/1505.04597).\"\n    def __init__(self, in_channels, out_channels):\n        super().__init__(OrderedDict([\n            (f'upsample', Upsample(scale_factor=2)),\n            (f'conv', Conv2d(in_channels, out_channels, 3, 1, 1, bias=False, padding_mode='circular')),\n            (f'bn', BatchNorm2d(out_channels, momentum=0.01)),\n        ]))\n        self.init_params()\n    \n    def init_params(self):\n        for n, p in self.named_parameters():\n            if re.search('conv.weight', n):\n                kaiming_normal_(p, nonlinearity='linear')\n\n\nHere is an example on how to use it:\n\nupc = UpConv(1024, 512)\noutp = enc(inp)\nprint(f'before: {outp[-1].shape}')\noutp = upc(outp[-1])\nprint(f'after: {outp.shape}')\n\nbefore: torch.Size([1, 1024, 4, 32])\nafter: torch.Size([1, 512, 8, 64])\n\n\n\nsource\n\n\nDecoder\n\n Decoder (channels=(1024, 512, 256, 128, 64))\n\nRIU-Net decoder architecture.\n\n\nExported source\nclass Decoder(Module):\n    \"RIU-Net decoder architecture.\"\n    def __init__(self, channels=(1024, 512, 256, 128, 64)):\n        super().__init__()\n        self.upconvs = ModuleList(\n            [UpConv(channels[i], channels[i+1]) for i in range(len(channels)-1)]\n        )\n        self.blocks = ModuleList(\n            [Block(channels[i], channels[i+1]) for i in range(len(channels)-1)]\n        )\n    \n    def forward(self, enc_features):\n        x = enc_features[-1]\n        for i, (upconv, block) in enumerate(zip(self.upconvs, self.blocks)):\n            x = upconv(x)\n            x = torch.cat([x, enc_features[-(i+2)]], dim=1)\n            x = block(x)\n        return x\n\n\nIt implements the following architecture:\n\n\n\n\n\nflowchart LR\n  A((\"\n  Input\n  [(bs, 1024, h/16, w/16),\n  (bs, 512, h/8, w/8),\n  (bs, 256, h/4, w/4),\n  (bs, 128, h/2, w/2),\n  (bs, 64, h, w)]\")) --&gt; B[\"\n  UpConv\n  1024 -&gt; 512\"]\n  B --&gt; C[\"\n  Block\n  concat(512,512) -&gt; 512\"]\n  A --&gt; C\n  C --&gt; D[\"\n  UpConv\n  512 -&gt; 256\"]\n  D --&gt; E[\"\n  Block\n  concat(256,256) -&gt; 256\"]\n  A --&gt; E\n  E --&gt; F[\"\n  UpConv\n  256 -&gt; 128\"]\n  F --&gt; G[\"\n  Block\n  concat(128,128) -&gt; 128\"]\n  A --&gt; G\n  G --&gt; H[\"\n  UpConv\n  128 -&gt; 64\"]\n  H --&gt; I[\"\n  Block\n  concat(64,64) -&gt; 64\"]\n  A --&gt; I\n  I --&gt; J((\"\n  Output\n  (bs, 64, h, w)\"))\n\n\n\n\n\n\nHere is an example on how to use it:\n\ndec = Decoder()\noutp = enc(inp)\nfts = dec(outp)\nassert fts.shape == (bs, out_c, h, w)\nprint(fts.shape, f'== ({bs}, {out_c}, {h}, {w})')\n\ntorch.Size([1, 64, 64, 512]) == (1, 64, 64, 512)\n\n\nIt initializes the weights from the upconv layers following the kaiming_normal_ algorithm in fan_in mode and nonlinearity set as ‘linear’, since no relu layer is used in the operation.\n\nplot_param_dists(dec, 'upconvs\\.\\d\\.conv\\.weight', 1.)\n\n\n\n\n\n\n\n\n\nfor n, p in dec.named_parameters():\n    if re.search('upconvs\\.\\d\\.conv\\.weight', n):\n        fan_in = p.shape[1]*p.shape[2]*p.shape[3]\n        mu, sigma = 0., np.sqrt(1./fan_in)\n        p_data = p.view(-1).data\n        assert abs(mu - p_data.mean()) &lt; 1e-3\n        assert abs(sigma - p_data.std()) &lt; 1e-3\n\n\nsource\n\n\nRIUNet\n\n RIUNet (in_channels=5, hidden_channels=(64, 128, 256, 512, 1024),\n         n_classes=20)\n\nRIU-Net complete architecture.\n\n\nExported source\nclass RIUNet(Module):\n    \"RIU-Net complete architecture.\"\n    def __init__(self, in_channels=5, hidden_channels=(64, 128, 256, 512, 1024), n_classes=20):\n        super().__init__()\n        self.n_classes = n_classes\n        self.input_norm = BatchNorm2d(in_channels, affine=False, momentum=None)\n        self.backbone = Sequential(OrderedDict([\n            (f'enc', Encoder((in_channels, *hidden_channels))),\n            (f'dec', Decoder(hidden_channels[::-1]))\n        ]))\n        self.head = Conv2d(hidden_channels[0], n_classes, 1)\n        self.init_params()\n\n    def init_params(self):\n        for n, p in self.named_parameters():\n            if re.search('head\\.weight', n):\n                normal_(p, std=1e-2)\n            if re.search('head\\.bias', n):\n                zeros_(p)\n    \n    def forward(self, x):\n        x = self.input_norm(x)\n        features = self.backbone(x)\n        prediction = self.head(features)\n        \n        return prediction\n\n\nWe slightly changed it to accept 5 input channels (i.e. x, y, z, depth and reflectance) instead of the 2 (depth and elevation) proposed in the original paper.\nIt implements the following architecture:\n\n\n\n\n\nflowchart LR\n  A((\"\n  Input\n  (bs, 5, h, w)\")) --&gt; B[\"Encoder\"]\n  B --&gt; C[\"Decoder\"]\n  C --&gt; D[\"\n  Conv(1x1)\n  64 -&gt; 20\"]\n  D --&gt; E((\"\n  Output\n  (bs, 20, h, w)\"))\n\n\n\n\n\n\nHere is an example on how to use it:\n\nn_classes=20\nmodel = RIUNet()\nlogits = model(inp)\nassert logits.shape == (bs, n_classes, h, w)\nprint(logits.shape, f'== ({bs}, {n_classes}, {h}, {w})')\n\ntorch.Size([1, 20, 64, 512]) == (1, 20, 64, 512)\n\n\nIt initializes the weights from the classification head from a normal distribution with a standard deviation of 1e-2. The motivation is to reduce any random bias on the outputs of the untrained model.\n\nplot_param_dists(model, 'head\\.weight', 0.0064)\n\n\n\n\n\n\n\n\n\nfor n, p in model.named_parameters():\n    if re.search('head\\.weight', n):\n        fan_in = p.shape[1]*p.shape[2]*p.shape[3]\n        mu, sigma = 0., 1e-2\n        p_data = p.view(-1).data\n        assert abs(mu - p_data.mean()) &lt; 1e-2\n        assert abs(sigma - p_data.std()) &lt; 1e-2",
    "crumbs": [
      "biasutti2019riu"
    ]
  },
  {
    "objectID": "biasutti2019riu.html#loss-function",
    "href": "biasutti2019riu.html#loss-function",
    "title": "biasutti2019riu",
    "section": "Loss function",
    "text": "Loss function\nThe proposed equation for the loss function is the following.\n\\[\nE = -\\sum_{x \\in \\Omega}{w(x)\\log(p_{l(x)}(x))\\mathbf{1}_{\\{m(x)&gt;0\\}}}\n\\]\nThe factor \\(w\\) is motivated as a way “to give more importance to pixels that are close to a separation between two labels”. It was first defined in U-Net as follows.\n\\[\nw(x) = w_c + w_{0}\\exp\\left ( -\\frac{(d_{1}(x) + d_{2}(x))^{2}}{2\\sigma^{2}} \\right )\n\\]\nFrom U-Net:\n\n“where \\(w_{c}\\) […] is the weight map to balance the class frequencies, \\(d_{1}\\) […] denotes the distance to the border of the nearest cell and \\(d_{2}\\) […] the distance to the border of the second nearest cell.\n\nWhile this particular equation for \\(w\\) can be seen as very specific for the original biomedical application in the Unet paper, its motivation is generally valid for any image semantic segmentation task such as ours. Hence, we can rewrite the equation for \\(w\\) as follows.\n\\[\nw(x) = w_{c} + \\lambda w_{b}\n\\]\nSimilarly to the previous equation, \\(w_{c}\\) accounts for class imbalance, but the second term is rewritten as a general \\(w_{b}\\) factor that should account for the boundaries of the semantic maps.\nFor now, we only implemented the \\(w_{c}\\) factor. We leave the \\(w_{b}\\) factor and the necessary experimentation to evaluate its impact in the final model as TODO items.\nSince the weighing and masking of the cross entropy loss is already implemented through the parameters weight and ignore_index in Pytorch’s CrossEntropyLoss module, we implement our own wrapper simply for convenience.\n\nsource\n\nWeightedMaskedCELoss\n\n WeightedMaskedCELoss (weight, device)\n\nConvenient wrapper for the CrossEntropyLoss module with a weight and ignore_index paremeters already set.\n\n\nExported source\nclass WeightedMaskedCELoss(Module):\n    \"Convenient wrapper for the CrossEntropyLoss module with a `weight` and `ignore_index` paremeters already set.\"\n    def __init__(self, weight, device):\n        super().__init__()\n        self.ignore_index = -1\n        self.wmCE = CrossEntropyLoss(weight=torch.from_numpy(weight).to(device), ignore_index=self.ignore_index)\n\n    def forward(self, pred, label, mask):\n        label = label.clone()\n        label[~mask] = self.ignore_index\n        loss = self.wmCE(pred, label)\n        return loss",
    "crumbs": [
      "biasutti2019riu"
    ]
  },
  {
    "objectID": "biasutti2019riu.html#metrics",
    "href": "biasutti2019riu.html#metrics",
    "title": "biasutti2019riu",
    "section": "Metrics",
    "text": "Metrics\nThe proposed metric is the Intersection over Union (a.k.a. the Jaccard Index), which is implemented in the following module.\n\nsource\n\nSegmentationIoU\n\n SegmentationIoU (num_classes, reduction='mean')\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\nExported source\nclass SegmentationIoU(Module):\n    def __init__(self, num_classes, reduction='mean'):\n        assert reduction in ['mean', 'none']\n        super().__init__()\n        self.num_classes = num_classes\n        self.reduction = reduction\n\n    def forward(self, pred, label, mask):\n        label = label.clone()\n        pred[~mask] *= 0\n        label[~mask] *= 0\n        oh_pred = F.one_hot(pred, num_classes=self.num_classes)\n        oh_label = F.one_hot(label, num_classes=self.num_classes)\n        intersect = (oh_pred*oh_label).sum(dim=(1, 2))\n        union = ((oh_pred + oh_label).clamp(max=1)).sum(dim=(1,2))\n        intersect[union == 0] = 1\n        union[union == 0] = 1\n        iou = (intersect/union)\n        if self.reduction == 'mean':\n            iou = iou[:,1:].mean()\n        return iou",
    "crumbs": [
      "biasutti2019riu"
    ]
  },
  {
    "objectID": "biasutti2019riu.html#semantickitti-experiments",
    "href": "biasutti2019riu.html#semantickitti-experiments",
    "title": "biasutti2019riu",
    "section": "SemanticKITTI Experiments",
    "text": "SemanticKITTI Experiments\nAs mentioned in the summarizing quotes in the first section, the original experiments used “range-images with segmentation labels exported from the 3D object detection challenge of the KITTI dataset”. Improving on that, we implement our experiments using the SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences, which was published around the same time as the RIU-Net paper and introduced a much bigger and better dataset for the task of pointcloud semantic segmentation.\nBased on what was summarized from the Methodology and Experiments sections and our adaptations described in this notebook, here are the specifications of the Data, Model, Loss, Optimizer and Metric used in our experiments:\n\nData:\n\nApplied spherical projection to 512 x 64 pixels\n5 channels: x, y, z, reflectance, depth\n\n\n\nfrom colorcloud.behley2019iccv import get_benchmarking_dls\n\n\ndls = get_benchmarking_dls(\n    data_path='/workspace/data',\n    proj_style='spherical',\n    proj_kargs={\n        'fov_up_deg': 3., \n        'fov_down_deg': -25., \n        'W': 512, 'H': 64\n    }\n)\n\n\nModel:\n\nRIUNet module with its default hyperparameters\n\n\n\ndevice = (\n    \"cuda\"\n    if torch.cuda.is_available()\n    else \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cpu\"\n)\nprint(f\"Using {device} device\")\n\n\nmodel = RIUNet().to(device)\n\n\nLoss:\n\nWeightedMaskedCELoss module\n\n\n\nloss_fn = WeightedMaskedCELoss(dls[0].dataset.content_weights, device)\n\n\nOptimizer:\n\nAdamW with the OneCycleLR lr scheduler\n\n\n\nlr = 1e-4\nn_epochs = 30\ntotal_steps = n_epochs*len(dls[0])\n\noptimizer = AdamW(model.parameters(), lr)\nlr_scheduler = OneCycleLR(optimizer, lr, total_steps)\n\n\nMetric:\n\nSegmentationIoU module\n\n\n\nmetric_fn = SegmentationIoU(dls[0].dataset.learning_map_np.max() + 1)\n\nTODO: needs proper documentation with code examples for the experiments.",
    "crumbs": [
      "biasutti2019riu"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "colorcloud",
    "section": "",
    "text": "(UNDER CONSTRUCTION…) :construction_worker:",
    "crumbs": [
      "colorcloud"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "colorcloud",
    "section": "Install",
    "text": "Install\nClone the repo and run:\npip install -e '.[dev]'",
    "crumbs": [
      "colorcloud"
    ]
  },
  {
    "objectID": "index.html#how-to-collaborate",
    "href": "index.html#how-to-collaborate",
    "title": "colorcloud",
    "section": "How to collaborate",
    "text": "How to collaborate\n\nClone the repo and build the development image:\n\ngit clone https://github.com/AIR-UFG/colorcloud.git\ncd colorcloud\ndocker build -t colorcloud .\n\nAfter the image is built, run a container:\n\ndocker run --gpus all --ipc=host --network=host --mount type=bind,source=~/.gitconfig,target=/etc/gitconfig --name &lt;name_your_container&gt; colorcloud:latest",
    "crumbs": [
      "colorcloud"
    ]
  }
]